








\) MASTER TECHNICAL REPORT




\)?



SRC\(^9\), or the  system, is an integrated, multi-layered computational framework that models how music transforms from raw sound into perceived structure and cognitive meaning. It is composed of three tightly coupled modules:


     \) – Spectral Sound Space: A high-resolution spectral analysis engine that extracts the fundamental acoustic content of music, including partials, harmonics, microtonal deviations, and time–frequency–amplitude relations.
    
     \) – Resonance-Based Relational Reasoning: A harmonic reasoning engine that interprets spectral data through scalar field theory, resonance topology, and psychoacoustic principles to model musical structure without symbolic assumptions.
    
     \) – Cognitive Consonance Circuit: A neurophysiologically grounded model of perceptual resonance and emotional impact, structured into nine cognitive units derived from EEG/fMRI literature and organized into measurable, time-varying neural signatures.


The superscript 9 (\(^9\)) denotes the system's expansion into , each modeled as a unique  in the C\(^3\) architecture, and accessible from the resonance outputs of R\(^3\) and the acoustic signals of S\(^3\).

\)?

Traditional music theory frameworks start from notation and seek meaning through pre-defined symbolic systems. Meanwhile, modern machine learning models such as Jukebox or Magenta generate musical output with no semantic or cognitive interpretability. Neuroscientific studies, though rich in EEG/fMRI data, lack a bridge to music-theoretical relevance.

SRC\(^9\) was created to solve this cross-domain disconnect by:


     Starting with physical audio (not symbolic input)
     Modeling resonance fields and energy topologies, not abstract pitch classes
     Mapping these fields into time-aligned neural signatures rooted in empirical neuroscience




SRC\(^9\) operates as a real-time or batch-based pipeline:

\[
 
 \)  \)  \) 
 
\]


     \) produces framewise spectral data: \(_t = \, , , \\)
     \) computes: Phantom Root (PR), Resonance Potential (\(\)), Resonance Field Map (RFM), Cognitive Resonance Vector (CRV)
     \) receives these values to drive:
    
         Tension mapping (CTU)
         Affective modeling (AOU)
         Memory alignment (SAU)
         Expectation violation (IEU)
         Group synchrony and attention modeling (IRU, NSU)
    


Each stage of SRC\(^9\) is aligned to a common temporal resolution (typically 0.1s), enabling multimodal synchronization with EEG, fMRI, audio playback, or Unity-based interactive scenes.



Let \(x(t)\) be the input waveform segmented into overlapping frames.

From S\(^3\):
\[
_i = \f_0, f_1, ..., f_16\,   f_n = _i,n
\]

From R\(^3\):
\[
_i, _i, _i(f), _i =  _i, _i, _i 
\]

From C\(^3\):
\[
C^3(t) = _i=1^9 w_i  _i(t),  _i(t) = _j w_ij  _j(t)
\]

Each  is an EEG/fMRI-derived observable (e.g., \(\) phase-locking, MMN, BOLD z-score), anchored anatomically using MNI coordinates in a 3D GlassBrainMap interface.



The architecture supports :

\[
\) feedback  \) weighting  \) filter adjustment
\]

For example:


     High CTU (tension) output may modulate the harmonic weights in R\(^3\)'s \(\) calculation
     Strong PIU (immersion) may suppress noisy partials in S\(^3\)’s visualization layer
     SAU (semantic memory) may trigger audio annotations or dynamic re-sequencing


This loop forms the foundation of responsive, perceptually aware music analysis, generation, or education platforms.



SRC\(^9\) is:


     : Grounded in psychoacoustics, computational neuroscience, and acoustic theory
     : Built with independently executable units, fully API-controllable
     : Every layer has an interpretable, dynamic output (HTML, PNG, 3D)
     : Exports to Unity, GlassBrainMap, OSC/VR platforms
     : Models not just structure, but perception


\)

SRC\(^9\) is divided into three orthogonal modules: S\(^3\) (Spectral), R\(^3\) (Resonance), and C\(^3\) (Cognitive). Each functions as an independent layer in the signal–structure–perception continuum, while maintaining tight alignment through shared temporal schemas, compatible data models, and reciprocal feedback.

\) — Spectral Sound Space

  
Transforms raw audio into high-resolution spectral frames by extracting:


   Fundamental frequency (\(f_0\))
   Harmonic partials (1–16)
   Amplitude in dBFS
   Microtonal pitch symbols (e.g., A4⁺¹, C3⁻²)


  
JSON array of frames at 0.1s intervals:

<pre><code>

  "time": 1.2,
  "partials": [
     "freq": 440.0, "db": -12.4, "isFundamental": true, "symbol": "A4⁰" ,
     "freq": 880.0, "db": -18.2, "harmonic_index": 2, "symbol": "A5⁰" 
  ]

</code></pre>




   CREPE (f₀ extraction)
   librosa (STFT, RMS, cent conversion)
   Custom Python pipeline with modular scripts


  
Inspired by spectral music theory (Grisey, Murail) and auditory physiology (tonotopic mapping), S\(^3\) treats the frequency domain as the true substrate of musical identity — discarding staff notation and tuning system assumptions.

\) — Resonance-Based Relational Reasoning

  
Processes S\(^3\) outputs to identify and model harmonic structure, not via tonal syntax, but via energetic interaction between partials.




   PRU — Phantom Root Unit: Detects implied fundamentals from overtone sets
   RPU — Resonance Potential Unit: Computes scalar coherence \(\) per frame
   RFMU — Resonance Field Modeling Unit: Generates Gaussian field over frequency
   CRVU — Cognitive Resonance Vectoring Unit: Extracts TPS, TFI, NSF metrics




\[
(f, t) = _i A_i(t)  e^-2^2
\]

This formula defines a scalar resonance density field over log-frequency space.


\[
 = [, , ]  [0,1]^3
\]

Used as the summary output of all R\(^3\) activity and as input to C\(^3\) modules.

  
Builds on psychoacoustic roughness theory (Plomp & Levelt), neural entrainment (Bidelman), and just intonation topology (Sethares, Tymoczko).

\) — Cognitive Consonance Circuit

  
Models how humans perceive, evaluate, and emotionally respond to the harmonic signals computed in R\(^3\). Each response is neurophysiologically grounded and structured by a 9-Unit circuit.

\) Units:


   CTU — Cognitive Tension
   AOU — Affective Orientation
   IEU — Intuitive Expectation
   SRU — Somatic Resonance
   SAU — Semantic-Autobiographical
   PIU — Phenomenological Immersion
   IRU — Interpersonal Resonance
   NSU — Neural Synchronization
   RSU — Resonance Synthesis (summary vector)



\[
C^3(t) = _i=1^9 w_i  _i(t)
  
_i(t) = _j w_ij  _j(t)
\]

Each Node is mapped to EEG/fMRI features (e.g., alpha asymmetry, gamma coherence, BOLD z-scores) and anatomically located via MNI coordinates in the .


CRVU \(\) CTU, AOU, PIU, NSU \\
Neural feedback loops modify RFM weighting, Φ computation, and partial salience in S\(^3\).

  
Combines computational music cognition (Lerdahl, Huron), neuroaesthetics (Zatorre, Koelsch), and brain–music entrainment literature.

\)

SRC\(^9\) formalizes music cognition through a hierarchy of equations, resonance functions, and vector spaces that bridge physical sound, psychoacoustic interaction, and perceptual abstraction.



Let the raw audio input be a continuous time-domain waveform \(x(t)\). SRC\(^9\) processes this waveform in fixed-length, overlapping windows:


x_i(t) = x(t + iH),   i


Where:

   \(H\) = hop size (e.g., 10 ms)
   \(x_i(t)\) = time-domain windowed signal


Each frame is then passed into CREPE or equivalent pitch tracking module to estimate:


f_0i,  A_i,   \f_in\_n=1^16


These define the fundamental + harmonic space used across the system.



The scalar measure \(\) represents the instantaneous coherence of all spectral partials within a frame:


(t) = _i<j |f_i(t) - f_j(t)| + 


Where:

   \(f_i(t), A_i(t)\) = frequency and amplitude of partial \(i\)
   \(\) = small constant to avoid division by zero


Interpretation:

   Higher amplitude → more weight
   Smaller interval → stronger resonance


This equation generalizes roughness and consonance models using continuous frequency data.



To represent spectral resonance topographically, a Gaussian kernel density is applied across a log-frequency grid:


(f, t) = _i A_i(t)  e^-2^2


This converts discrete spectral data into a continuous scalar field — a kind of “terrain map” of resonance.


To compute directionality of tonal pull:


 (f, t) = (f, t) f


Used in CRVU → TFI to model spectral fusion.



The final cognitive resonance vector is defined as:


 = [, , ]


Each metric is defined as:



 = 1 + _(t)




 = 1 +  | (f, t)| 




 = _t (t)  e^- t


Where \(\) is a decay constant modeling attention/memory trace.



Let \(\f_1, f_2, ..., f_n\\) be a group of detected pitch events. Then, the phantom root \(r\) is the frequency that minimizes mean harmonic error:


r^* = _r ( n _i | r  h_i | )


Where \(h_i\) are harmonic template integers (e.g., [1,2,3,4]).



Each frequency can be projected into prime-exponent vector space:


_i = (x_2, x_3, x_5, x_7, ...)
  f_i = 2^x_2  3^x_3  5^x_5 


Mean vector yields:


_PR = N _i _i


Which is mapped back to the frequency domain to compute phantom root in symbolic-harmonic space.




<table><tr><td>|

</td><td></td></tr><tr><td>
\(\) (Resonance Potential)</td><td>\((t) = _i<j |f_i - f_j| + \)</td></tr><tr><td>
RFM (Field)</td><td>\((f, t) = _i A_i e^-(f - f_i)^2 / 2^2\)</td></tr><tr><td>
Gradient</td><td>\(  =   /  f\)</td></tr><tr><td>
CRV</td><td>\([, , ]\)</td></tr><tr><td>
PR Estimation</td><td>\(r = _r _i |f_i - r h_i| / r h_i\)</td></tr><tr><td>
NSF</td><td>\(_t (t) e^- t\)</td></tr><tr><td></td></tr></table>




Time is not merely a parameter in SRC\(^9\); it is a structural axis along which all modules are synchronized, integrated, and compared. Every analytical unit, perceptual frame, and visual output is aligned to a common frame-based time grid, enabling coherence between real-time interaction, dynamic modeling, and retrospective analysis.



All SRC\(^9\) operations are executed in frames of fixed temporal resolution.


    \( t = 0.1\) seconds
    \(200\) frames
    S\(^3\) → R\(^3\) → C\(^3\)


This resolution provides a compromise between cognitive relevance (auditory segmentation and beat-level processes) and computational tractability.



Each frame is indexed and timestamped explicitly:

<pre><code>

  "time": 3.2,
  "partials": [...],
  "phi": 2.83,
  "crv": 
    "TPS": 0.814,
    "TFI": 0.652,
    "NSF": 0.042
  

</code></pre>

Additional time-windowed metrics (e.g., windowed Φ, RFM segments) use labeled intervals:

<pre><code>

  "window": "5.0–8.0",
  "phi": 9.183,
  "window_size": 3

</code></pre>

All time-based data are synchronized via integer multiples of \( t\).



Many perceptual phenomena operate on time windows rather than individual frames (e.g., expectancy, stability, modulation). To simulate this:


_T = _t=t_0^t_1 (t)
  T = [t_0, t_1]


Window lengths are configurable: 1s, 3s, 5s, or 7s (10–70 frames). 

These windows feed CRVU and symbolic inference layers, and provide smoothed curves for visualization.



Each SRC\(^9\) unit reads or writes data at the same temporal resolution, ensuring:


     Synchrony between harmonic events and cognitive metrics
     Real-time overlay of Φ, RFM, PR, and CRV
     Accurate PRU segment demarcation based on CentTracker (\(49\)c deviation)


Example: frame 38 at \(t=3.8\)s will contain:


     17 partials from S\(^3\)
     1 \(\) scalar from RPU
     RFM density array from RFMU
     3-element CRV vector from CRVU
     Segment label if included in a PRU group




To support live streaming or reactive composition, each frame can be evaluated asynchronously. A frame handler listens for input, processes data, and stores results:



\[
_t        
\]

Latency budget: \(< 50\) ms per frame.



SRC\(^9\) includes support for timeline-aligned playback and export:


       \(\) frame index
      Link frame analysis to audio segments
      Frame-aligned curves, scrollable graphs


Visual overlays are rendered in rasterized layers, each 216px tall, stacked into a 2160px 4K vertical space. These layers include:


     RawSpectrum
     PRU
     RPU
     RFMU
     CRVU




Each frame includes metadata for traceability:

<pre><code>

  "time": 12.3,
  "frame_id": 123,
  "source": "RawSpectrum01",
  "checksum": "ae347ac1...",
  "validated": true

</code></pre>

This ensures reproducibility and integrity in batch pipelines or dynamic environments.



SRC\(^9\) temporal architecture transforms time from a passive marker to an active modeling dimension. It enables:


     Segment-based cognition modeling (e.g., tonal drift, root migration)
     Layer-aligned visualization of concurrent harmonic and perceptual states
     Real-time reactivity and temporal learning models




The analytical power of SRC\(^9\) depends not only on its internal computations, but on its ability to represent, store, and exchange data in structured, interpretable, and extensible formats. This chapter outlines the file architectures, symbolic systems, and cross-platform export mechanisms that enable integration across scientific, educational, and creative platforms.



All unit processing in SRC\(^9\) is time-aligned to frames in the following structure:

<pre><code>

  "time": 3.2,
  "partials": [
     "freq": 261.63, "amplitude": 0.84, "symbol": "C4⁰", "harmonic_index": 0, "isFundamental": true ,
     "freq": 523.25, "amplitude": 0.51, "symbol": "C5⁰", "harmonic_index": 1, "isFundamental": false 
  ],
  "phi": 2.83,
  "crv":  "TPS": 0.812, "TFI": 0.694, "NSF": 0.039 

</code></pre>



     : Timestamp in seconds
     : List of harmonic components with symbolic pitch
     : Frame-level \(\) scalar
     : Cognitive resonance vector output (from CRVU)




Each SRC\(^9\) unit outputs a structured file:


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>PR-unit-temporal.json</td><td>PR frequency, symbol, harmonic group, fusion score</td></tr><tr><td>RPU</td><td>RP-framewise.json, RP-windowed.json</td><td>\(\) per frame or time window</td></tr><tr><td>RFMU</td><td>RFM-unit.json</td><td>Resonance field grid + gradient vector</td></tr><tr><td>CRVU</td><td>CRV-unit.json</td><td>Cognitive vector: TPS, TFI, NSF</td></tr><tr><td></td></tr></table>


All outputs are timestamp-aligned at 0.1s resolution.



SRC\(^9\) uses a compact symbolic system to encode pitch with microtonal precision:


     Format: 
     Superscripts denote deviation in cents:
    
         \(⁰\) = 0 cent
         \(⁺¹\) = +25 cent
         \(⁻¹\) = –25 cent
         \(⁺²\) = +50 cent
    





      = C4 at 0c
      = A4 +25 cents
      = G3 –50 cents


This allows symbolic readability while preserving microtonal resolution from S\(^3\) partial tracking.



Unity and WebGL-based environments operate on line-by-line streaming. Each partial is exported as:

<pre><code>
time,freq,amplitude,isFundamental,harmonic_index,symbol
1.0,220.0,0.82,True,0,G3⁰
1.0,440.0,0.51,False,1,G4⁰
</code></pre>

Used to instantiate prefabs or terrain meshes in:


     
     
     




Internally, each frame can also be represented in matrix form for ML pipelines:

\[
_t = 

f_0 & A_0 & h_0 \\
f_1 & A_1 & h_1 \\
 &  &  \\
f_16 & A_16 & h_16


_t = 

_t \\
_t \\
_t

\]

This supports CRV-based AI composition, harmonic fingerprint learning, or real-time ML inference.




      — RawSpectrum-unit.json
      — Phantom root segments
      — Framewise/windowed Φ
      — Grid + gradient fields
      — Cognitive vector layers


All files are UTF-8 encoded and stored in flat JSON or CSV formats for interoperability with scientific tools and frontend visual platforms.


---

```latex


SRC\(^9\) is designed not only to compute resonance and cognition, but to render it visually. Every analytical layer, from raw spectral frames to cognitive vectors, is projected into a coherent visual system aligned across time and frequency. These visualizations are not cosmetic: they serve as cognitive tools, allowing researchers, composers, and users to intuitively see the dynamics of harmonic structure.



SRC\(^9\) visualization output is composed of vertically stacked unit layers, each aligned to a shared horizontal timeline (0–20s). Each layer occupies 216px vertical space, with the RawSpectrum occupying 1080px as base.


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
RawSpectrum (S\(^3\))</td><td>colored partial markers</td><td>1080</td></tr><tr><td>PRU</td><td>red bars with pitch labels</td><td>216</td></tr><tr><td>RPU</td><td>\(\) line + window overlays</td><td>216</td></tr><tr><td>RFMU</td><td>resonance field heatmap</td><td>216</td></tr><tr><td>CRVU</td><td>RGB stacked bars</td><td>216</td></tr><tr><td>
Total</td><td>2160 px (4K)</td><td></td></tr><tr><td></td></tr></table>




 Y-axis (log scale)  
 Marker size, object scale, emission intensity  
 X-axis (0–20s, 0.1s resolution)  
  

   Frequency class (e.g., pitch class palette)
   Functional role (e.g., PR = red, \(\) = gray, field = inferno colormap)
   CRV: red = TPS, green = TFI, blue = NSF


 Microtonal notations (e.g., G2⁺¹) appear on PR bars and RawSpectrum points.




      for interactive HTML visualizations, hoverable markers, frame-aligned curves
      for static PNG exports, segment overlays, symbol-annotated graphs
      for 3D mesh-based field rendering and partial animation




The file  combines all unit visual outputs into a single 3840×2160 image or HTML frame.




     Frame-synchronized overlays
     Independent vertical scaling per layer
     Interactive time cursor
     Toggleable layers




CRVU’s cognitive outputs (TPS, TFI, NSF) are displayed as stacked colored bars:


     Red: TPS — stability
     Green: TFI — fusion
     Blue: NSF — memory encoding


The bar height corresponds to value magnitude \([0,1]\).



RFMU’s scalar field data are converted into 3D terrain meshes:


     X = time
     Z = frequency (log scale)
     Y = field strength \(\) terrain height
     Emission map = normalized \(\)
     Overlay: peak paths, PR lines, curvature ridges


Implemented using Unity’s , , and shader-based vertex displacement.




     Timeline scrubbing (linked to frame index)
     Real-time playback at 10 FPS (0.1s/frame)
     Sound-reactive visuals (optional)
     Dynamic camera tracking (e.g., PR curve follower)





      visually teach harmonic fields, voice leading, polyphony
      detect modulation, PR shift, dissonance zones
      display resonance terrain live in VR
      use RFM as a topographic canvas for generative tools




A defining feature of SRC\(^9\) is its recursive structure: each module not only feeds into the next but also receives feedback from downstream layers. This transforms the system from a static analyzer into a dynamic resonance engine — capable of adaptive learning, reweighting, and perceptually informed transformation.



The primary communication loop of SRC\(^9\) is:

\[
      
\]



     S\(^3\) \(\) R\(^3\): partial frames \(\) harmonic reasoning
     R\(^3\) \(\) C\(^3\): CRV vector + \(\) + RFM data




     C\(^3\) \(\) R\(^3\): attention, immersion, memory modulation
     R\(^3\) \(\) S\(^3\): spectral filtering, dynamic rescaling, visualization tuning





High immersion scores increase weight on core partials in RFM generation:

\[
A_i^* = A_i  (1 + _)
\]


RPU’s \(\) calculation uses tension-weighted denominators:

\[
'(t) = _i<j |f_i - f_j| +   _
\]


SAU can extend windowed Φ integration across prior phrases to model phrase re-entry or long-term attractor stabilization.




      Hide partials with low salience or low CRV
      Annotate or override f₀ labels with C\(^3\)-informed symbolic tags
      Increase opacity/size of key partials if memory/affect signal is high




Assume a phrase begins with stable CRV:

\[
_1 = [0.91, 0.88, 0.82]
\]

The system increases visual brightness of corresponding partials and amplifies RFM terrain peaks.

A modulation or PR shift occurs:

\[
_2 = [0.41, 0.33, 0.17]
\]

This results in:

     Sharpened \(\)RFM contours
     Increase in partial flicker effect in Unity
     CRV bars collapse → signaling cognitive destabilization






<pre><code>

  "time": 4.3,
  "feedback": 
    "CTU": 0.87,
    "PIU": 0.76,
    "NSU": 0.41
  

</code></pre>

Received by:


     RFM filter generator
     Partial weighting engine
     Visual modulation manager





     Feedback modulation is clamped between \( 25\%\) per frame
     Frame history buffers used to prevent oscillation artifacts
     Async-safe handlers allow interruption or override at runtime




The feedback loop is not just for refinement. It enables new applications:


     Interactive composition: CRV \(\) generative seed adjustment
     Brain–music co-evolution: EEG \(\) C\(^3\) \(\) R\(^3\) reshaping
     Self-regulating installations: perception \(\) structure \(\) perception




SRC\(^9\) is more than a computational toolkit — it is a conceptual shift in how music is understood, analyzed, and linked to perception. This chapter contextualizes SRC\(^9\) within existing scientific disciplines and explains its novel contribution to music theory, auditory neuroscience, cognitive modeling, and AI.




      Offers symbolic, style-specific models (e.g., Roman numerals, keys) that lack generalizability to non-Western, microtonal, or electronically produced music.
      Describes neural encoding of sound, but lacks structural models of music capable of predicting EEG/fMRI response.
      Generate convincing audio, but are black-box and devoid of interpretability or symbolic grounding.


\) bridges these silos by combining spectral analysis, resonance modeling, and cognitive simulation into a unified framework.





     Sub-cent partial tracking with harmonic identification
     Microtonal symbolic pitch encoding (\(\)25c steps)
     4K-resolution time–frequency mapping




     Real-valued \(\) coherence computation
     RFM field topography and attractor mapping
     Phantom root detection without symbolic grammar
     Resonance-based perception modeling (TPS, TFI, NSF)




     Unit–Node–Element hierarchy with EEG/fMRI anchors
     Integration with real-time neural interfaces (e.g., OpenBCI, Emotiv)
     Emotional salience, memory encoding, and inter-brain synchrony modeling




Traditionally:

\[
    
\]

SRC\(^9\) reverses this:

\[
    
\]

This change recognizes that human listeners don’t hear scores — they hear waveforms, from which meaning emerges through spectral convergence, not grammatical rule sets.




<table><tr><td>|

</td><td></td></tr><tr><td>
Lerdahl/Jackendoff GTTM</td><td>Symbolic-only, lacks spectral realism</td></tr><tr><td>Huron’s ITPRA</td><td>Predictive cognition, no spectral grounding</td></tr><tr><td>Bregman’s Auditory Scene Analysis</td><td>Compatible perceptually, no structural formalism</td></tr><tr><td>Schenkerian Analysis</td><td>Hierarchical tonality, score-dependent</td></tr><tr><td>Tonnetz/Neo-Riemannian Theory</td><td>Static topology, lacks time/frequency axes</td></tr><tr><td>MusicLM / Magenta</td><td>Non-interpretable deep generative models</td></tr><tr><td></td></tr></table>





     \(\) aligns with neural synchrony and EEG FFR data (Bidelman 2011)
     CRV mirrors attention and memory indices in fMRI studies (Zatorre et al. 2013)
     Microtonal segmentation reflects known auditory thresholds (Moore 2012)
     Temporal frame length matches auditory ERP resolution (MMN, P300)





     Neurocognitive analysis of music listening
     Dynamic modeling of musical form without score
     Empirical testing of tension, memory, or absorption in real time
     Cross-cultural harmonic modeling (e.g., gamelan, maqam, drone music)
     Tonotopic map visualization from real audio





     AI composition using CRV trajectories
     VR installations guided by RFM terrain
     Generative systems with real-time PR feedback
     Improvisation interfaces using \(\) heatmaps and modulation vectors





     Teaching spectral vs. symbolic harmony
     Visualizing modulation, drift, tension, and resolution
     Exploring affective resonance in sound
     Multisensory music learning through waveform → field → cognition




SRC\(^9\) can integrate with:


     EEG systems (OpenBCI, Emotiv) — real-time feedback to CRVU
     Machine learning — CRV as feature vector for emotion or form prediction
     Neuroscientific experiments — auditory-cognitive mapping under stimuli
     Notational systems — hybrid symbolic–spectral scores




While SRC\(^9\) is rooted in scientific theory and cognitive models, it is also a practical software system: a set of coordinated Python, JSON, CSV, Unity, and WebGL components that form a modular, executable pipeline.

This chapter describes the engineering architecture of SRC\(^9\) — the file structures, runtime logic, APIs, and execution modes that bring its resonance engine to life.



SRC\(^9\) is composed of three primary code layers:


      Python modules for S\(^3\), R\(^3\), and C\(^3\) computations
      Plotly, Matplotlib, and WebGL-based rendering scripts
      Unity scene controllers, OSC interfaces, and data streaming tools




<pre><code>
/src/
  /s3/
    extract_frequencies_crepe.py
    harmonics_matching.py
  /r3/
    PR_unit_temporal.py
    RP_unit_combined.py
    RFM_unit_analysis.py
    CRV_unit_analysis.py
  /c3/
    CTU_compute.py
    AOU_compute.py
    ...
  /visualize/
    visualize_PR_temporal.py
    visualize_RFM_unit.py
    visualize_overlay_all.py
  run_SRC9_pipeline.py
/data/
  /raw/
    RawSpectrum-unit.json
  /output/
    /PR/
    /RP/
    /RFM/
    /CRV/
/output/
  R3-overlay.html
  *.png
/unity/
  CSVLoader.cs
  SpectrumVisualizer.cs
  CRVOverlayHUD.cs
</code></pre>



Execution can occur:


      — via 
      — unit-by-unit for debugging
      — via live frame ingestion, under development





     S\(^3\): CREPE \(\) base spectrum
     R\(^3\): PRU, RPU, RFMU, CRVU
     C\(^3\): All 9 Units \(\) summary JSON
     Visualizer: Generate overlays and interactive outputs
     Unity export: CSV + animation parameters




 always JSON

 JSON + PNG + HTML (Plotly) + CSV (Unity)

Each function or script is:


     stateless (idempotent)
     reusable (called by other pipelines)
     visually testable (through plot outputs)








Modules are black-box compatible — meaning any layer (e.g., RFMU) can be replaced or extended with an alternate implementation without breaking upstream/downstream logic.



On a standard system (Intel i7, 16 GB RAM):


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>2.1 s</td><td>1.4 s</td></tr><tr><td>RPU</td><td>4.0 s</td><td>2.2 s</td></tr><tr><td>RFMU</td><td>5.2 s</td><td>3.1 s</td></tr><tr><td>CRVU</td><td>1.0 s</td><td>1.0 s</td></tr><tr><td>C\(^3\) full unit set</td><td>\(\)9 s</td><td>—</td></tr><tr><td>Overlay (HTML)</td><td>—</td><td>3–5 s</td></tr><tr><td></td></tr></table>


 ~20–25 seconds per 20s audio input.



 CSV  


      — object representation
      — parser and loader
      — prefab instantiation
      — affective bar display




     glowing spheres for partials
     PR curves via 
     terrain mesh for RFM via 





      3.9+
      2021.3 LTS
      for experiment notebooks
      HTML + Plotly/Three.js (experimental)
      WebSocket-ready




SRC\(^9\) is open-source under the MIT license. Code, data samples, visual exports, and Unity demos are available at:



Contributors are invited to fork units, extend field models, or contribute to future modules such as OL (Overtone Locking) or GMI (Global Musical Inference).



SRC\(^9\) is not merely a framework, a pipeline, or a set of scripts. It is a paradigm: a new way of conceptualizing, measuring, and interacting with musical structure. It brings together physics, perception, cognition, and computation into a unified temporal field theory of music.



Harmony is traditionally defined symbolically — as triads, scales, or functional progressions. SRC\(^9\) proposes a redefinition:



Instead of working in discrete steps (e.g., I–IV–V), SRC\(^9\) defines harmony as an evolving topology:


      Phantom roots, perceptual centers
      Tonal pull, dissonance slope
      \(\) coherence regions
      Topographic drift in RFM space




Through C\(^3\), harmony becomes measurable not only in acoustics, but in brain-space:


     TPS \(\) perceived stability
     TFI \(\) spectral coherence
     NSF \(\) memory encoding strength


This transforms music analysis into neuroperceptual logic — one grounded in actual listener behavior, emotion, and timing.



Because SRC\(^9\) is not bound to Western notation, keys, or 12-TET tuning, it generalizes to:


     Drone-based music (e.g., Indian raga, Tibetan chant)
     Just intonation and spectralism
     Electronic soundscapes and ambient textures
     Improvised music, microtonal works, non-metered environments


Its mathematical core — \(\), RFM, CRV — is culturally neutral but perceptually rich.



SRC\(^9\) invites a reimagination of music cognition as a form of field-based reasoning:

\[

\]

This claim opens doors to:


     New theories of musical time and memory
     Biofeedback systems that respond to sonic states
     Emotion-aware generative music engines
     Aesthetic theories rooted in resonance, not style




SRC\(^9\) is designed to evolve.

It is not only a tool for analyzing the music of the past — it is a language for the music of the future. A music that is:


     Spectrally informed
     Resonantly grounded
     Cognitively engaged
     Mathematically coherent
     Visually immersive


The resonance field is the new score.

The CRV vector is the new expressive arc.

The mind–ear interface is the new stage.

\).



SRC\(^9\) is not merely a framework, a pipeline, or a set of scripts. It is a paradigm: a new way of conceptualizing, measuring, and interacting with musical structure. It brings together physics, perception, cognition, and computation into a unified temporal field theory of music.



Harmony is traditionally defined symbolically — as triads, scales, or functional progressions. SRC\(^9\) proposes a redefinition:



Instead of working in discrete steps (e.g., I–IV–V), SRC\(^9\) defines harmony as an evolving topology:


      Phantom roots, perceptual centers
      Tonal pull, dissonance slope
      \(\) coherence regions
      Topographic drift in RFM space




Through C\(^3\), harmony becomes measurable not only in acoustics, but in brain-space:


     TPS \(\) perceived stability
     TFI \(\) spectral coherence
     NSF \(\) memory encoding strength


This transforms music analysis into neuroperceptual logic — one grounded in actual listener behavior, emotion, and timing.



Because SRC\(^9\) is not bound to Western notation, keys, or 12-TET tuning, it generalizes to:


     Drone-based music (e.g., Indian raga, Tibetan chant)
     Just intonation and spectralism
     Electronic soundscapes and ambient textures
     Improvised music, microtonal works, non-metered environments


Its mathematical core — \(\), RFM, CRV — is culturally neutral but perceptually rich.



SRC\(^9\) invites a reimagination of music cognition as a form of field-based reasoning:

\[

\]

This claim opens doors to:


     New theories of musical time and memory
     Biofeedback systems that respond to sonic states
     Emotion-aware generative music engines
     Aesthetic theories rooted in resonance, not style




SRC\(^9\) is designed to evolve.

It is not only a tool for analyzing the music of the past — it is a language for the music of the future. A music that is:


     Spectrally informed
     Resonantly grounded
     Cognitively engaged
     Mathematically coherent
     Visually immersive


The resonance field is the new score.

The CRV vector is the new expressive arc.

The mind–ear interface is the new stage.

\).










  
The S³ Module is the spectral analysis and fundamental sound modeling layer of the SRC⁹ system. This report documents all theoretical, algorithmic, mathematical, engineering, and aesthetic structures of the module on both scientific and applied levels.




     Within SRC⁹, S³ forms the first layer of the S³–R³–C³ system.
     This report establishes the data foundation for R³ and C³ integration.





     Introduction and General Overview
     Theoretical Foundations
     Mathematical and Algorithmic Model
     Signal Processing Pipeline Architecture
     Data Structures and Formats
     Visualization Layers
     R³ and C³ Integration
     Optimization and Performance
     References and Sources
     Appendices and Code Examples






     General structure of the SRC⁹ system
     Role of S³ in the system
     Scientific and aesthetic goals
     Development history
     Use cases: music analysis, composition, AI, cognitive science






     Summary of theoretical frameworks from Schaeffer, Grisey, Murail
     Frequency-based harmony
     Role of harmonic series in music




     Critical bands
     Harmonic fusion
     Perceptual resonance




     Time–frequency duality
     Limitations of Fourier theory






     Frequency distributions beyond 12TET
     Cent and pitch class calculations




     Concept of partials
     Harmonic series tolerance (±50 cents)
     Partial grouping algorithms




     Use of CREPE
     Comparison with pYIN
     Viterbi optimization








     , , , , 




     
     
     




     Audio → f₀ + dB → harmonic matching → color coding → visualization




      structure
     Command examples






     Frame structure
     Partial fields: freq, dB, isFundamental


 (optional)

     Extended data post harmonic matching


  

     Prepared output for R³


  

     Optional: record of spectral color mapping






     Time–Frequency map (3840×2160)
     Microtonal grid
     Donut Spectrum color transitions




     Time–Frequency–Amplitude (x, y, z)
     Unity and Plotly usage
     Mesh and dot modes




     CSV export format
     Prefab system
     Real-time camera and lighting setup






     JSON → Resonance Potential (Φ), Harmonic Distance (HD), Phantom Root (PR)




     Microtonal notation + amplitude → cognitive resonance estimation




     R³ → S³ color changes
     C³ → S³ attention level simulation






     JSON file size
     CREPE runtime
     Viterbi duration




     4K render times
     Number of prefabs in Unity






     Grisey, Murail, Schaeffer
     Lerdahl, Tymoczko, Sethares
     Cognitive modeling: Zatorre, Patel, Koelsch
     CREPE (Kim et al., 2018)
     Librosa and pYIN documentation
     Plotly, Unity technical docs




     Code blocks and annotations
     JSON examples
     Color spectrum image (C → B transition)
     Unity scene settings (camera, light, prefab connections)
     Screenshots and final outputs






The Spectral Sound Space (S³) module constitutes the foundational analytic layer of the SRC⁹ framework—an interdisciplinary system that unites spectral acoustics, resonance modeling, and cognitive neuroscience. Positioned as the first pillar in the tripartite structure of SRC⁹ (S³–R³–C³), S³ provides the fundamental data structures and perceptual primitives from which harmonic reasoning (R³) and neural interaction modeling (C³) emerge.

Whereas traditional music analysis systems typically begin from symbolic notation (e.g., MIDI, scores), S³ reverses the process: it operates directly on audio waveforms to extract a detailed, high-resolution representation of acoustic content in both time and frequency. This bottom-up approach ensures that the analytic foundation is directly grounded in the physical properties of sound, enabling it to generalize across styles, cultures, tuning systems, and performance modalities.



The design of S³ is motivated by three fundamental observations:


     \\
    Any auditory event can be decomposed into partials—individual frequency components that change over time in pitch, amplitude, and phase. These partials are the building blocks of tone perception and harmonic structure.

     \\
    The human auditory system does not “hear notes,” but rather detects frequency patterns and intensity envelopes. Notation is a cultural abstraction layered atop an auditory substrate. Therefore, analysis should start at the spectral level if it aims to reflect perceptual and cognitive realities.

     \\
    The perception of consonance, root, resonance, and tonality emerges from interactions between partials, not from theoretical scales. S³ enables the measurement and visualization of these interactions in their raw, physical form.




In the architecture of SRC⁹, S³ performs three critical roles:



S³ converts raw audio into structured data representations including:


     Fundamental frequencies (f₀) over time
     Partial tracks (harmonics and inharmonic components)
     Amplitude (in dB) of each component
     Microtonal symbolic notations and pitch-class mappings
     Spectral centroids, energy envelopes, and entropy values




It selectively isolates acoustically meaningful content from noise, silences, and irrelevant transients using amplitude thresholds, frame-based smoothing, and overtone filters.



S³ produces high-resolution 2D and 3D visualizations of the spectral data:


     2D time–frequency maps at 3840×2160 resolution
     3D spectrograms with frequency–amplitude towers
     Interactive spectral canvases for analysis and composition


These outputs form the primary input for the R³ module (Resonance-Based Relational Reasoning), where spectral data is analyzed for harmonic structure, resonance potential, overtone locking, and phantom root phenomena.



The S³ module draws simultaneously from:


     \\
    Emphasizing sound itself as the basis of musical form, S³ adopts this premise and extends it computationally.

     \\
    Tools such as CREPE (deep learning pitch estimator), librosa (audio analysis library), and FFT algorithms are integrated to allow frame-level pitch and amplitude extraction with sub-millisecond precision.

     \\
    Through microtonal accuracy and overtone modeling, S³ mirrors how the auditory cortex processes complex sound structures.

     \\
    S³ interfaces directly with Unity, WebGL, and VR environments, producing real-time interactive spectral landscapes for education, analysis, and artistic use.




The Spectral Sound Space (S³) module serves as the foundation of the SRC⁹ system. It provides the raw material—both data and perceptual structure—from which musical reasoning and cognitive mapping can emerge. Unlike traditional systems that operate on abstracted notation or symbolic input, S³ roots its analysis in the physical substance of sound.

Its capacity to extract, quantify, and visualize meaningful partials across styles and tuning systems makes it a uniquely versatile tool. Whether applied to Renaissance counterpoint, spectral composition, or neural music analysis, S³ enables a new form of bottom-up, physics-based music understanding.





The S³ module is predicated on a fundamental epistemological shift: that music analysis should begin not with the score, but with the sound itself. Western music theory has historically prioritized symbolic abstraction (notation, keys, chords), often detaching the study of music from the phenomena it arises from—vibrations in air, shaped by instruments, perceived by human bodies.

This module challenges that precedence.

Rather than assuming that sound serves as a mere carrier for symbolic content, S³ posits the inverse: symbolic constructs are interpretations of an underlying spectral substrate. Frequencies, amplitudes, and overtones are not peripheral—they are the music.

This shift aligns with the work of spectral composers (e.g., Gérard Grisey, Tristan Murail), auditory cognition researchers (e.g., Diana Deutsch, Albert Bregman), and philosophers of music (e.g., Pierre Schaeffer). S³ bridges their insights with modern computation.



The theoretical roots of S³ are deeply informed by spectralism: a movement in contemporary composition that foregrounds the timbral and acoustical properties of sound over traditional harmonic systems.




     Sound is a complex spectrum of partials, not a fixed pitch.
     Harmony arises from the overtone series, not from abstract interval systems.
     The orchestration of spectra defines form and tension more than functional harmony.


S³ translates these ideas into data structures:


     Every partial is tracked as an individual frequency–amplitude event.
     No assumption of equal temperament is made—frequencies are real-valued, microtonal.
     Harmonic relationships are computed, not assumed.


Through its modular design, S³ formalizes the intuition of the spectralists into a machine-readable format.



S³ is not merely mathematically accurate; it is psychoacoustically meaningful. Its architecture reflects how human auditory perception operates:


      S³ models perceptual overlap via overtone locking mechanisms.
      Each partial is assigned an identity, allowing for grouping and source separation.
      Partial tracks are evaluated over time, reflecting how we perceive tone continuity.


These principles are embedded in the frame-based, high-resolution analysis pipeline. The system's sensitivity to cent-level frequency shifts, amplitude decay curves, and overtone fusion makes it not just accurate but also cognitively plausible.



Unlike traditional pitch class systems which operate on discrete steps (12TET), S³ operates in continuous pitch space. Every frequency is stored as a floating-point value, and pitch class labeling is optional, reversible, and tolerant to cent deviations.


     Pitch is mapped not via quantization, but through continuous cent distance metrics.
     Microtonal variations (±5 to ±25 cents) are preserved and visualized explicitly.
     Symbolic mappings (e.g., C4⁺¹) are generated only for readability—not as assumptions.


This enables the analysis of music outside the bounds of Western tuning: just intonation, 24-TET, gamelan pelog/slendro, non-octave repeating scales, or even completely aleatoric sound structures.



Finally, the theoretical foundation of S³ leads naturally into R³, the Resonance-Based Relational Reasoning module.

Where S³ represents the raw materials of sound, R³ interprets those materials relationally:


     How do partials converge?
     Which phantom roots emerge?
     What is the resonance potential of a harmonic field?


S³ provides the data; R³ provides the reasoning.

This progression mirrors human musical experience:


     First, we hear sound (S³).
     Then, we infer structure and coherence (R³).
     Finally, we respond cognitively and emotionally (C³).


In this chain, S³ is the anchor: a physically-grounded, perceptually-aligned, computationally robust representation of what music is, before it is interpreted.





The S³ module translates raw acoustic signals into structured, symbolic, and interpretable representations. Its algorithmic core centers around three dimensions:


     Frequency (Hz)
     Amplitude (dB)
     Time (ms resolution)


These three variables are extracted and tracked for each partial—a distinct sinusoidal component of a sound. Unlike traditional pitch-tracking systems that detect only the fundamental (\(f_0\)), S³ treats the full overtone field as first-class data. This makes it possible to model harmonic content, resonance, and spectral evolution with great precision.



The input signal \(x(t)\) is divided into overlapping frames for analysis. Typical frame parameters are:


     Sampling rate: \(f_s = 16000\) Hz (CREPE optimal)
     Frame length: 1024 samples (\(\)64 ms)
     Hop size: 160 samples (\(\)10 ms)


Let:

\[
x_i(t) = x(t + i  H),   i
\]

where \(H\) is the hop size. Frames are then passed into the frequency estimation pipeline.



The system uses CREPE (Kim et al., 2018), a deep convolutional network trained to estimate the fundamental frequency of monophonic signals with high accuracy.

Given an audio frame \(x_i(t)\), CREPE returns:


     \(f_0i\): estimated fundamental frequency
     \(c_i  [0,1]\): confidence score
     optionally, \(a_i\): activation maps (ignored in S³)


The result is a time series:

\[
\ (t_i, f_0i, c_i) \_i=1^N
\]

If \(c_i < \) (confidence threshold), the value is discarded or interpolated.



Each frame’s amplitude is derived via RMS energy or CREPE confidence:

\[
A_i = 20  _10((x_i))  
\]

or, if using confidence:

\[
A_i = 20  _10(c_i + )
\]

All amplitudes are normalized between \([-50 , 0 ]\) and clipped accordingly.



S³ computes not only the fundamental \(f_0\), but constructs a harmonic field:

\[
H_i = \ n  f_0i  n = 1, 2, , N_h \
\]

where \(N_h\) is the number of harmonics tracked (typically 16). Each harmonic is stored with:

\[
f_i,n = n  f_0i
\]

\[
A_i,n = A_i - (n)  
\]

 is flagged  if \(n = 1\).

These are saved per frame in the following format:

<pre><code>

  "time": 0.010,
  "partials": [
    "freq": 440.0, "db": -21.0, "isFundamental": true,
    "freq": 880.0, "db": -31.0, "isFundamental": false,
    "freq": 1320.0, "db": -35.0, "isFundamental": false
  ]

</code></pre>



For each partial, the module calculates the pitch in cents relative to A4 (440 Hz):

\[
(f) = 1200  _2 ( 440 )
\]

This allows:


     Microtonal deviation tracking (e.g., +8 cents)
     Conversion to symbolic pitch classes (e.g., A4⁺⁸)


This symbolic mapping is computed but not quantized; the raw frequency is retained as canonical.



S³ maintains full resolution in time (10 ms), frequency (floating-point Hz), and amplitude (floating-point dB).


      arrays of frames with timestamps and partials
      \(\)10,000 frames per 100 seconds of audio, \(\)160,000 partials




An optional phase identifies recurring partials and clusters them. This supports later stages in R³ (e.g., overtone locking and phantom root detection).

Matching is done by:


     Nearest-neighbor search across frames using frequency proximity
     Harmonic number estimation:
    \[
    n =  ( f_0 )
    \]




The S³ module translates sound into a structured lattice of time–frequency–amplitude events. Its mathematical model ensures:


     Sub-millisecond temporal resolution
     Cent-level pitch accuracy
     Frame-wise harmonic field tracking
     Explicit microtonal notation
     Compatibility with resonance modeling in R³






The signal processing architecture of the S³ module is designed as a modular, multi-stage pipeline that transforms raw audio into structured, symbolically annotated, and visually renderable spectral data.

This architecture balances three key principles:


      Time frames in 10 ms steps, frequency in cent-level precision, amplitude in dB.
      Each step is encapsulated as an independent script with defined input/output formats (typically JSON).
      The system supports integration with other modules (R³, C³), external libraries (CREPE, librosa), and interactive engines (Unity, Plotly).




The pipeline operates within a clearly defined project structure:

<pre><code>
S3-Module/
├── audio/                     # Input audio files (.wav or .mp3)
│   └── cello_suite_no1.wav
├── json/                      # Intermediate and final data outputs
│   ├── base_frequencies.json
│   └── partials.json
├── output/                    # Visualization exports
│   ├── s3_visualization.png
│   └── s3_visualization.html
├── scripts/                   # Pipeline scripts
│   ├── extract_frequencies_crepe.py
│   ├── harmonics_matching.py
│   └── s3_visualization.py
├── utils/                     # Utility modules (shared functions)
│   ├── freq_to_rgb.py
│   └── freq_to_microtonal.py
└── requirements.txt
</code></pre>

This structure ensures reproducibility and separation of concerns across computation, data, and display.



  
 


       or  audio file  
      frame size, hop size, duration (default: 10 seconds)
     
    
         Load audio using librosa
         Segment into overlapping frames
         Pass frames to  for \(f_0\) estimation
         Estimate amplitude via RMS or CREPE confidence
    
     


<pre><code>

  "frames": [
    
      "time": 0.010,
      "partials": [
        "freq": 440.0, "db": -21.0, "isFundamental": true
      ]
    ,
    ...
  ]

</code></pre>

  
 


       from Stage 1
     
    
         For each frame, extract fundamental \(f_0\)
         Construct harmonics: \(H_n = n  f_0\) (typically for \(n = 2\) to \(16\))
         Assign decreasing amplitude per harmonic (e.g., \(-10\) dB per step)
         Merge harmonics with original fundamental
    
       — contains full harmonic field per frame with  flags


  
 


       from Stage 2
     
    
         Convert frequency to log-scale (Hz \(\) cent)
         Normalize amplitude to dB \(\) size and opacity
         Assign color using  (Donut Spectrum with 7-note color wheel)
         Add microtonal labels using 
    
     
    
         PNG at 3840×2160
         Optional: HTML (Plotly interactive)
    




<pre><code>
# Step 1: Extract fundamentals
python3 scripts/extract_frequencies_crepe.py audio/Debussy.mp3 json/base_frequencies.json

# Step 2: Build harmonic field
python3 scripts/harmonics_matching.py json/base_frequencies.json json/partials.json

# Step 3: Visualize
python3 scripts/s3_visualization.py json/partials.json output/s3_visualization.png
</code></pre>



<pre><code>
python3 scripts/s3_visualization.py json/partials.json output/s3_visualization.html
</code></pre>



You may include a master pipeline script () that automates all stages:

<pre><code>
#!/bin/bash
echo "Running S³ Pipeline..."
python3 scripts/extract_frequencies_crepe.py audio/input.wav json/base_frequencies.json
python3 scripts/harmonics_matching.py json/base_frequencies.json json/partials.json
python3 scripts/s3_visualization.py json/partials.json output/s3_visualization.png
echo "Done."
</code></pre>



Each script includes:


     Usage help when arguments are missing
     File existence checks for inputs
     JSON schema validation (planned)
     Logging system for progress and warnings




The functions in each script can also be exposed via an internal API for integration with:


     Jupyter Notebooks (for educational/research use)
     Unity (real-time input/output via OSC or TCP)
     R³ module (resonance analysis integration)




The S³ module's signal processing architecture forms a clean, high-resolution, and extensible pipeline from sound to structure. It is engineered to allow spectral data to be transformed into meaningful musical structures, ready for further analysis by R³ and C³. The use of standard tools, modular scripts, and clear data formats ensures that the system is not only scientifically robust but also developer-friendly and future-proof.





The Spectral Sound Space (S³) module operates on a carefully designed set of data structures to ensure maximum flexibility, resolution, and interoperability. These formats serve as both internal data representations and external interfaces for downstream modules (R³, C³), visualizations, and interactive environments (e.g., Unity).

The primary data structure is JSON, chosen for its human readability, machine parsability, and web compatibility. All spectral events—frequencies, amplitudes, time steps, and symbolic annotations—are encoded in JSON using standardized field names and schema.



At its core, S³ stores information as a sequence of time-ordered frames, each corresponding to a small window of the input signal (typically every 10 ms).


<pre><code>

  "frames": [
    
      "time": 0.010,
      "partials": [
        
          "freq": 440.0,
          "db": -21.0,
          "isFundamental": true,
          "note": "A4+0",
          "rgb": [1.0, 0.0, 0.0],
          "harmonicIndex": 1
        ,
        
          "freq": 880.0,
          "db": -31.0,
          "isFundamental": false,
          "note": "A5+0",
          "rgb": [0.0, 0.0, 1.0],
          "harmonicIndex": 2
        
      ]
    ,
    ...
  ]

</code></pre>



      (in seconds)
     : an array of objects describing frequency components





<table><tr><td>|

</td><td></td><td></td></tr><tr><td>
</td><td>float</td><td>Frequency in Hz</td></tr><tr><td></td><td>float</td><td>Amplitude in dBFS (normalized between -50 and 0)</td></tr><tr><td></td><td>bool</td><td>Whether this is the frame’s \(f_0\)</td></tr><tr><td></td><td>string</td><td>Symbolic label, e.g. </td></tr><tr><td></td><td>list</td><td>RGB color derived from freq (used in visualization)</td></tr><tr><td></td><td>int</td><td>1 for fundamental, 2+ for overtones</td></tr><tr><td></td></tr></table>





<table><tr><td>
</td><td></td><td></td></tr><tr><td>
Time</td><td>10 ms (adjustable)</td><td>100 frames per second</td></tr><tr><td>Frequency</td><td>Float (cent-level precision)</td><td>442.7 Hz</td></tr><tr><td>Amplitude</td><td>Float (-50 to 0 dB)</td><td>-27.3 dB</td></tr><tr><td>Pitch Notation</td><td>Microtonal (cent offset)</td><td>A4⁺⁷</td></tr><tr><td></td></tr></table>


This high resolution enables perceptual modeling (via R³) and microtonal music analysis.





Used in 3D interactive rendering:

<pre><code>
time,frequency,dB,r,g,b
0.010,440.0,-21.0,255,0,0
0.010,880.0,-31.0,0,0,255
</code></pre>



For 3D terrain mapping:

<pre><code>
x (time), y (frequency), z (normalized dB)
0.010, 440.0, 0.58
0.010, 880.0, 0.32
</code></pre>



(Optional) For integration with Unity or Max/MSP:

<pre><code>
/s3/frame 0.010 440.0 -21.0 1.0 0.0 0.0
</code></pre>



You may enforce structure via JSON schema:



<pre><code>

  "type": "object",
  "properties": 
    "time":  "type": "number" ,
    "partials": 
      "type": "array",
      "items": 
        "type": "object",
        "properties": 
          "freq":  "type": "number" ,
          "db":  "type": "number" ,
          "isFundamental":  "type": "boolean" ,
          "note":  "type": "string" ,
          "rgb": 
            "type": "array",
            "items":  "type": "number" ,
            "minItems": 3,
            "maxItems": 3
          ,
          "harmonicIndex":  "type": "integer" 
        ,
        "required": ["freq", "db"]
      
    
  ,
  "required": ["time", "partials"]

</code></pre>

This formalism ensures forward compatibility and guards against malformed data.



For a 10-second file with 10 ms frames and 16 partials per frame:


     1,000 frames × 16 partials = 16,000 data points
     Approx. 2–5 MB as compressed JSON
     Easily processed in memory on modern systems


You can compress JSON with gzip or use a binary format (e.g., MessagePack) for lower latency streaming.




      is the canonical format for R³ analysis
      reads from it to generate high-res plots
      converts it to CSV for interactive 3D display
     Optional mapping to  format (SRC standard, in progress)




The data structures used by S³ are designed for high-resolution spectral modeling, downstream integration with resonance/cognitive modules, and compatibility with artistic, analytical, and interactive applications.

The JSON-based frame–partial model ensures that time, frequency, amplitude, and pitch data are tightly coupled and fully traceable. This standardization supports rigorous analysis, intuitive visualization, and machine readability.





Visualization in the S³ module is not simply a graphical rendering of spectral data—it is a perceptually-aligned, musically meaningful, and aesthetically optimized representation of sound. The purpose of visualizing partials is twofold:


      Allow researchers, theorists, and composers to examine the spectral and temporal structure of sound at microtonal and microtemporal resolution.
      Mirror how auditory structures are perceived, allowing visual artifacts to stand in for psychoacoustic phenomena such as overtone fusion, resonance, and vibrato.


The design principles of S³ visualizations stem from an integrated philosophy of scientific legibility, musical interpretation, and visual minimalism.



The system employs two synchronized visual layers:




      Time (seconds), linear scale
      Frequency (Hz), log scale
      Amplitude (only in 3D mode)




     Resolution: 3840 px × 2160 px (default)
     Frame step: 10 ms
     Frequency precision: cent-level (\(\)0.58 px per cent)


Each partial is visualized as a dot or micro-line, positioned at its (time, frequency) coordinate, and styled according to amplitude and pitch-class-based color.




     Aligned on X (time) with Tier 1
     Contains labels for:
    
         Fundamental pitches (e.g., A4, C5⁺⁸)
         Onset durations (shown as segment lengths)
         Microtonal deviations (in cent format)
    
     Can be toggled or overlaid for interpretive use


This layer enables mapping from physical spectrum to musical language (e.g., score-independent notation).



Instead of static color coding (e.g., red = high freq), S³ implements a musically cyclic, frequency-based color system:


Colors cycle with octaves, not linear Hz.




     C: Red
     D: Orange
     E: Yellow
     F: Green
     G: Light Blue
     A: Blue
     B: Violet


Intermediate tones interpolate between anchors.

Repeat per octave: \(_2(f/f_)  1\)



     261.6 Hz (C4) \(\) Red
     440 Hz (A4) \(\) Blue
     1046 Hz (C6) \(\) Red again




\[
(f) = (_2(f/f_)  1)  360^
\]

Converted to HSV hue \(\) RGB

This mapping aids intuitive identification of tonal color, enhances spectral grouping perception, and allows visual equivalence across octaves.



Amplitude (in dB) is mapped to:


     Dot size (larger = louder)
     Opacity (higher dB = more solid)
     Optional Z-height (in 3D mesh)


Amplitude range is normalized between -50 dB and 0 dB.


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
-50</td><td>1.0 px</td><td>barely visible</td></tr><tr><td>-30</td><td>3.5 px</td><td>translucent</td></tr><tr><td>-10</td><td>6.5 px</td><td>prominent</td></tr><tr><td>0</td><td>8.0 px</td><td>fully saturated</td></tr><tr><td></td></tr></table>




In 2D interactive views (Plotly), each partial responds to hover:



<pre><code>
Time: 3.140 s
Frequency: 445.6 Hz
Amplitude: -23.4 dB
Note: A4⁺¹⁵
Harmonic Index: 1
</code></pre>

This ensures each data point is interpretable in real time and can be cross-referenced with musical structure.

In Unity, similar hover behavior is achieved using Raycast + Tooltip systems.



S³ visualizations are governed by three design maxims:


      Every color and glyph must carry meaning—no arbitrary or decorative elements.
      Reflect the sparsity of partials, allow negative space, and avoid visual clutter.
      Layer complexity gradually (e.g., hide partials \(<\) -40 dB by default), allow user toggles.





      Static, high-resolution (e.g., for papers, print). Rendered at 3840×2160 using Plotly + Kaleido.
      Interactive hover-capable plots. Zoom, layer toggle, export options.
      For 3D rendering (frequency \(\) height). Used in VR/AR sound-space explorations.





     Temporal motion blur to represent vibrato or tremolo
     Animated playback with cursor-following time marker
     2.5D stacked pitch-class visualization (similar to piano roll, but spectral)




Visualization in the S³ module is not a cosmetic feature—it is a tool for perception, cognition, and musical logic. Every graphical element is tied to a psychoacoustic or musical principle, and the rendering stack ensures that physical properties of sound are transposed into visually legible, interpretable, and aesthetically powerful structures.





While S³ provides a high-resolution, physically-grounded representation of sound, it is only the first layer of the broader SRC⁹ system. The modules that follow—R³ (Resonance-Based Relational Reasoning) and C³ (Cognitive Consonance Circuit)—rely on the structured spectral data output by S³ to compute:


     Harmonic relationships (R³)
     Resonance fields and perceptual centers (R³)
     Neural correlates of sound structure (C³)
     Bounded attention, emotional salience, and cognitive load (C³)


This section describes how data flows from S³ into R³ and C³, and how architectural compatibility is maintained across analytical, real-time, and interactive systems.






      
     R³ extracts for each frame:
    
         \(f_0\) (fundamental)
         Full harmonic field
         Frequency ratios
         Harmonic intervals
         Microtonal deviations
    





     R³ outputs for each frame:
    
         Resonance potential (\(\))
         Harmonic distance (HD)
         Phantom root (PR)
         Overtone locking (OL)
         Vectorized harmonic embeddings
    
     C³ computes:
    
         Temporal Perceptual Stability (TPS)
         Tonal Fusion Index (TFI)
         Neural Synchronization Fields (NSF)
    





Each module reads and writes time-aligned frame arrays with shared conventions.


<pre><code>

  "frames": [
    
      "time": 0.010,
      "partials": [
         "freq": 440.0, "db": -21.0, "isFundamental": true 
      ]
    
  ]

</code></pre>


<pre><code>

  "frames": [
    
      "time": 0.010,
      "phantom_root": 65.4,
      "resonance_potential": 0.92,
      "harmonic_vectors": [[1, 0, -1], [0, 1, -2]],
      "harmonic_distances": [
         "pair": [0, 1], "hd": 0.25 
      ]
    
  ]

</code></pre>


<pre><code>

  "frames": [
    
      "time": 0.010,
      "tps": 0.82,
      "tfi": 0.61,
      "nsf": 0.45
    
  ]

</code></pre>



Each module exposes a consistent API:


<pre><code>
from r3_engine import compute_resonance_metrics
r3_out = compute_resonance_metrics(s3_data)  # JSON in, JSON out
</code></pre>


<pre><code>
from c3_engine import compute_cognitive_metrics
c3_out = compute_cognitive_metrics(r3_out)
</code></pre>

This chainable interface design enables automated pipelines, real-time computation, and batch processing.



S³ frames can be streamed in real time (e.g., OSC or WebSocket) and passed frame-by-frame to downstream modules.


<pre><code>
/s3/frame 0.012 441.3 -22.5 1.0 0.0 0.0
</code></pre>


     R³ computes \(\) and PR on the fly.
     C³ updates perceptual stability fields.


This architecture supports use in:


     VR/AR environments
     Generative composition engines
     Real-time performance analytics




To ensure downstream module alignment:


     All frames are timestamped with exact onset time
     Optional global clock source can synchronize external sensors (e.g., EEG, motion)
     Each module can interpolate, pad, or drop frames to maintain temporal consistency
     Maximum allowable latency for inter-module propagation: \(<\) 20 ms





<table><tr><td>|

</td><td></td><td></td></tr><tr><td>
Harmonic Field Tracking</td><td>S³ \(\) R³</td><td>Spectral components to resonance models</td></tr><tr><td>Tonal Center Estimation</td><td>S³ \(\) R³ \(\) C³</td><td>Phantom root influences TPS and NSF</td></tr><tr><td>Real-Time Attention Feedback</td><td>S³ \(\) C³</td><td>EEG alignment with spectral events</td></tr><tr><td>Audio–Visual Synchronization</td><td>S³ \(\) R³ + Unity</td><td>Spectral peaks drive color, shape, camera cues</td></tr><tr><td></td></tr></table>





      CREPE, librosa, numpy, Plotly, Unity (visual output)
      Custom harmonic analysis engine, fractional prime decomposition, Euler Tonnetz geometry
      numpy, scipy, TensorFlow (optional for neural modeling), EEG data pipelines




Each module logs:


     Frame time
     Input checksum (for verification)
     Output integrity (range checks)
     Processing time (profiling)
     Synchronization offsets


A central dashboard () allows tracking of inter-module health and alignment.



The integration of S³ with R³ and C³ establishes a vertically layered architecture:


     S³ \(\) raw perceptual primitives
     R³ \(\) harmonic structure and resonance logic
     C³ \(\) cognitive and emotional interpretation


The clear API design, standardized data structures, and optional real-time compatibility ensure that all modules communicate reliably, maintain synchronization, and can evolve independently while sharing a unified foundation.





Due to its high-resolution time–frequency modeling, microtonal accuracy, and support for large-scale audio datasets, the S³ module requires careful performance tuning. This section details the computational characteristics of each pipeline stage and outlines optimization strategies at multiple levels:


     Algorithmic
     Architectural
     Real-time constraints
     Cross-platform compatibility (desktop, embedded, Unity)





<table><tr><td>|p3.2cm|p4.2cm|

</td><td></td><td></td><td></td></tr><tr><td>
f₀ Extraction (CREPE)</td><td>Neural net inference</td><td>CPU-bound</td><td>Batch inference or GPU acceleration</td></tr><tr><td>Amplitude Estimation</td><td>RMS over frames</td><td>I/O bound</td><td>Pre-slice audio, frame caching</td></tr><tr><td>Harmonic Field Generation</td><td>Harmonic expansion per frame</td><td>Memory/compute</td><td>Vectorized array ops (NumPy)</td></tr><tr><td>Symbolic Mapping</td><td>Cents + RGB + note assignment</td><td>None (fast)</td><td>Already optimized</td></tr><tr><td>Visualization</td><td>Plotly rendering</td><td>GPU/UI bottleneck</td><td>Static output or throttled rendering</td></tr><tr><td>Unity Export</td><td>CSV generation</td><td>Disk I/O</td><td>Streamed JSON \(\) buffer cache</td></tr><tr><td></td></tr></table>




 10 ms hop (100 FPS): sufficient for most music. Adjustable to 5 ms (high accuracy) or 20 ms (fast).

 Floating point (e.g. 442.76 Hz): cent-level (\(\)0.6 px) precision. No quantization unless explicitly requested.

 Normalized between -50 dB and 0 dB. Visualization supports amplitude-dependent size and color mapping.



Assuming a 10-second clip at 100 FPS, 16 partials per frame:


     Total frames: 1,000
     Total partials: \(\)16,000
     Typical JSON size: 2–5 MB uncompressed
     In-memory size: \(\)8–12 MB (with symbolic fields)


 For long-form analysis, stream partials per segment (e.g., 100 frames) into memory, then flush.



Use NumPy for harmonic expansion:

<pre><code>
harmonics = f0 * np.arange(1, N + 1)
</code></pre>

Use list comprehensions and avoid deeply nested loops.

Batch compute cent/pitch/RGB fields per frame.

Avoid recalculating pitch class mappings if frequency hasn’t changed.






     Throttle marker size and opacity for very low dB
     Hide partials < -40 dB (optional toggle)
     Use Kaleido for static rendering instead of Orca (faster)





     Use  instead of  clones
     Use object pooling
     Export only "significant" partials (e.g., fundamental + harmonics up to -35 dB)




To enable interactive use (e.g., in VR or live audio streams):


     Use frame queues to pre-load analysis windows
     Perform \(f_0\) + harmonic expansion in a separate thread
     Use audio input ring buffers (with PyAudio or SoundDevice)


 \(<\) 50 ms end-to-end




<table><tr><td>|

</td><td></td></tr><tr><td>
Desktop (Python)</td><td>Full pipeline with CREPE + Plotly</td></tr><tr><td>Unity (C\#)</td><td>CSV import + GPU mesh render</td></tr><tr><td>Web (WebGL)</td><td>Pre-rendered  or WASM visualizer</td></tr><tr><td>Embedded (Raspberry Pi)</td><td>Use downsampled audio + partial-only export</td></tr><tr><td></td></tr></table>




S³ includes performance tracking hooks:

<pre><code>
import time
start = time.time()
# ... processing ...
print(f"Step completed in time.time() - start:.2f seconds.")
</code></pre>

Future improvement: a dashboard that reports:


     FPS throughput
     Memory usage
     Partial density over time
     Processing heatmap





     Use GPU-accelerated libraries (e.g. CuPy, TensorRT for CREPE)
     Parallel frame analysis with  or 
     Use lightweight binary formats ( or ) for JSON




S³ is designed for high fidelity and extensibility, but with attention to efficiency at each level. By combining frame-wise processing, vectorized operations, intelligent filtering, and GPU/Unity export strategies, the system remains responsive and scalable—from short musical phrases to full-length performances, from desktop analysis to embedded playback.





The S³ module is grounded in a rich body of theoretical, technical, and scientific literature. This section documents the foundational sources that inform the system’s design, including references from music theory, signal processing, psychoacoustics, and cognitive neuroscience. It also provides integration notes for each cited source, detailing how the ideas have been translated into algorithmic and computational form within S³.





 Grisey, G. (1996). 




     Rejection of abstract harmonic systems in favor of the overtone series
     Advocacy for time–frequency as a compositional space





     Partial tracking over time
     Harmonic field construction
     Overtone-based pitch logic




 Schaeffer, P. (1966). 




     Classification of sonic objects based on spectral content





     Time-framed spectral analysis
     Object-based spectral segmentation






 Bregman, A. S. (1990). 




     Stream segregation, grouping of partials by proximity





     Harmonic clustering
     Fundamental and overtone coherence tracking




 Deutsch, D. (1982). 




     Illusory pitch perception, frequency grouping





     Microtonal deviation representation
     Phantom root preparation for R³




 Moore, B. C. J. (2003). 




     Modeling auditory filters and perceptual interference





     Overtone locking zone threshold (used in OL module of R³)






 Kim, J., Salamon, J., Li, P., \& Bello, J. P. (2018). 




     Deep learning-based frame-by-frame pitch estimation





     Used for extracting \(f_0\) from raw audio at 10 ms resolution




 McFee, B., Raffel, C., Liang, D., et al. (2015). 




     Audio loading, STFT, RMS, cent mapping





     Core utility for RMS estimation, time axis construction






 Lerdahl, F., \& Jackendoff, R. (1983). 




     Cognitive modeling of grouping, metric structure





     Inspired tiered visualization (partials vs. symbolic layer)




 Tymoczko, D. (2011). 




     Mapping musical structures to geometric topologies





     Inspires geometric pitch-space design (future S³–R³ integration)




 Sethares, W. A. (2005). 




     Non-Western tuning systems and perceptual consonance





     Support for non-12TET frequencies and just intonation









     Zatorre, R. J. (2002). 
     Koelsch, S. (2011). 
     Patel, A. D. (2008). 





     Mapping music perception to cortical activity





     Informing the downstream design of C³ module
     Supporting microtemporal precision as neurologically relevant






 Plotly (open-source docs)




     High-resolution, log-frequency 2D plotting





     Used for , hover data, export to PNG/HTML




 Unity Technologies (docs)




     Mesh, prefab, and real-time rendering





     Unity receives CSV exports for real-time 3D partial landscapes





     Open Sound Control (OSC): For live audio input/output into S³ pipeline.
     VR/AR Extensions: Using Unity WebXR for immersive spectral interaction.
     EEG Integration APIs: Connecting S³ output with neural data for use in C³.




The S³ module is deeply grounded in a wide range of academic sources, and every computational decision is anchored in at least one theoretical or empirical reference. From pitch tracking to visualization, from psychoacoustics to symbolic abstraction, S³ reflects a comprehensive and scholarly synthesis of the last century of acoustical, musical, and perceptual research.





This section provides detailed technical examples, supplemental illustrations, and code snippets to support the core content of the S³ module. These resources are designed for developers, researchers, and artists seeking to extend, test, or embed S³ into larger computational or artistic environments.

Each appendix includes a fully functional code excerpt, JSON schema, visualization samples, and system integration examples.

)

<pre><code>
import crepe
import librosa
import numpy as np
import json
import sys

def extract_frequencies(audio_path, output_json, step_size=10, duration=10.0):
    y, sr = librosa.load(audio_path, sr=None, duration=duration)
    time, frequency, confidence, _ = crepe.predict(
        y, sr, step_size=step_size, model_capacity='full', viterbi=True
    )
    frames = []
    for t, f, c in zip(time, frequency, confidence):
        frames.append(
            "time": float(t),
            "partials": [
                "freq": float(f), "db": 20 * np.log10(c + 1e-6), "isFundamental": True
            ]
        )
    with open(output_json, 'w') as f:
        json.dump("frames": frames, f, indent=2)

if __name__ == "__main__":
    audio_path = sys.argv[1]
    output_json = sys.argv[2]
    extract_frequencies(audio_path, output_json)
</code></pre>

)

<pre><code>
import json
import sys

def add_harmonics(input_json, output_json, max_harmonics=16):
    with open(input_json) as f:
        data = json.load(f)
    for frame in data["frames"]:
        base_freq = next((p["freq"] for p in frame["partials"] if p.get("isFundamental")), None)
        if base_freq:
            for n in range(2, max_harmonics + 1):
                frame["partials"].append(
                    "freq": n * base_freq,
                    "db": -30,
                    "isFundamental": False
                )
    with open(output_json, 'w') as f:
        json.dump(data, f, indent=2)

if __name__ == "__main__":
    add_harmonics(sys.argv[1], sys.argv[2])
</code></pre>



<pre><code>
import math

NOTE_COLORS = [
    (255, 0, 0),       # C
    (255, 128, 0),     # D
    (255, 255, 0),     # E
    (0, 255, 0),       # F
    (0, 255, 255),     # G
    (0, 0, 255),       # A
    (128, 0, 255),     # B
]

def freq_to_rgb(freq):
    if freq <= 0:
        return (0, 0, 0)
    midi = 69 + 12 * math.log2(freq / 440.0)
    semitone = midi % 12
    anchor = [0, 2, 4, 5, 7, 9, 11]
    i = max([j for j in range(len(anchor)-1) if semitone >= anchor[j]])
    ratio = (semitone - anchor[i]) / (anchor[i+1] - anchor[i])
    r1, g1, b1 = NOTE_COLORS[i]
    r2, g2, b2 = NOTE_COLORS[(i+1) % len(NOTE_COLORS)]
    r = r1 + (r2 - r1) * ratio
    g = g1 + (g2 - g1) * ratio
    b = b1 + (b2 - b1) * ratio
    return (r / 255.0, g / 255.0, b / 255.0)
</code></pre>



<pre><code>
def freq_to_microtonal(freq):
    import math
    A4 = 440.0
    NOTES = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
    midi_float = 69 + 12 * math.log2(freq / A4)
    note_index = int(round(midi_float))
    cent_offset = int(round((midi_float - note_index) * 100))
    octave = (note_index // 12) - 1
    note = NOTES[note_index % 12]
    return f"noteoctavecent_offset:+"
</code></pre>

)

<pre><code>
import json, plotly.graph_objs as go, numpy as np

def amplitude_norm(db): return max(0.0, min(1.0, (db + 50) / 50))

def draw_s3(json_path, png_out):
    with open(json_path) as f: data = json.load(f)
    X, Y, C, S, T = [], [], [], [], []
    for frame in data["frames"]:
        t = frame["time"]
        for p in frame["partials"]:
            X.append(t)
            Y.append(p["freq"])
            amp = amplitude_norm(p["db"])
            S.append(2 + amp * 8)
            rgb = tuple(int(c * 255) for c in freq_to_rgb(p["freq"]))
            C.append(f"rgb(rgb[0],rgb[1],rgb[2])")
            T.append(f"p['freq']:.2f Hz @ p['db']:.1f dB")
    fig = go.Figure(go.Scatter(x=X, y=Y, mode='markers', marker=dict(color=C, size=S), text=T, hoverinfo='text'))
    fig.update_layout(width=3840, height=2160, yaxis=dict(type='log', title='Frequency (Hz)'), xaxis=dict(title='Time (s)'))
    fig.write_image(png_out)

# Example:
# draw_s3("partials.json", "s3_visualization.png")
</code></pre>



<pre><code>

  "type": "object",
  "properties": 
    "frames": 
      "type": "array",
      "items": 
        "type": "object",
        "properties": 
          "time":  "type": "number" ,
          "partials": 
            "type": "array",
            "items": 
              "type": "object",
              "properties": 
                "freq":  "type": "number" ,
                "db":  "type": "number" ,
                "isFundamental":  "type": "boolean" 
              ,
              "required": ["freq", "db"]
            
          
        ,
        "required": ["time", "partials"]
      
    
  

</code></pre>



Omitted in text version – included in report PDF or interactive Jupyter companion notebook.


     Spectrogram at 3840×2160 resolution
     Donut color wheel illustration
     Partial track overlays
     Unity-based 3D cube field




The  concludes its first part with complete technical references, reusable code snippets, and implementation-ready structures. These examples empower researchers, developers, and artists to reconstruct the entire pipeline, audit its logic, or embed it into larger analytical or creative ecosystems.

This foundation now fully supports the integration of R³ (Resonance-Based Relational Reasoning) and C³ (Cognitive Consonance Circuit), enabling next-generation modeling of music as a spectrum–resonance–consciousness continuum.












Music theory, for centuries, has been dominated by symbolic and categorical thinking. Chords are labeled; keys are named; progressions are prescribed. These abstractions, while elegant and effective in many stylistic contexts, fail to reflect the continuous, physical nature of acoustic phenomena and the non-discrete, probabilistic mechanisms of perception. This misalignment between how music is theorized and how it is actually heard is a foundational problem that motivates the creation of R³.

In recent decades, both scientific and artistic movements have exposed the limits of traditional pitch-class and functional harmony systems:


     , led by composers such as Gérard Grisey and Tristan Murail, focused on the actual overtone content of sound rather than idealized chords.
      (Terhardt, 1974; Plomp \& Levelt, 1965) demonstrated that perceived pitch and consonance are emergent properties of partial alignment, not of symbolic classification.
     , including frequency-following response (FFR) and brainstem phase-locking studies (cf. Bidelman et al., 2011), showed that the brain tracks periodicity and overtone structures even without conscious musical attention.
      (Doty, 2002; Sethares, 1998) provided mathematical models of tuning that highlight resonance, not abstraction, as the organizing principle of pitch space.


The convergence of these fields leads to a necessary shift in harmonic reasoning: from symbolic theory to resonance-based computation.

 is designed to model music not as a grammar of discrete signs, but as a flow of structured vibrational energy. It treats pitch as a function of spectral gravity, energy proximity, and temporal anchoring, rather than categorical labeling.

At the core of R³ lie three principles:


     \\
    \(\) They result from statistical convergence of overtones, not pre-defined labels.
    
     \\
    \(\) Harmonic coherence is not binary (consonant vs. dissonant), but a gradient, computable via \(\).
    
     \\
    \(\) Tonal stability is derived from how partials interact — in time, in frequency, and in amplitude — not from isolated entities.


This paradigm is grounded not only in music but in broader systems theory, signal processing, and neuroacoustics. The resonance field becomes the new tonal map, where perception, meaning, and emotion are drawn not on a grid of pitches, but across a topology of dynamic acoustic pressure.

As Zatorre and Salimpoor (2013) observed, musical reward correlates with prediction and violation in time-dependent structures — R³ provides the computational substrate for such dynamics. Through \(\), PR, and RFM, it becomes possible to chart the internal logic of sound as it unfolds, not as a score, but as a fluid field of tonal potential.



The motivation for R³ is not merely the refinement of harmony theory. It is the redefinition of what harmony is: no longer a symbolic artifact, but a topological, energetic, and cognitive resonance surface.



SRC⁹ is designed as a modular, multi-domain cognitive-auditory system with three principal modules:


      Spectral Sound Space – acoustic extraction, microtonal analysis
      Resonance-Based Relational Reasoning – harmonic topology, field modeling
      Cognitive Consonance Circuit – perceptual synthesis, memory, valuation


R³ sits at the exact center of this architecture — mathematically, informationally, and conceptually.

It functions as the Y-axis of the SRC⁹ lattice, where:


     X = spectral frequency space (S³)
     Y = resonant interaction space (R³)
     Z = perceptual response dimension (C³)


This triaxial topology is not metaphorical — it is computationally enforced through data routing, inter-unit feedback, and shared time/frequency schemas.



R³’s input is the fully expanded harmonic spectrum:

<pre><code>

  "time": 2.1,
  "partials": [
    
      "freq": 196.0,
      "amplitude": 0.84,
      "isFundamental": true,
      "harmonic_index": 0,
      "symbol": "G3⁰"
    ,
    ...
  ]

</code></pre>

Sampling: 0.1s frames (200 per 20s session)

Per Frame: 1 f₀ + 16 harmonics

Extras: Microtonal symbol mapping, cent deviation flags

Each R³ unit extracts features relevant to its function (e.g., PR focuses on \(f_0\) trajectory; RFM on amplitude–frequency density).



Within R³, each unit runs in temporal and spectral parallel. However, their semantic roles are different:


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>Virtual root estimation</td><td>RP, RFM, CRV</td></tr><tr><td>RPU</td><td>Framewise \</td><td>windowed \(\)</td><td>CRV, RFM</td></tr><tr><td>RFMU</td><td>Field topology (RFM, \(\)RFM)</td><td>CRV, visualization</td></tr><tr><td>CRVU</td><td>Vector summary [TPS, TFI, NSF]</td><td>C³ – attention \</td><td>memory</td></tr><tr><td></td></tr></table>


Together, these units transmute the raw spectrum into a dynamic resonance surface — a structure rich in perceptual cues, mathematical relations, and cognitive triggers.



R³’s final output — the CRV vector — is transmitted to C³ for use in:


     Attention anchoring: TPS stability scores influence focus
     Fusion response modeling: TFI reflects perceptual unity
     Temporal expectation modeling: NSF controls time-weighted relevance


These values modulate cognitive consonance curves and neural resonance gating within higher-order evaluative modules.



Though R³ receives data from S³ and passes to C³, the flow is not strictly feedforward.

R³ can also be:


     Influenced by C³ feedback, adjusting \(\) weighting or field granularity based on attention level
     Trigger of S³ re-analysis, when PR or TFI instability passes a threshold


This recurrent loop allows SRC⁹ to behave like a resonant cognitive engine — not just analyzing, but reacting to musical content dynamically.



The data flow between S³, R³, and C³ can be visualized as:

<pre><code>
             [ S³ ]  →  Raw Spectrum (partials)
               ↓
         ┌─────────────┐
         │     R³      │
         │ PR → Φ → RFM│
         └────┬────────┘
              ↓
           [ CRV ]
              ↓
            [ C³ ]
       (Attention, Valuation, Memory)
</code></pre>

Each arrow represents a JSON/array structure or vector of numerical values, all time-aligned, standardized, and validated.



R³ enables SRC⁹ to:


     Move beyond symbolic analysis
     Embrace probabilistic, topological, and energy-based models
     Bridge physics (S³) with cognition (C³)
     Operate in real-time or offline batch analysis
     Output data suitable for visualization, sonification, or interactivity (Unity)




In the architecture of SRC⁹, a domain is not merely a categorical label — it defines the epistemic framework by which musical structure is interpreted. Domains provide the foundational assumptions, theoretical orientation, and algorithmic style of all downstream unit processing.

Two such domains are currently defined within the R³ module architecture:





     Music is a symbolic grammar of signs.
     Tonality arises from hierarchical relations between named pitch classes.
     Harmony is a sequence of discrete, functional progressions (e.g., tonic \(\) dominant \(\) subdominant).




     Rameau’s fundamental bass theory
     Roman numeral analysis
     Common-practice tonality (1600–1900)
     Schenkerian structuralism




     Rule-based systems
     Symbol-to-symbol transitions
     State machines over pitch-class sets





     Unpopulated. The domain is intentionally preserved for comparative or pedagogical use but no active units are assigned.





     Incorporating classical root analysis
     Mapping Roman numeral logic to PR output
     Providing symbolic contrast to field-theoretic R³ outputs






     Music is a flow of interacting energy fields.
     Tonality emerges from the statistical convergence of partials.
     Harmony is a dynamic topology shaped by amplitude, frequency, and time.




     Psychoacoustics (Plomp \& Levelt, 1965; Terhardt, 1974)
     Spectral composition (Grisey, Murail)
     Just intonation and tuning theory (Doty, Sethares)
     Auditory neuroscience (Zatorre, Bidelman, FFR)




     Vector calculus in spectral domains
     Probabilistic detection of resonance attractors
     Topographic field modeling (RFM)
     Temporal variance and cognitive vector construction (CRV)





<table><tr><td>|p5.5cm|

</td><td></td><td></td></tr><tr><td>
Pitch Type</td><td>Discrete pitch classes</td><td>Continuous frequency space</td></tr><tr><td>Harmony Basis</td><td>Root progression (symbolic)</td><td>Spectral proximity \</td><td>convergence</td></tr><tr><td>Time Treatment</td><td>Syntactic units (measures)</td><td>Framewise + windowed resonance maps</td></tr><tr><td>Tonal Center Definition</td><td>Key, tonic function</td><td>Phantom root via overtone intersection</td></tr><tr><td>Mathematical Form</td><td>Rule-based grammar</td><td>Scalar field + differential operators</td></tr><tr><td>Cognitive Link</td><td>Abstract schema</td><td>Auditory energy alignment (FFR)</td></tr><tr><td>Adaptivity</td><td>Static system</td><td>Real-time dynamic modeling</td></tr><tr><td></td></tr></table>




SRC⁹ includes both domains to support:


     Comparative modeling
     Pedagogical clarity
     Theoretical transparency


Although the Traditional-Based Theory domain is empty, its presence allows users to compare symbolic models vs. resonance models, test different theoretical assumptions, and use R³ as both an analytical tool and a research platform.

Eventually, units such as:


     Tonic Function Classifier (TFC)
     Symbolic Root Evaluator (SRE)
     Key Probabilistic Mapper (KPM)


...may populate the Traditional domain for hybrid analysis.




<table><tr><td>
</td><td></td></tr><tr><td>
PRU</td><td>R³ – Resonance-Based</td></tr><tr><td>RPU</td><td>R³ – Resonance-Based</td></tr><tr><td>RFMU</td><td>R³ – Resonance-Based</td></tr><tr><td>CRVU</td><td>R³ – Resonance-Based</td></tr><tr><td>OL (future)</td><td>R³ – Resonance-Based</td></tr><tr><td>
</td><td>(empty)</td></tr><tr><td></td></tr></table>




The Phantom Root Unit (PRU) models one of the most perceptually paradoxical yet experientially central aspects of musical cognition: the ability to perceive a fundamental pitch even when it is not acoustically present. This unit formalizes and operationalizes the phenomenon of “virtual pitch” by detecting statistical convergence across harmonic spectra and time.



Since the 1970s, experimental psychoacoustics has demonstrated that listeners can identify the “missing fundamental” of a complex tone purely from overtone relationships (Terhardt, 1974; Houtsma \& Goldstein, 1972). These percepts are non-linear and emergent: they do not result from individual partials, but from their alignment in log-frequency space.

Auditory scene analysis research (Bregman, 1990) shows that virtual pitch perception is guided by temporal stability, harmonic simplicity, and statistical regularity. PRU unifies these dimensions using an intersection-based harmonic model, a vector-space fusion score, and symbolic microtonal tagging.

In neurophysiological terms, virtual pitch tracking is associated with brainstem-level phase locking (cf. Bidelman \& Krishnan, 2011), where neurons track periodicity of inferred tones even when the spectral content is incomplete. PRU aligns with this processing architecture.




      
      0.1s
      fundamental + 16 harmonics
      cent-deviation-based (e.g., G3⁺¹)


Partial data are first segmented into fundamental frequency sequences, grouped by \(49\) cent stability window, and then processed through pattern recognition templates.





Tracks whether the current fundamental frequency remains within a \(49\) cent band. Once a deviation exceeds threshold, a note boundary is declared.

Let \(f_n\) be the current \(f_0\) value:

\[
 | (f_n, f_n-1) | > 49  
\]

This creates time-stable pitch clusters, e.g., G2 (1.4s–2.1s), D3 (2.2s–3.3s), G3 (3.4s–4.2s)



Matches these sequential pitch groups to harmonic templates such as:


     [1, 2, 3] \(\) simple harmonic stack
     [1, 2, 3, 4, 5] \(\) extended overtone set
     [1–11] \(\) full resonance model (A4 group)


Given \(N\) consecutive note frequencies \(\f_1, f_2, , f_N\\), PRU seeks a scalar \(r\) such that:

\[
f_i  r  h_i   H = \h_1, h_2, , h_N\
\]

The best-fit \(r\) becomes the phantom root candidate.



To evaluate whether partials align at a common origin, PRU computes:

\[
(r) = _i<j \#(H_i _ H_j)
\]

Where:


     \(H_i = \k  f_i\\): harmonic set of note \(i\)
     \(_\): fuzzy intersection under cent-tolerance \(\)


This gives a resonance-weighted score of harmonic “compatibility.”



As an alternative or supplement, PRU may also compute the HIM:

\[
(a) = a + b - 1
\]

This models perceptual fusion likelihood (cf. Sethares, 1998) and allows comparing different PR candidates.



For symbolic systems like Just Intonation or Prime-Limit modeling, PRU represents each note as a vector:

\[
_i = (x_2, x_3, x_5, x_7, ),   f_i = 2^x_2  3^x_3  5^x_5 
\]

The mean vector:

\[
_PR = N _i _i
\]

...is projected back into frequency space to suggest a symbolic root in prime-vector space.



Final phantom root frequency is mapped into symbolic pitch notation:


     Cent bins: \(\)25 cent steps
     Output: , , etc.




<pre><code>

  "time_range": [2.1, 4.2],
  "phantom_root": 130.81,
  "symbol": "C3⁺¹",
  "group": "A2",
  "fusion_score": 0.431

</code></pre>

Each record corresponds to a stable perceptual segment.




      defines resonance weighting center
      sets initial attractor in field
      primary input for TPS (temporal stability)




The Resonance Potential Unit (RPU) provides a scalar measure of spectral coherence — quantifying how “tightly” the partials within a time window resonate with one another. The \(\) metric is a mathematically continuous and perceptually grounded proxy for what listeners describe as "harmonic richness," "fusion," or "tonal gravity."



Consonance perception is not binary but continuous. Listeners experience certain spectra as more stable or fused depending on how close and strong partials are in frequency and amplitude. This insight, originating from psychoacoustic studies by Plomp \& Levelt (1965), forms the perceptual core of \(\).

Further evidence from Bidelman et al. (2011) and Leman (2016) suggests that harmonic coherence triggers neural entrainment and reward systems. High \(\) values are theorized to activate neural structures such as the nucleus accumbens (associated with musical pleasure) and generate higher predictive certainty in auditory processing streams.




     Input file: 
     Frame duration: 0.1 seconds
     Each frame: list of partials (freq, amp)


All \(\) computations are frame-aligned, amplitude-weighted, and cent-aware.





For every frame \(t\), a full pairwise distance matrix is constructed across all partials:

\[
(t) = _i<j |f_i - f_j| + 
\]

Where:


     \(A_i\), \(A_j\): amplitudes
     \(f_i\), \(f_j\): frequencies
     \(\): small constant (1e⁻⁶)


This formulation ensures:


     Higher amplitude = higher resonance impact
     Smaller frequency distance = stronger spectral fusion


Each \((t)\) is a scalar representing the spectral “density” or “pull” within that frame.



To capture longer-term coherence, sliding time windows (1s, 3s, 5s, 7s) are defined:

\[
_T = _t  T (t)
\]

This models harmonic pressure across time — analogous to field density in physics.



     Detecting modulation
     Smoothing over local instability
     Feeding into NSF (memory-weighted) metrics in CRVU




Future extensions may modulate \(\) with field attractor weights (from RFM):

\[
_RFM(t) = _i<j |f_i - f_j| + 
\]

Where \(\) is a Gaussian function centered on current PR or spectral centroid.



Variance in \((t)\) over time is highly correlated with tonal anchoring. A stable tonal segment will show low variance; modulation or dissonance increases fluctuation.

\[
_(t) = N _i=1^N ( _i -  )^2
\]

This index feeds into CRVU's TPS node (Temporal Perceptual Stability).




<pre><code>

  "time": 4.1,
  "phi": 3.142

</code></pre>


<pre><code>

  "window": "6.0–9.0",
  "phi": 9.762,
  "window_size": 3

</code></pre>

Both forms are visually overlaid in multi-colored \(\) curves (e.g., red = 1s, blue = 7s).




      TPS (std dev), NSF (sum)
      used as raw data for Gaussian smoothing
      PR can shift \(\) weighting center




\(\) unifies the perceptual with the computational:


     It approximates roughness, fusion, and tension metrics
     It enables real-time scalar monitoring of harmonic convergence
     It mathematically extends Plomp–Levelt’s roughness curve to high-resolution, amplitude-weighted spectral structures




The Resonance Field Modeling Unit (RFMU) constructs a continuous, dynamic field over frequency space that represents the distribution of spectral energy and tonal gravity. It transforms discrete spectral events into a spatiotemporal topography, allowing resonance to be visualized, measured, and interpreted as a scalar field.



Traditional models define harmony as sequences of pitch-class structures. RFMU redefines harmony as field energy: the shape, slope, and peaks of a continuously distributed amplitude-weighted spectral surface.

This approach is inspired by:


     Physical fields in electromagnetism and gravity
     Topographic models of pitch space (e.g., Tymoczko, 2006)
     Neural tonotopic maps (Moore, 2012)
     Auditory Gestalt theory, where pitch centers attract auditory focus


In RFM, harmonic tension and resolution are not rules — they are gradients across a vibratory surface.




      
      list of partials (freq, amp)
      becomes a kernel on the field






\[
(f, t) = _i=1^N A_i(t)  e^-2^2
\]

Where:


     \(f\): target frequency grid point
     \(f_i(t)\): frequency of partial \(i\) at time \(t\)
     \(A_i(t)\): amplitude of partial \(i\)
     \(\): resonance spread parameter (controls field smoothness)


This Gaussian convolution converts discrete partials into a smooth frequency-density curve.



The frequency space (20–20000 Hz) is divided into \(N\) points:

\[
f_j = (20, 20000, N)
\]

\(N\) is typically 512 or 1024, log-scaled to match auditory perception.



\[
 (f, t) = (f, t) f
\]

Implemented numerically via finite differences.


     Points toward direction of tonal pull
     Magnitude of \(\)RFM is used in CRVU’s TFI node




At each time \(t\), RFMU extracts local maxima (peaks) in the field:


     These peaks = tonal centers
      = persistence of peak across frames
      = shift in peak position over time





     
    
         x = time
         y = frequency (log scale)
         color = field intensity (e.g., inferno colormap)
    
     
    
         white lines = peak paths
         vector arrows = \(\)RFM direction
    




<pre><code>

  "time": 3.5,
  "grid": [20.0, 25.1, ..., 20000.0],
  "field": [0.001, 0.0023, ..., 0.0],
  "gradient": [0.004, 0.003, ..., -0.002]

</code></pre>



RFMU makes it possible to:


     Observe resonance motion as a flow
     Detect zones of spectral convergence (tonal mass)
     Model directional pull (gradient dynamics) of harmonic tension
     Build resonance topographies for spatial cognition





     
    
         \(\)RFM \(\) TFI
         RFM stability \(\) TPS supplement
    
      Peak history \(\) implied root movement
     
    
         Field visualization \(\) VR/AR immersion
         Resonance attractors \(\) attention targets
    





      Extend RFM to include time-depth (spectrovolume)
      Use \(^2\)RFM to detect dissonance basins
      Integrate \(\)RFM into NSF decay structures




RFMU is where harmony becomes spatial. It transforms the linear structures of tonal analysis into landscapes of spectral motion — surfaces that pull, release, and define perceptual musical form.



The Cognitive Resonance Vectoring Unit (CRVU) synthesizes the outputs of R³ into a compact, perceptually-relevant vector. It provides a quantitative bridge from spectral resonance data to cognitive interpretation, enabling SRC⁹ to assess how stable, fused, and memory-relevant a sound structure is over time.

CRVU defines tonal cognition not as symbolic logic, but as a vector of resonance behaviors.



Listeners do not passively receive spectral information — they process it in terms of:


      Is the tonal center clear and persistent?
      Do the partials cohere into a unified sound?
      Does the resonance persist cognitively over time?


These questions are reflected in core neural structures:


     TPJ + ACC: involved in perceptual switching and ambiguity resolution
     Auditory cortex: phase-locking and coherence detection
     Hippocampus: short-term memory formation of tonal events
     Nucleus accumbens: reward prediction via temporal regularity


CRVU mathematically simulates these perceptual axes through three metrics: TPS, TFI, and NSF.



CRVU consumes:


     Framewise \(\) (from RPU) \(\) resonance magnitude over time
     Gradient field \(\)RFM (from RFMU) \(\) fusion and divergence flow


Each input is frame-aligned at 0.1s resolution, processed into single or windowed summary statistics.





\[
 = 1 + _(t)
\]

Computes standard deviation of \(\) over entire (or local) window.


     High TPS = consistent tonal field \(\) strong perceptual anchoring




\[
 = 1 +  | (f, t)| 
\]

Measures average steepness of resonance field across time.


     Flatter fields = stronger fusion
     Steeper slopes = divergence, spectral instability




\[
 = _t (t)  e^- t
\]

Applies exponential decay (\( = 0.05\)–\(0.1\)) to earlier \((t)\) values.


     Models short-term auditory memory + attention decay
     High NSF = early, strong, cohesive resonance = likely to be encoded





<table><tr><td>
</td><td></td><td></td></tr><tr><td>
TPSNode</td><td>\((t)\)</td><td>Scalar \( [0, 1]\)</td></tr><tr><td>TFINode</td><td>\(\)RFM\((f, t)\)</td><td>Scalar \( [0, 1]\)</td></tr><tr><td>NSFNode</td><td>\((t)\), \(\)</td><td>Scalar \( [0, 1]\)</td></tr><tr><td></td></tr></table>


Each node is independent but computed over the same time base.



<pre><code>

  "TPS": 0.812,
  "TFI": 0.693,
  "NSF": 0.0385

</code></pre>

Vector = 3-tuple \(\)stability, fusion, memory\(\)

Normalized to \([0, 1]\) for visual and cognitive mapping.




     Displayed as stacked horizontal bars (216px high)
     Red = TPS, Green = TFI, Blue = NSF
     Used as perceptual “signature” for a sound segment
     Overlayable on waveform, RFM, or symbolic score




CRVU directly feeds into C³'s interpretive layers:


      TPS modulates expectedness
      NSF correlates with phase-locking metrics
      TFI relates to absorption metrics


CRVU is the only R³ unit whose output is designed to directly map onto affective and attentional models.




     Weighted CRV vectors across musical phrases
     Real-time CRV streaming for interactive music engines
     CRV-linked generation: use resonance signature to drive AI composition
     Fusion + stability mapping across multichannel inputs (ensemble CRV)




CRVU is the cognitive mirror of R³ — a window into how sound, structured by physics and filtered by resonance, becomes psychologically meaningful.



The phenomenon of phantom root perception — the brain's ability to identify a "missing" fundamental from a group of overtones — is among the most counterintuitive findings in auditory science. Unlike direct pitch recognition, it requires a form of inferred periodicity, where the brain estimates the source of harmonic structure based solely on spectral relationships.

The PRU formalizes this cognitive mechanism using harmonic intersection and vector matching models.



The core mathematical function is:

\[
 = _f _i<j \#(H_i _ H_j)
\]

Where:


     \(H_i = \k  f_i  k   \\)
     \(_\): fuzzy intersection within a cent tolerance \(\) (typically \(49\) cents)
     \(\#\): counts the number of overlapping harmonics


 This function seeks the base frequency \(f\) whose harmonic series would generate the highest number of overtone alignments across a group of perceived pitches. Even if \(f\) is not acoustically present, it may be implied by these intersections.



To operationalize this, PRU evaluates grouped sequences (e.g., G2–D3–G3) against canonical harmonic stacks:


     Group A: [1,2,3]
     Group A1: [1–5]
     Group A2: [1–7]
     Group A3: [1–9]
     Group A4: [1–11]


Given note sequence \( = [f_1, f_2, ..., f_n]\), we search for base frequency \(r\) such that:

\[
f_i  r  h_i   i, h_i  H
\]

Where \(H\) is the harmonic template.

A candidate is accepted if average error:

\[
_ = n _i | r  h_i | < 
\]

(Default: \( = 0.03\), i.e., 3\% deviation)



To further differentiate candidates, a perceptual fusion metric is computed:

\[
(a) = a + b - 1
\]

As introduced in Sethares (1998), this metric estimates how well a ratio \(b\) supports tonal fusion. Lower denominators and lower sums produce higher HIM values — signaling simpler, more consonant ratios.




     3:2 (perfect fifth): HIM = \(4/6 = 0.667\)
     7:4 (septimal minor 7th): HIM = \(10/28  0.357\)




For advanced systems supporting symbolic pitch spaces (e.g., Just Intonation), each pitch is expressed as a vector:

\[
_i = (x_2, x_3, x_5, x_7, )   f_i = 2^x_2  3^x_3  5^x_5 
\]

The mean vector:

\[
_PR = N _i=1^N _i
\]

...is mapped back to a rational frequency. This method allows geometric averaging of complex harmonic ratios and facilitates field-aware root finding.



Crucially, PR is not computed frame-by-frame but across time-stable pitch sequences. This enables it to:


     Capture phrasing-based tonal centers
     Filter out transient modulations
     Model tonality as a temporally weighted attractor




New group starts when \(f_0\) deviates \(> 49\) cents from previous.

 [G2] \(\) [D3] \(\) [G3] \(\) matched to [1,2,3] = PR: G1



Each PR record includes:


      [start, end] of stable group
      root frequency in Hz
      user-defined microtonal pitch label (e.g., D3⁺²)
      matching harmonic template label (e.g., A2)
      optional metric from HIM or harmonic count




The Resonance Potential (\(\)) is the fundamental scalar measure of harmonic coherence within the R³ framework. It captures how energetically close — and thus perceptually “fused” — a group of partials are at a given moment. Unlike symbolic harmonic functions (e.g., tonic, dominant), \(\) offers a mathematically continuous, spectrally grounded, and amplitude-sensitive metric for tonal tightness.



The main formulation of \(\) is defined over all pairwise combinations of partials within a time slice:

\[
(t) = _i<j |f_i(t) - f_j(t)| + 
\]

Where:


     \(A_i(t), A_j(t)\): amplitudes of partials at time \(t\)
     \(f_i(t), f_j(t)\): their frequencies
     \(\): a small regularization constant (e.g., \(1e^-6\)) to prevent division by zero


This equation models:


     Higher amplitude \(\) greater resonance contribution
     Closer frequencies \(\) stronger spectral fusion
     Denser clusters \(\) higher perceptual cohesion




\(\) generalizes earlier psychoacoustic roughness models (e.g., Plomp \& Levelt, 1965), replacing frequency ratios with physical frequencies and amplitude scaling.

It corresponds to perceptual phenomena such as:


     Tonal fusion
     Consonance gradience
     Spectral “weight” of a sound structure


EEG studies (Bidelman et al., 2011) suggest that high harmonic coherence triggers stronger FFR synchrony — supporting \(\) as a proxy for perceived resonance strength.



To move from instantaneous coherence to temporal resonance modeling, \(\) is accumulated across time windows:

\[
_T = _t=t_0^t_1 (t)
\]

Where:


     \(T\) = time window (e.g., 1s, 3s, 5s, 7s)
     Frames sampled at 0.1s \(\) \(_T\) includes 10–70 values


This windowed \(_T\) models:


     Tonal momentum (field pressure)
     Stability regions (high and flat \(_T\))
     Modulation zones (\(_T\) dips or spikes)




\(\) can be viewed as an inverse spectral entropy measure.


     A spectrum with many equally spaced partials \(\) lower \(\)
     A tightly clustered, loud spectrum \(\) higher \(\)


\(\) is therefore analogous to a negative KL divergence between energy distributions.

This link allows R³ to potentially connect with probabilistic models of expectation and surprise (e.g., Huron’s , 2006).





Later harmonics receive less weight:

\[
A_i^* = 1 + h_i
\]



Include field density around a pitch center (e.g., PR):

\[
_(t) = _i<j 2^2|f_i - f_j| + 
\]

Where \(\) is the perceptual center of gravity.





<pre><code>
 "time": 3.2, "phi": 2.831 
</code></pre>



<pre><code>
 "window": "5.0–8.0", "phi": 9.183, "window_size": 3 
</code></pre>

Each frame or window can be directly plotted as a \((t)\) curve or used for comparative analysis.




     
    
         TPS = \(()\)
         NSF = weighted \(\) sum
    
      Used to generate field intensity
      As raw resonance potential data for affective modeling




The RFM function generates a scalar field over the frequency domain at each time point, representing the density and distribution of harmonic energy. It transforms a list of discrete partials into a smooth, continuous resonance map — providing a foundation for spatially-aware tonal reasoning.

Whereas \(\) quantifies total harmonic coherence within a frame, RFM visualizes how that resonance is distributed across the pitch spectrum — forming a field of tonal gravity.



The resonance field at time \(t\), over frequency coordinate \(f\), is computed as:

\[
(f, t) = _i=1^N A_i(t)  e^-2^2
\]

Where:


     \(f\): continuous frequency grid point
     \(f_i(t)\): frequency of \(i\)-th partial at time \(t\)
     \(A_i(t)\): amplitude of \(i\)-th partial
     \(\): spread parameter (resonance width)


This is a Gaussian kernel density estimator, where each partial casts a resonance hill over frequency space.



To build RFM numerically, a discrete frequency grid is defined:

\[
F = \ f_1, f_2, ..., f_n \ = (20, 20000, N)
\]


     \(N = 512\) or \(1024\) (typical values)
     Logarithmic scaling reflects cochlear frequency mapping


Each grid point will hold one \((f, t)\) value.\\
Result: a 2D matrix where each row = time slice, each column = frequency bin.



RFM approximates the perceptual landscape of sound:


      = tonal centers or attractors
      = spectral gaps or anti-resonance zones
      = tonal pull
      = harmonic spread


Musically, RFM enables analysis of:


     Tonal convergence and divergence
     Modulation zones (shifting attractors)
     Multi-center textures (polytonality)
     Voice-leading through field movement




To extract perceptual “direction,” RFM computes its gradient:

\[
 (f, t) = (f, t) f
\]


     This is discretized via finite differences on the grid
     High \(\)RFM \(\) rapid spectral change \(\) dissonance, instability
     Low \(\)RFM \(\) smooth flow \(\) stability, fusion


Gradient magnitude is used in CRVU’s TFI metric.



Local maxima in RFM indicate momentary tonal centers.

Let:

\[
f_p(t)  F     = 0,   < 0
\]

Tracking \(f_p(t)\) over time forms a tonal trajectory or attractor path.


     Field segmentation methods (e.g., watershed or ridge detection) can be applied for higher-level grouping





     X-axis = time (0–20s)
     Y-axis = log frequency (20–20kHz)
     Color = field intensity (resonance strength)
     Overlay = vector arrows from \(\)RFM or contour lines for attractors


This map becomes the visual body of tonal behavior over time.



<pre><code>

  "time": 3.7,
  "grid": [20.0, 24.3, ..., 20000.0],
  "field": [0.001, 0.005, ..., 0.0],
  "gradient": [0.002, -0.001, ..., -0.003]

</code></pre>

Each frame produces a scalar field vector and optional gradient vector.



RFM draws from:


     Spectrogram theory: smoothed representation of energy over time/frequency
     Field theory (physics): scalar potential fields
     Tonnetz spaces: extended to real-valued, log-frequency domains
     Auditory cortex modeling: tonotopic fields + lateral inhibition





     CRVU \(\) TFI: average \(\)RFM magnitude
     Modulation analysis: movement of peaks
     Polycentricity: multi-peak stability across frames
     VR/Unity visual grounding: resonance fields as terrain surfaces




Cognitive perception of harmony is not based on static symbols, but on dynamic acoustic behavior: how stable, unified, and memorable a sound feels over time. The CRVU summarizes this behavior through three scalar metrics: Temporal Perceptual Stability (TPS), Tonal Fusion Index (TFI), and Neural Synchronization Field (NSF).

These metrics operate as projections of resonance into perceptual space — each compressing a dimension of resonance behavior into a scalar value \( [0, 1]\).




\[
 = 1 + _(t)
\]

Where:


     \((t)\): framewise resonance potential
     \(_(t)\): standard deviation across full or local time window





     High TPS \(\) consistent \(\) \(\) stable resonance center
     Low TPS \(\) fluctuating \(\) \(\) modulation, instability


 Temporal regularity correlates with attentional focus and pitch certainty (cf. Leman, 2016). TPS models tonal anchoring as experienced in both classical and non-tonal contexts.




\[
 = 1 +  | (f, t)| 
\]

Where:


     \( (f, t)\): gradient of the resonance field at time \(t\)
     \(  \): average over frequency domain and time frames





     High TFI \(\) smooth field \(\) tight spectral coherence
     Low TFI \(\) jagged field \(\) spectral diffusion


 Auditory cortex entrains more strongly to spectrally fused sounds. Fusion models correlate with gamma coherence, phase-locking, and sound object formation (cf. Bidelman et al., 2014).




\[
 = _t (t)  e^- t
\]

Where:


     \(\): decay coefficient (0.05–0.1 typical)





     High NSF \(\) strong early resonance \(\) likely encoding into short-term memory
     Low NSF \(\) delayed or inconsistent resonance \(\) weaker impression


 NSF captures the recency effect of musical perception — the brain’s tendency to weight earlier salient events more heavily in expectation and evaluation processes (cf. Zatorre \& Salimpoor, 2013).



The full cognitive resonance signature is a vector:

\[
 = [, , ]
\]

Each value is:


     Normalized \( [0, 1]\)
     Interpretable individually
     Composable into weighted salience models





      feeds attention allocation, memory modeling, immersion scores
     
      changes in CRV may mark structural transitions
      use CRV to guide harmonic generation toward cognitive targets





<pre><code>

  "TPS": 0.842,
  "TFI": 0.713,
  "NSF": 0.0362

</code></pre>

Visualized as three stacked bars (R/G/B), overlaid on waveform or resonance map.



CRV is the cognitive endpoint of R³: a compact, interpretable summary of how a given sound structure will likely be experienced, memorized, and evaluated by a human listener.



The structural integrity of R³ depends not only on its theoretical formulations, but also on the consistency, extensibility, and interpretability of its data output formats. All R³ units generate machine-readable, human-interpretable, and visualization-ready files. These files follow a strict temporal alignment and a modular format architecture.




<table><tr><td>
</td><td></td></tr><tr><td>
Total Duration</td><td>20.0 seconds</td></tr><tr><td>Frame Rate</td><td>0.1 seconds</td></tr><tr><td>Total Frames</td><td>200</td></tr><tr><td>Fundamental + Harmonics</td><td>1 + 16</td></tr><tr><td></td></tr></table>


Each frame is timestamped and encapsulates a complete harmonic snapshot.



<pre><code>

  "time": 3.2,
  "partials": [
    
      "freq": 261.63,
      "amplitude": 0.81,
      "isFundamental": true,
      "harmonic_index": 0,
      "symbol": "C4⁰"
    ,
    
      "freq": 523.25,
      "amplitude": 0.42,
      "harmonic_index": 1,
      "isFundamental": false,
      "symbol": "C5⁰"
    
  ]

</code></pre>

This format is unit-agnostic and powers all R³ modules.




<pre><code>

  "time_range": [2.1, 4.3],
  "phantom_root": 130.81,
  "symbol": "C3⁺¹",
  "group": "A2",
  "fusion_score": 0.42

</code></pre>

One record per detected PR segment, with group-matched harmonic stack label and symbolic pitch.



Framewise:
<pre><code>
 "time": 5.2, "phi": 3.714 
</code></pre>

Windowed:
<pre><code>
 "window": "5.0–8.0", "phi": 9.23, "window_size": 3 
</code></pre>

Both datasets can be plotted as continuous \(\) curves, with optional variance indicators.


<pre><code>

  "time": 7.1,
  "grid": [20.0, 24.1, ..., 20000.0],
  "field": [0.0012, 0.0044, ..., 0.0],
  "gradient": [0.0004, -0.0003, ..., -0.0021]

</code></pre>


      = scalar intensity at each frequency point
      = \(\)RFM used for TFI



<pre><code>

  "TPS": 0.843,
  "TFI": 0.702,
  "NSF": 0.0361

</code></pre>

Single vector summarizing entire input segment’s resonance dynamics.




<table><tr><td>
</td><td></td></tr><tr><td>
Raw input</td><td></td></tr><tr><td>PR segment</td><td></td></tr><tr><td>RP framewise</td><td></td></tr><tr><td>RP windowed</td><td></td></tr><tr><td>Field maps</td><td></td></tr><tr><td>Cognitive</td><td></td></tr><tr><td></td></tr></table>


All files are written to  subdirectories, with script-driven generation.




<table><tr><td>
</td><td></td><td></td><td></td></tr><tr><td>
PRU</td><td></td><td>PNG</td><td>3840 × 216</td></tr><tr><td>RPU</td><td></td><td>PNG</td><td>3840 × 216</td></tr><tr><td>RFMU</td><td></td><td>PNG</td><td>3840 × 216</td></tr><tr><td>CRVU</td><td></td><td>PNG</td><td>3840 × 216</td></tr><tr><td>Master</td><td></td><td>HTML</td><td>3840 × 2160</td></tr><tr><td></td></tr></table>


Visuals use:


     Plotly for HTML interactive
     Matplotlib for static export
     Log-scale y-axis for frequency mapping
     Dark mode with frequency-hue colorization




<pre><code>
time,freq,amplitude,isFundamental,harmonic_index,symbol
</code></pre>




      to populate 
      to map x (time), y (log freq), size (amplitude)
     Visualized in 3D scene using prefabs, color shaders, and optional PR overlays





     OL-unit output (locking events)
     Symbolic export to MusicXML (planned)
     Annotated resonance flows for interactive learning




The R³ module is implemented as a fully modular, automatable pipeline. Its entire analytical process — from raw spectral input to visual output and Unity export — can be executed via a single orchestration script. This design ensures reproducibility, clarity, and efficient development.



All scripts in R³ are written in Python and follow a unit-modular standard:


     Each unit has:
    
         One analysis script \(\) produces 
         One visualization script \(\) produces 
    
     All units share a common input file: 


This architecture supports both:


     Independent execution (for testing/debugging)
     Sequential batch runs (via automation script)




This script executes the full R³ pipeline in order:


<pre><code>
[
  "PR_unit_temporal.py",
  "RP_unit_combined.py",
  "RFM_unit_analysis.py",
  "CRV_unit_analysis.py",
  "visualize_PR_temporal.py",
  "visualize_RP_unit.py",
  "visualize_RFM_unit.py",
  "visualize_CRV_unit.py",
  "visualize_overlay_all.py"
]
</code></pre>

Each entry is executed via:

<pre><code>
subprocess.run(["python", script], check=True)
</code></pre>

If any step fails, the pipeline halts — ensuring fail-fast validation.






     Python 3.9+
     Libraries: , , , , , , 
     Virtual environment: 
     Scripts are path-relative and designed for cross-platform compatibility (macOS, Linux, Windows)





<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>RawSpectrum</td><td>PR-unit-temporal.json</td></tr><tr><td>RPU</td><td>RawSpectrum</td><td>RP-framewise, RP-window</td></tr><tr><td>RFMU</td><td>RawSpectrum</td><td>RFM-unit.json</td></tr><tr><td>CRVU</td><td>RPU + RFMU outputs</td><td>CRV-unit.json</td></tr><tr><td></td></tr></table>


All outputs are time-aligned (0.1s resolution) and normalized where needed.




<pre><code>
/scripts/
├── PR_unit_temporal.py
├── RP_unit_combined.py
├── RFM_unit_analysis.py
├── CRV_unit_analysis.py
├── visualize_*.py
└── run_R3_pipeline.py
</code></pre>


<pre><code>
/data/
└── output/
    ├── PR/
    ├── RP/
    ├── RFM/
    ├── CRV/
    └── raw/RawSpectrum-unit.json
</code></pre>


<pre><code>
/output/
├── *.png
└── R3-overlay.html
</code></pre>




<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>\(\)2 seconds</td><td>\(\)1.5 seconds</td></tr><tr><td>RPU</td><td>\(\)4 seconds</td><td>\(\)2 seconds</td></tr><tr><td>RFMU</td><td>\(\)5 seconds</td><td>\(\)3 seconds</td></tr><tr><td>CRVU</td><td>\(\)1 second</td><td>\(\)1 second</td></tr><tr><td>Overlay</td><td>—</td><td>\(\)3–5 seconds</td></tr><tr><td></td></tr></table>


 \(<\) 20 seconds



Each script includes:


     
     
      wrappers with error logging
     Optional: logging to file (), runtime profiling, unit test suites (planned)







     Hook into live CREPE output stream
     Frame-by-frame analysis and accumulation
     Unity/VR feedback loop using CRV in real-time


This will require conversion of R³ modules to stream-safe, low-latency processes (e.g., via NumPy Live, C++, or Python async/generator pattern).



Visualization is a core dimension of R³’s design philosophy. It is not merely a presentation layer, but a cognitive interface — converting dense spectral data into visually interpretable resonance structures. Each R³ unit contributes a semantically encoded visual layer aligned across a global time-frequency plane.

The goal is to present harmony not as static notation, but as a dynamic topology of vibratory interaction.






     All plots align to a common x-axis (0–20s)
     Frame resolution = 0.1s
     Windowed overlays align precisely with frame start times





<table><tr><td>
</td><td></td></tr><tr><td>
Frequency</td><td>Y-axis (log scale)</td></tr><tr><td>Amplitude</td><td>Marker size / line thickness</td></tr><tr><td>\(\)</td><td>Y-axis (RPU layer)</td></tr><tr><td>Partial role</td><td>Color (e.g., fundamental = red)</td></tr><tr><td>Field strength</td><td>Color density (RFMU)</td></tr><tr><td>CRV metrics</td><td>R/G/B bar mapping</td></tr><tr><td></td></tr></table>





     Each unit occupies 216px vertical space
     RawSpectrum layer = 1080px (reference base)
     Total image = 3840 × 2160 (4K full overlay)







      square or circle
      frequency class (HSV or pitch-mapped palette)
      amplitude
      harmonic index scaled
      log(frequency)
      time slice index for animation or Unity rendering
      Plotly’s  for high-speed rendering





      red horizontal bars
      PR frequency
      symbolic pitch (e.g., C3⁺²)
      width of perceptual root duration
      can be color-coded (A, A1, A2, …)





      thin gray line (baseline)
      colored overlays:
    
         1s = red
         3s = orange
         5s = green
         7s = blue
    
      \(\) value (scalar resonance density)
      time





      2D heatmap
      time
      frequency (log)
      RFM\((f, t)\) field strength (e.g., inferno, magma)
     
    
         white peak paths
         vector arrows (\(\)RFM)
         field contour lines
    





      stacked bars
      TPS
      TFI
      NSF
      numeric values (0.000–1.000)
      not used (bar only)


This layer acts as the summary strip, linking resonance data to perceptual metrics.



Final full overlay is generated using . It combines:


     5 unit layers (216 px each)
     1 RawSpectrum base layer (1080 px)
     Common time axis
     Global dark mode for color clarity
      interactive, 4K resolution




Each unit also produces a  file:


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>PR-unit.png</td><td>3840 × 216 px</td></tr><tr><td>RPU</td><td>RP-unit.png</td><td>3840 × 216 px</td></tr><tr><td>RFMU</td><td>RFM-unit.png</td><td>3840 × 216 px</td></tr><tr><td>CRVU</td><td>CRV-unit.png</td><td>3840 × 216 px</td></tr><tr><td>Master</td><td>R3-overlay.html</td><td>3840 × 2160 px</td></tr><tr><td></td></tr></table>


These exports allow both modular inspection and publication-level usage.



Visual layers are linked to Unity via:


     Prefab scaling (amplitude)
     Z-depth encoding (harmonic index)
     Dynamic camera tracking of PRU or RFM peaks
     CRV bar overlays as HUDs in 3D scenes





     PR trail =  path
     RFM = surface terrain with \(\)-based displacement
     CRV = color modulation of environment




R³ visual outputs aim to:


     Replace static notation with spectral cartography
     Encode mathematical depth in intuitive visuals
     Make resonance not only computable — but seeable




The Unity integration of R³ transforms resonance data from abstract mathematical structures into a spatial, interactive, and visual environment. This enables researchers, musicians, and users to walk through, see, and interact with spectral and harmonic structures — making resonance literally visible.

Unity is used not just as a renderer, but as a cognitive translation platform: it visualizes how frequencies resonate, how roots shift, how fields flow — in real time or through immersive playback.



Although R³ internally uses  for maximum flexibility, Unity consumes data as  via its lightweight, line-based loading mechanisms.

  \(\) converted to:

 

<pre><code>
time,freq,amplitude,isFundamental,harmonic_index,symbol
1.1,196.0,0.82,True,0,G3⁰
1.1,392.0,0.42,False,1,G4⁰
...
</code></pre>

Each line represents one partial (including harmonics), with symbolic encoding for microtonal interpretation.





<pre><code>
[System.Serializable]
public class Partial 
    public float time;
    public float freq;
    public float amplitude;
    public bool isFundamental;
    public int harmonic_index;
    public string symbol;

</code></pre>



<pre><code>
public List<Partial> partials;

void LoadCSV(TextAsset file) 
    string[] lines = file.text.Split('');
    for (int i = 1; i < lines.Length; i++) 
        string[] values = lines[i].Split(',');
        Partial p = new Partial();
        p.time = float.Parse(values[0]);
        p.freq = float.Parse(values[1]);
        p.amplitude = float.Parse(values[2]);
        p.isFundamental = values[3] == "True";
        p.harmonic_index = int.Parse(values[4]);
        p.symbol = values[5];
        partials.Add(p);
    

</code></pre>




<table><tr><td>
</td><td></td></tr><tr><td>
X</td><td>time (horizontal progression)</td></tr><tr><td>Y</td><td>log(freq) (vertical placement)</td></tr><tr><td>Z</td><td>harmonic index (depth, optional)</td></tr><tr><td>Size</td><td>amplitude (object scale)</td></tr><tr><td>Color</td><td>frequency (HSV hue-based mapping)</td></tr><tr><td></td></tr></table>


Each partial = a colored glowing sphere.


     Stronger harmonics = larger/brighter objects
     Fundamental = red core; others vary by frequency




  (Sphere with Unlit Shader)




     Emission Color = mapped hue
     Scale = amplitude × scalar
     Tag = Fundamental / Harmonic


Optional shader features:


     Pulse = temporal dynamics
     Glow = amplitude modulation
     Flicker = instability (if \(\) is low)





     Unity’s  aligns playback with partial spawning
     Optional: timeline scrubber
     Scene camera can track:
    
         PR path (via )
         Field peak in RFM (via surface mesh)
    





<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>Red line sweep</td><td>LineRenderer along PR freq</td></tr><tr><td>RPU</td><td>Height curve</td><td>Dynamic plot (\(\) over time)</td></tr><tr><td>RFMU</td><td>Mesh surface</td><td>Terrain object from </td></tr><tr><td>CRVU</td><td>HUD bar graph</td><td>UI Panel with TPS, TFI, NSF</td></tr><tr><td></td></tr></table>


Additional interaction options:


     Filter by group (A2, A3, ...)
     Highlight tonal drift zones
     Switch between symbolic and spectral views





     Link Unity’s  to visual spawning
     Synchronize resonance events with real sound
     Use amplitude thresholding to trim non-audible points
     Optional: real-time \(\) modulator \(\) dynamically warp terrain or brightness





     Object pooling (for partials)
     Async CSV loading (for large datasets)
     GPU instancing for visual particles
     Log-space Y-axis prevents vertical crowding




The Unity implementation allows users to:


     Step through harmonic space
     See tonality emerge and dissolve
     Hear resonance while seeing its structure
     Manipulate partials and watch CRV change live


Use cases include:


     Education (teaching tonal centers)
     VR concert staging
     Interactive composition
     Research on tonotopic attention in motion




The Cursor AI platform serves as the interactive, explorable knowledge interface of the SRC⁹ system. All R³ content — scientific explanations, equations, visualizations, and output samples — are embedded within Cursor's domain–unit–node hierarchy, providing a seamless gateway between theory, data, and cognitive navigation.



R³ exists as a dedicated modular domain within the Cursor site structure:


      
      Resonance-Based Relational Reasoning
      Gateway page introducing the theory, architecture, and units of R³





     Scientific overview
     Mathematical core (\(\), PR, RFM, CRV equations)
     S³ \(\) R³ \(\) C³ flowchart
     Unit summary table
     Domain toggle menu (vs. Traditional-Based Theory)


Users can explore individual units by clicking cards linking to their respective pages.



Each R³ unit (PRU, RPU, RFMU, CRVU) has a standalone interactive document:


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td></td><td>Phantom Root Unit</td></tr><tr><td>RPU</td><td></td><td>Resonance Potential Unit (\(\))</td></tr><tr><td>RFMU</td><td></td><td>Resonance Field Modeling Unit</td></tr><tr><td>CRVU</td><td></td><td>Cognitive Resonance Vectoring</td></tr><tr><td></td></tr></table>


Each unit page includes:


     Scientific Function
     Mathematical Foundation (LaTeX supported)
     Node Architecture Table
     Sample Output (JSON snippet)
     Visualization Preview (216px PNG)
     Integration pathways (to C³ or back to S³)




Each unit page has expandable  components for its node definitions.



<pre><code>
<details><summary>GroupMatcher</summary>
Matches sequences of stable f₀ segments to harmonic template stacks like [1,2,3] or [1–11].
</details>
</code></pre>

This allows deep structure without visual clutter.

Nodes are cross-linkable and potentially host their own  pages in future iterations.



Every unit’s visualization is embedded via:


     Inline PNG
     Collapsible  blocks
     Optional Plotly iframe (HTML interactive graphs)



<table><tr><td>
</td><td></td><td></td></tr><tr><td>
PRU</td><td>bar + label plot</td><td></td></tr><tr><td>RPU</td><td>\(\) overlay curves</td><td></td></tr><tr><td>RFMU</td><td>Heatmap grid</td><td></td></tr><tr><td>CRVU</td><td>RGB bars</td><td></td></tr><tr><td></td></tr></table>


The full overlay () may be shown in a dedicated interactive gallery.



Each unit page includes a reference sidebar linking:


      
      e.g., PRU links to RPU
      CRVU \(\) CTU, NSU, PIU


Additionally, source references are hyperlinked inline (e.g., Zatorre et al., 2013).



The dual-domain system is shown via a toggle interface:

<pre><code>
[ �� R³ Resonance Theory ] [ ⚪ Traditional Theory ]
</code></pre>

Currently, Traditional Theory domain is empty — shown as inactive but present.

Future units may populate this view for contrastive analysis.



Each unit page includes:


     JSON sample snippets
     Direct download link ()
     Code preview block (e.g., Python )


Cursor supports syntax-highlighted code and LaTeX-based equations in parallel.



Cursor’s R³ structure enables:


     Progressive disclosure (unit \(\) node \(\) formula)
     Citation-based expansion
     Interactive concept comparison
     Cross-disciplinary accessibility


Users can enter from abstract, scroll into algorithm, and emerge with conceptual clarity.



R³ presents a robust, fully functional resonance analysis framework. Yet, like any scientific system, it operates within a set of defined constraints and assumptions. As the system matures, both its epistemic foundation and computational scope invite further exploration.

The following questions and proposed future extensions represent frontiers, not failures — theoretical edges where new forms of resonance reasoning, perceptual modeling, and interactivity may emerge.



  
There is currently no canonical mapping from symbolic harmony (e.g., “C major”, “G7”) to resonance field structures.

  
Can classical chord symbols be translated into predictable RFM patterns or CRV signatures?



     Construct resonance fingerprints for chord classes
     Reverse-map RFM peaks to symbolic root-inversion labels
     Apply symbolic labeling to R³ field outputs for hybrid navigation


  
Enable bidirectional harmony interpretation — symbolic \(\) resonance



  
R³ is currently batch-processed from fixed input (offline mode)

  
Can PRU, RPU, RFMU, and CRVU operate on live streamed data at frame-rate (0.1s or faster)?



     Use async generators or NumPy Live for buffer streaming
     Convert  generation into audio listener \(\) CREPE \(\) partial emitter \(\) unit executor
     Port core algorithms (\(\), RFM) to C++/CUDA for low-latency computation


  
Enable live visualization, performance-driven analysis, and generative harmony via real-time feedback loops.



  
Current PRU logic assumes monophonic fundamental tracking

  
Can PRU extract multiple phantom roots simultaneously — modeling polycentric tonality?



     Implement time-overlapping root group tracking
     Use spectral clustering to separate multiple root flows
     Model each root’s gravitational zone in RFM separately


  
Capture layered tonality and its interaction in complex textures.



  
R³ is designed for generalized perceptual inference — not individual neural profiles

  
Can CRVU metrics be personalized based on neural data (e.g., EEG), musical background, or auditory profile?



     Collect listener-specific FFR or ERP responses to stimulus sets
     Train models to predict TPS/TFI/NSF weightings per profile
     Tune resonance field weighting dynamically in response to engagement metrics


  
Model resonance-perception diversity, and adapt analysis per listener.



  
Field modeling currently operates in linear frequency space

  
What happens when RFM is computed in prime-vector space (e.g., 5-limit, 11-limit)?



     Encode partials as vectors \( = (x_2, x_3, x_5, )\)
     Define RFM in log-geometry of prime-lattice
     Extend \(\)RFM to multi-axis slope computation


  
Enable symbolically grounded resonance maps with real-number continuity



  
Current generative AI models are melody/chord/beat based

  
Can an AI compose music guided purely by RFM field shape and CRV evolution?



     Define target RFM \(\) search partials to generate matching field
     Use \(\) targets to constrain harmonic grammar
     Tune CRV vector toward affective intent (e.g., high TPS \(\) stability)


  
Create music from resonance, not just producing resonance from music.



R³ challenges centuries-old assumptions about musical structure:


     That tonality is symbolic
     That function is rule-based
     That perception is discrete


But if resonance is continuous, embodied, and cognitive, then:


     What is a “note”?
     Where is the boundary between sound and structure?
     Can harmony exist without symbols — only through flow?


These are not technical questions — they are conceptual invitations.



This section consolidates all formal references, system definitions, microtonal encodings, data format standards, and mathematical mappings used throughout the R³ module. It serves as both a technical appendix and a citation-ready bibliography for researchers, developers, and composers.





     Terhardt, E. (1974). “Pitch, consonance, and harmony.” 
     Plomp, R. \& Levelt, W. (1965). “Tonal consonance and critical bandwidth.”
     Bregman, A. (1990). 
     Bidelman, G.M. et al. (2011). “Brainstem pitch encoding.”
     Zatorre, R. \& Salimpoor, V. (2013). “Prediction and reward in music.” 
     Moore, B. (2012). 




     Sethares, W. (1998). 
     Tymoczko, D. (2006). “The Geometry of Musical Chords.” 
     Doty, D. (2002). 
     Huron, D. (2006). 
     Parncutt, R. (1989). 
     Roederer, J.G. (2008). 




     Shannon, C.E. (1948). “A Mathematical Theory of Communication.”
     Mallat, S. (2009). 
     Sethares, W. (2005). “Spectral Convergence and Dissonance.” 




The R³ module uses a symbolic pitch notation system that encodes:


     Pitch class
     Octave number
     Microtonal deviation in cents (rounded to \(\)25c steps)



<pre><code>
G2⁺¹  →  Pitch: G, Octave: 2, +25 cents deviation
C4⁻²  →  Pitch: C, Octave: 4, –50 cents deviation
</code></pre>



     ⁰ = no deviation
     ⁺¹, ⁺², ⁻¹, ⁻² = \(\)25c, \(\)50c, etc.


This system aligns with symbolic notation while reflecting real spectral deviations.




<table><tr><td>|

</td><td></td><td></td></tr><tr><td>
PRU</td><td></td><td>Phantom root estimations + groups</td></tr><tr><td>RPU</td><td>, </td><td>\(\) metrics per frame/window</td></tr><tr><td>RFMU</td><td></td><td>Resonance field grid (\(f  t\))</td></tr><tr><td>CRVU</td><td></td><td>Cognitive vector [TPS, TFI, NSF]</td></tr><tr><td></td></tr></table>


All files are time-aligned (0.1s resolution), normalized, and UTF-8 encoded.



<pre><code>
time,freq,amplitude,isFundamental,harmonic_index,symbol
</code></pre>

Used in ,  — mapped to Unity object parameters for real-time 3D rendering.




<table><tr><td>
</td><td></td><td></td></tr><tr><td>
time</td><td>X-axis</td><td>horizontal flow</td></tr><tr><td>log(freq)</td><td>Y-axis</td><td>vertical tonal position</td></tr><tr><td>amplitude</td><td>Scale</td><td>object size / brightness</td></tr><tr><td>harmonic\_index</td><td>Z-axis (optional)</td><td>depth layering</td></tr><tr><td>symbol</td><td>UI label</td><td>displayed on HUDs</td></tr><tr><td></td></tr></table>





<table><tr><td>|

</td><td></td><td></td></tr><tr><td>
CentTracker</td><td>PRU</td><td>Segment \(f_0\) sequences by cent deviation</td></tr><tr><td>GroupMatcher</td><td>PRU</td><td>Match sequences to harmonic stacks</td></tr><tr><td>PairwisePhi</td><td>RPU</td><td>Compute \(\) across partials</td></tr><tr><td>WindowIntegrator</td><td>RPU</td><td>Accumulate \(\) in time windows</td></tr><tr><td>FieldGenerator</td><td>RFMU</td><td>Create Gaussian resonance fields</td></tr><tr><td>GradientScanner</td><td>RFMU</td><td>Compute \(\)RFM across frequency space</td></tr><tr><td>TPSNode</td><td>CRVU</td><td>Variance tracker of \(\) \(\) perceptual stability</td></tr><tr><td>TFINode</td><td>CRVU</td><td>Field slope magnitude \(\) tonal fusion</td></tr><tr><td>NSFNode</td><td>CRVU</td><td>Memory-weighted \(\) integral \(\) neural salience</td></tr><tr><td></td></tr></table>





      scalar harmonic coherence
      inferred tonal anchor
      resonance topography
      perceptual signature




All R³ code and structure is:


     Open source under MIT license (default)
     Freely distributable for research, educational, and creative use
     Citable with proper attribution: 




R³ unites mathematical rigor, perceptual truth, and computational clarity into a single resonance engine. This appendix stands as the foundation for collaborative development, academic referencing, and future expansion.














     What is C³? What is cognitive resonance?
     The position of C³ within the music–mind–neurophysiology triad
     The role of C³ in SRC⁹: S³–R³–C³ triple integration





     Interdisciplinary disconnection: why was a system like C³ necessary?
     Fragmented approaches across psychoacoustics, EEG/fMRI, and cognitive theory
     The SRC⁹ vision: reconstructing the part–whole relationship







     What is sensory–cognitive resonance?
     Concepts of oscillation–synchronization–network integration
     Temporal Perceptual Stability (TPS), Tonal Fusion Index (TFI), Neural Synchronization Fields (NSF)





\[
C^3(t) = _i=1^9 w_i  _i(t)
\]


\[
_i(t) = _j=1^N w_ij  _j(t)
\]


     Normalized resonance computations
     Temporal resolution
     Weight coefficients






Each defined in a separate section:


      CTU
      AOU
      IEU
      SRU
      SAU
      PIU
      IRU
      NSU
      RSU





     Nodes = Cognitive functions
     Elements = Measurable EEG/fMRI outputs


Each NODE includes:


     Conceptual definition
     Literature reference
     GlassBrainMap coordinate







     Each NODE assigned a 3D spherical region in MNI space
     Anatomical localization (e.g., ACC, SMA, HG)





     SVG layer
     React/D3.js interface
     Hover–click interaction
     Tooltip + page linkage







     How is the overall output of C³ computed over time?
     Frame calculations via EEG resolution





     Harmonic Distance and Φ values from R³
     Spectral profile data from S³
     Feedback from C³: attentional dispersion, valence mapping







     Effects of different UNITs in therapeutic settings
     Trauma-informed listening through IRU and PIU





     Sound selection guided by C³
     Listener direction through CTU + AOU interaction





     Attention enhancement in children (IEU + SAU)
     Resonance density in learning via RSU







     Unity/WebGL + OSC/WebSocket connectivity





     GlassBrainVR
     Resonance–narrative integration





     Gaps in existing systems
     Innovative solutions offered by C³
     The irreplaceable role of C³ within SRC⁹






 is a neurophysiologically grounded, mathematically formulated cognitive modeling engine that analyzes the , , , and  evoked by music in the human brain as a time-dependent, multilayered system.

The main goal of this system is to measure, classify, and represent the  in a music listener's brain using data such as:


     EEG (alpha, beta, gamma, MMN, P300),
     MEG (oscillatory phase-locking),
     fMRI (region-specific activations)


C³ operates through . Each UNIT represents a specific cognitive function. Internally, these Units are composed of  (functional mechanisms) and  (measurable neural outputs). In this way, the system creates a fully connected network between music and the brain.



C³ is one of the three main modules of the :


     : Physical–acoustic analysis of sound
     : Mathematical modeling of tonal, microtonal, and harmonic structures
     : Representation of musical structures' impact on the brain through time-based neurocognitive models


There is  between these modules. For example:


     Spectral data from S³ (e.g., noise intensity, tonal center frequency) \(\) enters R³ for computations of Φ (resonance potential) and HD (harmonic distance) \(\) these values are then transmitted to C³ to generate cognitive load or emotional response in modules like CTU and AOU.


C³ also sends its results as  to R³ (harmonic resonance suggestions) and to S³ (perceptual spectrum map filtering). This structure defines the system’s  nature.





In the C³ system, cognitive resonance is defined as the  of processes such as , , , and  in the brain during music listening. This resonance is studied across the following layers:


      EEG data (phase locking, oscillation power, ERP components)
      fMRI BOLD increase, regional localization in MNI coordinates
      Moment-to-moment alignment of music events with brain responses





     : Conceptual framework (e.g., CTU – Cognitive Tension Unit)
     : Specific functional structure (e.g., Harmonic Dissonance Node)
     : Measurable EEG/fMRI output (e.g., alpha–beta phase locking)


This structure ensures that each UNIT consists of layers that are , , and . Thus, each NODE becomes , , and .



Each NODE in C³ is assigned a brain region (ROI – Region of Interest). These regions are defined as spherical volumes using .


<table><tr><td>
</td><td></td><td></td><td></td></tr><tr><td>
AOU – HGAINT 01</td><td></td><td>Left posterior STG</td><td>Potes et al., 2012</td></tr><tr><td>IEU – HARMONIC 01</td><td></td><td>Fronto-central EEG</td><td>Crespo-Bojorque et al., 2018</td></tr><tr><td>CTU – RESOLVED 05</td><td></td><td>Anterior HG</td><td>Norman-Haignere et al., 2013</td></tr><tr><td></td></tr></table>


These coordinates are fully aligned with the data in  and .

On the web platform, this information is connected to a visual map through the SVG + React component .



As of now, a total of  have been integrated into the C³ system. Each NODE is supported by at least one experimental study. These include:


     EEG/MEG-based ERP or mismatch studies
     fMRI studies (valence, arousal, pitch, syntax, entrainment)
     Multimodal meta-analyses (FFR, P300, dopamine release, MMN)
     Theoretical models (predictive coding, Wundt curve, information content models)




 forms the  of the SRC⁹ system. It receives spectral and resonance data from modules like S³ and R³, interprets them through EEG/fMRI-supported models in the human brain, and outputs results that allow musical structures to be measured at the level of . It is the only system that achieves this function in a fully integrated form.








The  is a multidimensional, time-sensitive neural modeling system designed to measure, classify, and structurally represent the cognitive, emotional, and motor impacts of music on the human brain. C³ integrates empirical neurophysiological data—specifically EEG (alpha, beta, gamma, MMN, P300), MEG, and fMRI—to track brain responses to musical stimuli in real-time across nine cognitive domains known as .

Each Unit corresponds to a distinct cognitive subsystem (e.g., expectation, tension, memory, emotion), modeled as an independent but fully integrable node within the system. Internally, each Unit is hierarchically structured into , which denote abstract cognitive processes, and , which refer to quantifiable neural signals. These signals include frequency-specific oscillatory phenomena, phase-locked responses, and region-specific BOLD activations.



C³ is the cognitive core of the  framework, which comprises three interdependent modules:


     : Performs advanced time–frequency analysis of musical signals.
     : Models harmonic and tonal structures through vector lattices, scalar spaces, and resonance metrics.
     : Converts these structural data into biologically realistic neural representations of how the brain interprets music.


These modules are not isolated silos; they form a :


     Spectral information from S³ (e.g., timbral flux, pitch entropy) is used by R³ to calculate harmonic metrics such as Φ (Resonance Potential) and HD (Harmonic Distance).
     These R³ outputs then inform C³, which uses EEG and fMRI-informed functions to simulate attention, emotional valence, arousal, and tension.
     C³, in turn, feeds back cognitive feedback into S³ and R³, enabling dynamic perceptual recalibration.


This feedback loop creates a closed analytical ecosystem that integrates perception, structure, and cognition.





Cognitive resonance refers to the dynamic synchronization of neural, emotional, and motor processes elicited by musical stimuli. This phenomenon is understood not as localized brain activation, but as a distributed resonance field involving:


     Oscillatory synchronization (e.g., beta/gamma coupling between SMA and auditory cortex)
     Predictive mismatch mechanisms (e.g., MMN and P300 ERP responses)
     Region-specific activation (e.g., DLPFC for tension, ACC for ambiguity, amygdala for affect)


These mechanisms interact through temporal coherence and multi-band entrainment, enabling a real-time neural “tracking” of musical structure.



The system's top-level model is defined as:

\[
C^3(t) = _i=1^9 w_i  _i(t)
\]

Where:


     \(C^3(t)\): total cognitive resonance at time \(t\)
     \(_i(t)\): normalized activity of the \(i\)-th cognitive unit
     \(w_i\): weight coefficient for each unit (empirically tunable)


Each Unit’s internal model is further decomposed as:

\[
_i(t) = _j=1^N w_ij  _j(t)
\]

Each Node is derived from EEG or fMRI data—either continuous oscillatory amplitudes or discrete event-related potentials—and parameterized using real-time neuroimaging standards.



The C³ system is hierarchically organized:


     : Defines a domain of cognitive-musical processing (e.g., tension, memory, flow).
     : Represents a functional module within the Unit (e.g., Harmonic Dissonance).
     : A neurophysiological observable (e.g., beta-band phase locking, BOLD activation).


Each Element is associated with:


     A named brain region
     A measurement method (EEG, fMRI, or both)
     A validated reference from neuroscience literature
     An anatomical coordinate (MNI system)


This allows C³ to link abstract musical cognition to empirical neurobiology in a deterministic way.



C³ integrates seamlessly with the , a visual representation system that places each Node/Element at anatomically and functionally relevant coordinates.


<table><tr><td>
</td><td></td><td></td><td></td></tr><tr><td>
AOU – HGAINT 01</td><td></td><td>Left posterior STG</td><td>Potes et al., 2012</td></tr><tr><td>IEU – HARMONIC 01</td><td></td><td>Fronto-central cortex</td><td>Crespo-Bojorque et al., 2018</td></tr><tr><td>CTU – RESOLVED 05</td><td></td><td>Right Anterior HG</td><td>Norman-Haignere et al., 2013</td></tr><tr><td></td></tr></table>


These coordinates are verified against the  document, using MNI-space localization. In the web interface, they are rendered interactively via  and SVG/D3.js overlays.



The C³ system is constructed upon  in the fields of neuroscience, music cognition, EEG/fMRI research, and mathematical modeling. Each Node is grounded in direct empirical evidence. Key domains include:


     MMN, P300, N2 ERP responses to pitch/syntax deviations
     Beta/gamma-band entrainment in auditory–motor synchronization
     Dopaminergic pathways during emotional peaks (Salimpoor et al., 2011)
     Functional anatomy of musical memory (Janata, 2009; Zatorre \& Halpern, 2005)


All references are encoded into structured  and  formats for API-level integration.



 is not a passive music analysis tool. It is an  that transforms symbolic or spectral musical structures into meaningful, neurologically validated resonance fields. Its position within the  system ensures that structure, perception, and cognition are analyzed as a single unified continuum.







Despite significant progress in music theory, neuroscience, and artificial intelligence, the academic and technological landscape surrounding music cognition remains deeply fragmented. Most systems operate in isolation:


     Spectral analysis platforms (e.g., Fourier-based spectrograms) offer detailed acoustic profiles but lack perceptual or cognitive grounding.
     Harmonic and mathematical theories (e.g., Lewin’s GIS, Tonnetz models) focus on intervallic structure but often disregard empirical brain data.
     Neuroscientific research (EEG, fMRI) reveals profound insights into perception and emotion but rarely interfaces with formal music theory or real-time systems.


This disjunction has created a theoretical bottleneck. Without a shared framework, advances in one domain fail to propagate meaningfully into others. As a result, most current tools lack cross-domain explanatory power, cognitive transparency, and compositional usability.



Traditional music-theoretical models such as Generalized Interval Systems (GIS), Harmonic Distance metrics, and Tonal Hierarchies (e.g., Krumhansl’s tonal profiles) are mathematically elegant, but often fail to predict real listener responses.

Conversely, neuroscientific findings—such as:


     the MMN response to unexpected harmonies,
     the P300 component linked to rhythmic anomalies,
     or reward-related dopamine release during musical peaks (Salimpoor et al., 2011),


have rarely been integrated into formal, computationally usable structures.

There is no established mapping between music-theoretical constructs (e.g., cadence, modulation, dissonance) and neural metrics (e.g., BOLD, ERP, phase-locking). This absence of structural integration drastically reduces the predictive and pedagogical power of existing systems.



While AI tools such as OpenAI’s Jukebox, Google Magenta, or Amper Music have enabled impressive generative outputs, they typically operate without explanatory or cognitive modeling frameworks. They generate music without understanding what attention, memory, emotion, or structural coherence mean to a human listener.

This black-box architecture:


     Offers no feedback on listener state
     Cannot simulate emotional or cognitive pathways
     Provides no route for targeted composition or real-time feedback


C³ was conceived precisely to close this loop—not to replace such systems, but to make them cognitively explainable, analyzable, and affectively steerable.



Even the most sophisticated neuroscientific work on music (e.g., Janata, 2009; Zatorre \& Halpern, 2005) remains technically inaccessible to composers, theorists, or real-time music systems. The brain data is not structured in a way that can be operationalized.

For example:


     Neural entrainment in STG and SMA during rhythmic listening (Nozaradan, 2012)
     Amygdala activation during tonal familiarity or melodic recall (Koelsch, 2008)
     fMRI-detected dopaminergic release in ventral striatum (Blood \& Zatorre, 2001)


—all provide powerful evidence, but without an architectural scaffold to translate them into music-analytical or compositional insight.



C³ was developed as an architectural answer to this methodological impasse.




     To provide a layered system that explicitly links:
    
         Symbolic musical events (e.g., harmonic surprise, tempo shift)
         Neural mechanisms (e.g., alpha desynchronization, ERP signatures)
         Anatomical mappings (e.g., ACC, STG, dPMC)
         Time-based resonance metrics
    
    
     To construct a modular model (9 Units) that mirrors actual neuroscientific categorizations:
    
    
         Tension (CTU), Expectation (IEU), Emotion (AOU), Flow (PIU), Memory (SAU), etc.
    
    
     To ensure every part of the system is:
    
         Empirically grounded (with MNI coordinates, EEG/fMRI citations)
         Mathematically formalized (via resonance models and time equations)
         Interactively visualized (through the GlassBrainMap and unit dashboards)
    
    
     To allow integration with generative, educational, or therapeutic systems.




C³ draws from and synthesizes:


     David Lewin’s transformational music theory and the notion of “network transformations”
     Julian Hook’s cross-type intervallic mappings
     Jean-Jacques Nattiez’s tripartite semiotic model (poietic – neutral – esthesic)
     Zatorre \& Halpern’s fMRI studies on musical imagery
     Large \& Snyder’s work on neural resonance and attentional entrainment
     Recent computational neuroscience models such as Temporal Perceptual Stability (TPS), Tonal Fusion Index (TFI), and Neural Synchronization Fields (NSF)


These form the philosophical, neurobiological, and mathematical pillars of the C³ system.



C³ was not built to replace existing methods—it was built to unify them.

It serves as a cognitive-mathematical bridge between acoustic data, symbolic musical structures, and neurological response patterns. Its modular design, mathematical backbone, and empirical grounding position it as a unique tool in both theoretical and practical domains.

It is not merely a system. It is a new paradigm for understanding music.







Cognitive resonance refers to the simultaneous activation and phase-alignment of neural systems—cortical, subcortical, and limbic—elicited by musical structures perceived as meaningful, surprising, emotionally salient, or motorically engaging.

It is not a static "response" but a dynamic system of oscillatory entrainment that unfolds over time in direct correlation with acoustic and symbolic musical events. In this sense, C³ does not model perception as a reaction, but rather as a temporally evolving resonance field shaped by attention, prediction, emotion, and embodied synchronization.



Cognitive resonance is anchored in three core mechanisms:



Neural populations synchronize their firing phases with periodic or structured stimuli in music, especially rhythm and pulse. EEG reveals:


     Beta-band phase-locking in motor regions (SMA, PMC) during pulse alignment
     Gamma coherence between auditory and frontal regions for tonal/melodic fusion
     Alpha synchrony in frontal–parietal regions indicating attentional top-down integration




The brain actively predicts musical continuations. Violations of these predictions result in:


     Mismatch Negativity (MMN) in STG and Fz (EEG), typically for harmonic or rhythmic deviations
     P300 ERPs for consciously detected rhythmic or temporal anomalies


These responses form the backbone of the IEU unit in C³.



Music evokes emotion through dopaminergic reward pathways, primarily involving:


     Ventral Striatum, Nucleus Accumbens (peak pleasure: Salimpoor et al., 2011)
     Amygdala, Insula, and ACC (valence/arousal distinction: Koelsch, 2008; Zatorre \& Blood, 2001)


Together, these three systems create multiband, multisite synchronization patterns that constitute what we term .



In C³, cognitive resonance is mathematically expressed as the time-evolving summation of weighted neural activity across multiple Units:

\[
C^3(t) = _i=1^9 w_i  _i(t)
\]

Where:


     \(_i(t)\): Time-normalized resonance output of the \(i\)-th Unit (e.g., CTU, AOU)
     \(w_i\): Tunable weight coefficient based on experimental or functional prioritization
     \(t\): Time (sampled at EEG resolution, e.g., 100ms)


Each Unit is composed of Nodes and Elements:

\[
_i(t) = _j=1^N w_ij  _j(t)
\]

Each Node is grounded in a real EEG/fMRI marker:


     : conceptual function (e.g., "Harmonic Dissonance")
     : observable signal (e.g., “\(\)–\(\) phase-locking in DLPFC”)


These form the structural hierarchy:

\[
      \\
\]



C³ operates in time-series frames, typically aligned to 0.1-second EEG windows. This enables high-resolution modeling of:


     Fast transitions (e.g., rhythmic inflections, expectation violations)
     Slow evolutions (e.g., emotional immersion, tonal unfolding)


Each Element generates a signal trace over time, which is then:


     Normalized to a common scale [0, 1]
     Weighted within its Node and Unit
     Aggregated into the total resonance field \(C^3(t)\)


: a spatiotemporal map of musical cognition, updated per frame, and navigable across Units, Nodes, and regions.



The final output of C³ can be interpreted as a multidimensional field evolving in time:


     Each axis corresponds to a Unit (9D space)
     Each point \(C^3(t)\) is a vector of dimension 9
     Over time, the output forms a trajectory curve in this high-dimensional space


This allows:


     Trajectory clustering for musical styles
     Dynamical stability analysis (e.g., flow states vs. high-tension nodes)
     Comparative signature modeling (e.g., comparing Bach vs. Radiohead vs. AI-generated pieces)


These analytical outputs feed into applications like music therapy profiling, personalized listening models, or composition-guidance systems.



Each Element has a unique anatomical anchor:


     Region name (e.g., Broca, SMA, NAcc)
     MNI coordinates (from )
     Visualization via  or Unity 2D/3D interfaces


This results in:


     Real-time activation maps per frame
     Tooltip-based summaries
     Click-through access to associated Unit/Node pages


Each region is plotted as a sphere in anatomical space and linked to its resonance value over time.



Cognitive resonance is not a metaphor. It is a quantifiable, observable, and mathematically modelable system of neural alignment elicited by music.

C³ builds a bridge between:


      (from S³ and R³)
      (via neural synchronization)
      (through limbic modeling)
      (via predictive structures)


This fusion yields not only a deepened understanding of musical experience, but also a practical computational system that allows music to be measured, mapped, predicted, and creatively shaped.







The C³ system is constructed as a hierarchical neural modeling architecture operating on three levels:

\[
C^3(t) = _i _i(t),  _i(t) = _j _ij(t),  _ij(t) = _k _ijk(t)
\]

Each Element represents a measurable neural signal, modeled mathematically. Nodes aggregate these signals into functional units (e.g., "Expectation Violation", "Tonal Familiarity"). Units represent full cognitive subsystems (e.g., IEU, SAU).

This bottom-up structure enables us to compute macro-level phenomena (emotion, memory, flow) from micro-level EEG/fMRI signals.



At the system level:

\[
C^3(t) = _i=1^9 w_i  _i(t)
\]

Where:


     \(C^3(t)\): Total cognitive resonance at time \(t\)
     \(_i(t)\): Activity of Unit \(i\)
     \(w_i  \): Empirically tunable weight for each Unit (default: 1.0)


Each Unit has a local expansion:

\[
_i(t) = _j=1^N_i w_ij  _ij(t)
\]


     \(_ij(t)\): Activity of Node \(j\) in Unit \(i\)
     \(w_ij  [0,1]\): Functional contribution weight of Node \(j\)


Each Node is derived from its underlying Elements:

\[
_ij(t) = _k=1^M_ij w_ijk  _ijk(t)
\]



An Element is a neural feature defined as:

\[
_ijk(t) = S_ijk(t)  [0,1]
\]

Where \(S_ijk(t)\) is the normalized neural signal derived from:


     EEG band power (e.g., \(\), \(\), \(\))
     ERP component (e.g., MMN, P300)
     fMRI BOLD z-score in MNI-space
     Phase coherence between regions


Each signal is transformed to the [0,1] scale using:

\[
S_ijk(t) = (x) - (x)
\]

For oscillatory components:

\[
x(t) = | \ (t) \ |^2
\]

(Where \(\) is the Hilbert Transform envelope)

For ERP components:

\[
x(t) = (t - )
\]

For fMRI:

\[
x(t) = z(t) = (t) - 
\]

All data is resampled to a common timeline resolution (typically 10 Hz, i.e., 100ms frames).



Weights \(w_i\), \(w_ij\), \(w_ijk\) are initialized based on empirical studies:


     \(w_ijk\): based on effect size from literature (e.g., Cohen’s \(d\))
     \(w_ij\): based on relative contribution within Unit (e.g., entropy vs. ERP)
     \(w_i\): application-specific (e.g., PIU emphasized in flow-based systems)


Over time, weights can be adapted dynamically based on:


     Task relevance (e.g., in therapy, CTU may be downregulated)
     User feedback (e.g., neurofeedback systems)
     Generative models (e.g., AI uses the feedback to alter music in real time)




The entire C³ system can be formalized as a three-layer matrix multiplication:

\[
C^3(t) = ^T  (t)
\]

Where:

\[
(t) = [_1(t), , _9(t)]^T  ^9  1
\]

Each \(_i(t)\) is:

\[
_i(t) = _i^T  _i(t)
\]

And each \(_i(t)\) (Node vector) is:

\[
_i(t) = [_i1(t), , _iN(t)]^T
\]

This layered abstraction makes it possible to:


     Implement efficient GPU-based computation
     Track changes per unit or per frame
     Enable real-time synchronization with audiovisual feedback




Each Element is linked to:


      (e.g., CTU)
      (e.g., harmonic\_dissonance)
     MNI coordinates
     Measurement method (EEG, fMRI)
     Source file path (for dynamic signal streaming)


All metadata is stored in , and signals are fed as , , or  time series.

Visual representation is done through:


      for cognitive UI
      for anatomical UI
      (or Python backend) for signal computation




The C³ system is not merely a conceptual framework but a fully formalized, mathematically grounded computational model. It:


     Integrates real neural signals into a unified cognitive resonance space
     Adapts to multiple temporal and structural resolutions
     Operates across Unit, Node, and Element layers
     Is designed for interactive, real-time applications in analysis, composition, education, and therapy


This mathematical architecture makes it possible to represent the complexity of musical cognition as a dynamic, quantifiable, and deeply interpretable process.









The Cognitive Tension Unit (CTU) is designed to capture and model the neural mechanisms of cognitive dissonance, ambiguity, and harmonic instability in response to complex musical stimuli. It measures the brain’s response to tonal unpredictability, spectral irregularity, and harmonic deviation by tracking electrophysiological indicators and hemodynamic signals primarily across the dorsolateral prefrontal cortex (DLPFC), anterior cingulate cortex (ACC), and inferior frontal gyrus (Broca area).

This unit operates on the hypothesis that musical dissonance, entropy, and distance from tonal centers create a measurable increase in cognitive load. These effects are observable via:


     Alpha and beta phase-locking in frontal regions (EEG)
     Increased BOLD activity in cingulate cortex (fMRI)
     Beta-band amplitude increases in Broca’s area during spectral irregularity




The internal model of CTU is:

\[
(t) = w_1  _(t) + w_2  _(t) + w_3  _(t)
\]

Where:


     \(w_1, w_2, w_3\): empirically tunable node weights


Each Node has one or more Elements grounded in EEG/fMRI markers.






     : Models increased cognitive stress when dissonant chords or pitch clusters appear in a tonal context
     : DLPFC, ACC
     : Alpha–beta phase locking
     : Fishman et al., 2001


:

     �� EEG Phase Locking (\(\)–\(\))
     EEG electrodes: F3, Fz
     MNI Coordinate: [+32, +50, +20]
     GlassBrainMap ID: 





     : Quantifies the unpredictability and information density in the sound spectrum
     : Broca’s area (left IFG), temporal cortex
     : Beta amplitude increase
     : Norman-Haignere et al., 2013


:

     �� Spectral Complexity Response
     MNI Coordinate: [+64, –22, +4]
     GlassBrainMap ID: 
     EEG channel cluster: F7–T7
     fMRI: Left IFG BOLD increase





     : Measures perceived tonal instability caused by deviations from local tonal center
     : DLPFC, ACC
     : Alpha amplitude increase
     : Hyde et al., 2008


:

     �� EEG Alpha Power (ACC-centered)
     MNI Coordinate: [+30, +36, +20]
     GlassBrainMap ID: 
     Method: Power spectral density (EEG)




The CTU Nodes are spatially registered within the GlassBrain system. Their associated coordinates and regions are as follows:


<table><tr><td>
</td><td></td><td></td><td></td></tr><tr><td>
</td><td>[+32, +50, +20]</td><td>DLPFC + ACC</td><td>EEG \(\)–\(\) phase-locking</td></tr><tr><td></td><td>[+64, –22, +4]</td><td>Broca + Temporal Cortex</td><td>Beta increase in spectral complexity</td></tr><tr><td></td><td>[+30, +36, +20]</td><td>ACC + DLPFC</td><td>EEG Alpha increase for tonal ambiguity</td></tr><tr><td></td></tr></table>


These markers appear in the  component and are directly linked to the  file in the system.



The CTU unit functions as a cognitive load detector, dynamically representing how musical instability translates into mental effort. It is especially responsive to:


     Tonal deviations
     Unexpected dissonances
     High spectral entropy


These events are interpreted by the brain as uncertainty or cognitive conflict, which C³ measures in real time.

CTU is often antagonistic to PIU (Phenomenological Immersion Unit): increased CTU activity often leads to decreased absorption or flow.




     : Cognitive overload indicators in neurorehabilitation
     : Dynamic tension mapping for film scoring or generative music
     : Real-time CTU readout for adaptive sound environments








The Affective Orientation Unit (AOU) quantifies the listener’s affective response to music by modeling two principal emotional axes:


     : The pleasantness or unpleasantness of the musical stimulus
     : The intensity or physiological activation induced by the music


These dimensions are computed through EEG and fMRI indicators within limbic, prefrontal, and motor cortical areas—including the amygdala, ventral striatum, MPFC, SMA, and STG.

AOU is particularly sensitive to:


     Tonal stability (linked to positive valence)
     Spectral balance and consonance (linked to emotional reward)
     Tempo and rhythmic complexity (linked to arousal and motor drive)




AOU is computed as a weighted combination of two core Nodes:

\[
(t) = w_1  (t) + w_2  (t)
\]

Each of which is decomposed into signal-bearing Elements:

:

\[
(t) = v_1  (t) + v_2  (t) + v_3  (t)
\]

:

\[
(t) = a_1  (t) + a_2  (t) + a_3  (t)
\]

All elements are normalized between 0–1, with weights derived from empirical affective neuroscience research.






     : Detects emotional polarity of the musical input
     EEG Alpha Asymmetry in MPFC correlates with valence level
     Gamma and BOLD activation in amygdala and ventral striatum correlate with emotional reward





     Region: MPFC
     EEG Marker: Alpha asymmetry
     MNI: [+6, +52, +10]
     Citation: Zatorre \& Halpern, 2005





     Region: Amygdala, Insula
     Method: fMRI + EEG Gamma
     MNI: [-20, 0, -12]
     Citation: Koelsch, 2011





     Region: Ventral Striatum, NAcc
     Method: fMRI
     MNI: [+10, +8, -10]
     Citation: Salimpoor et al., 2011





     : Captures intensity and activation driven by musical rhythm, tempo, and flux





     Region: Motor Cortex
     EEG: Beta-band amplitude
     MNI: [+40, -10, +60]
     Citation: Janata et al., 2009





     Region: STG
     EEG: Theta
     MNI: [+50, +10, -6]
     Citation: Alluri et al., 2012





     Region: SMA
     EEG/fMRI: Beta + BOLD
     MNI: [+6, -6, +70]
     Citation: Chen et al., 2008





<table><tr><td>
</td><td></td><td></td><td></td></tr><tr><td>
Tonal Stability</td><td>[+6, +52, +10]</td><td>MPFC</td><td>Zatorre \</td><td>Halpern (2005)</td></tr><tr><td>Spectral Balance</td><td>[-20, 0, -12]</td><td>Amygdala, Insula</td><td>Koelsch (2011)</td></tr><tr><td>Harmonic Conson.</td><td>[+10, +8, -10]</td><td>Ventral Striatum</td><td>Salimpoor et al. (2011)</td></tr><tr><td>Tempo Dynamics</td><td>[+40, -10, +60]</td><td>Motor Cortex</td><td>Janata (2009)</td></tr><tr><td>Spectral Flux</td><td>[+50, +10, -6]</td><td>STG</td><td>Alluri et al. (2012)</td></tr><tr><td>Rhythmic Comp.</td><td>[+6, -6, +70]</td><td>SMA</td><td>Chen (2008)</td></tr><tr><td></td></tr></table>


These entries are directly mapped into the GlassBrain SVG and visualized in the  component as colored resonance hotspots.



The AOU unit is the affective engine of the C³ system. It accounts for the emotional tone of musical events using neurobiologically validated metrics. It plays a key role in:


     Differentiating emotionally positive/negative musical content
     Identifying peaks of arousal or relaxation
     Guiding adaptive audio systems (e.g., emotion-matching playlists, AI composition targeting affective impact)


It interacts heavily with:


     CTU (tension)
     PIU (flow state)
     RSU (integrated resonance summary)





     Affective tagging in music libraries (real-time emotional metadata)
     Neurofeedback therapy for emotional regulation
     Real-time composition tools for mood shaping and expressive calibration








The Intuitive Expectation Unit (IEU) simulates the listener’s internal predictive model during music listening. It monitors how the brain anticipates musical structure and reacts to violations of those expectations. This encompasses both pre-conscious responses (e.g., MMN) and attended violations (e.g., P300), as well as uncertainty metrics (e.g., melodic entropy).

IEU reflects a foundational principle of musical cognition: expectation and surprise are primary drivers of attention, emotion, and memory. The system quantifies these dynamics through:


     Early prediction-error signals (EEG MMN in STG)
     P300 ERP responses in premotor regions
     Neural tracking of melodic uncertainty (entropy measures in dACC and amygdala)




IEU operates via three primary Nodes, weighted and combined over time:

\[
(t) = w_1  (t) + w_2  (t) + w_3  (t)
\]

Each Node computes one functional aspect of expectation processing:


     \(w_1\): Surprise in harmony
     \(w_2\): Surprise in rhythm
     \(w_3\): Global uncertainty in melodic information







     : Detects dissonant or out-of-key chords in a tonal context
     : MMN ERP (mismatch negativity)
     : STG, auditory cortex
     : Crespo-Bojorque et al., 2018





     EEG Sites: Fz, Cz
     MNI: [0, +50, +20]
     Type: ERP (automatic deviance detection)





     : Identifies tempo/beat violations and rhythmic anomalies
     : P300 ERP (conscious deviance)
     : SMA, Motor Cortex
     : Schön et al., 2005





     MNI: [+10, -10, +60]
     Type: ERP (attended violation detection)





     : Measures statistical uncertainty in melodic sequences
     : Alpha/theta shift in medial regions
     : dACC, amygdala
     : Koelsch, 2008





     MNI: [+4, +18, +26]
     Metric: Entropy of pitch sequences (Shannon index, Markov chain predictability)





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Harmonic Violation</td><td>[0, +50, +20]</td><td>STG, Fronto-central</td><td>EEG (MMN)</td><td>Crespo-Bojorque et al., 2018</td></tr><tr><td>Rhythmic Violation</td><td>[+10, -10, +60]</td><td>SMA, Motor Cortex</td><td>EEG (P300)</td><td>Schön et al., 2005</td></tr><tr><td>Melodic Entropy</td><td>[+4, +18, +26]</td><td>dACC, Amygdala</td><td>EEG (\(\)/\(\)), fMRI</td><td>Koelsch, 2008</td></tr><tr><td></td></tr></table>


These coordinates are rendered dynamically in the GlassBrain system and updated in real time as IEU activity changes.



IEU measures how expected a musical moment is and how the brain responds to the unexpected. When expectation is fulfilled, IEU output remains low. When it is violated, resonance spikes occur—often triggering cognitive reorientation or emotional reappraisal.

IEU is crucial for:


     Segmenting musical flow
     Signaling novelty or structural change
     Synchronizing listener attention to surprise events


It interacts strongly with:


     CTU (tension under surprise)
     SAU (memory under uncertainty)
     AOU (emotional impact of surprise)





     Adaptive AI music systems: Dynamically vary predictability to hold listener attention
     Neuroeducation: Track student surprise and engagement in music learning
     Therapeutic design: Gradually increase predictability to rebuild trust in rhythm/melody recognition








The Somatic Resonance Unit (SRU) models how the brain's motor and premotor systems synchronize with perceived musical rhythm. It captures entrainment phenomena in motor cortices, pulse clarity processing, and tempo stability detection, which form the physiological basis for movement, tapping, dancing, and temporal prediction during music listening.

This Unit operates on the principle that rhythmic structures entrain cortical beta-band oscillations, and that clear pulse and metric structures elicit synchronized activity in:


     Supplementary Motor Area (SMA)
     Premotor Cortex (PMC)
     Basal Ganglia (Putamen)
     Cerebellum


SRU plays a crucial role in connecting auditory input to bodily response via motor entrainment and has direct applications in rehabilitation, rhythm training, and movement–based therapies.



SRU is defined by a linear combination of three functional Nodes:

\[
(t) = s_1  (t) + s_2  (t) + s_3  (t)
\]

Where:


     \(s_1, s_2, s_3\): weight coefficients derived from neural effect size or application-specific relevance







     : Detects the salience of rhythmic beat and its effect on motor cortex
     : Beta amplitude increase
     : Motor Cortex, Putamen, Cerebellum
     : Fujioka et al., 2012





     MNI: [+20, -10, +60]
     EEG: \(\)-band power at C3/Cz
     fMRI: BOLD in cerebellar vermis and PMC





     : Represents the regularity and predictability of rhythmic subdivisions
     : Beta phase-locking
     : SMA, Premotor Cortex
     : Chen et al., 2008





     MNI: [+6, +4, +64]
     EEG: \(\) phase coherence
     fMRI: SMA BOLD activation





     : Measures consistency in tempo; correlates with sensorimotor coupling strength
     : Interregional beta coherence
     : Putamen, PMC
     : Thaut et al., 2015





     MNI: [+28, -12, +60]
     EEG: \(\) coherence (PMC \(\) Basal Ganglia)





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Pulse Clarity</td><td>[+20, -10, +60]</td><td>Motor Cortex, Putamen</td><td>EEG (\(\)), fMRI</td><td>Fujioka et al., 2012</td></tr><tr><td>Metric Stability</td><td>[+6, +4, +64]</td><td>SMA, Premotor Cortex</td><td>EEG phase-locking</td><td>Chen et al., 2008</td></tr><tr><td>Tempo Stability</td><td>[+28, -12, +60]</td><td>PMC, Basal Ganglia</td><td>EEG coherence</td><td>Thaut et al., 2015</td></tr><tr><td></td></tr></table>


These coordinates are used in the interactive GlassBrainMap, providing anatomical precision for motor–rhythmic coupling during music listening.



SRU tracks real-time neural entrainment of the motor system to rhythmic music. It reflects how the body prepares to move, taps to the beat, and predicts upcoming rhythmic events. Its signal rises with:


     Stable and predictable beats
     High beat salience and metric clarity
     Entraining pulse structures (e.g., groove, swing, syncopation)


It often correlates positively with:


     PIU (immersion through movement)
     AOU (arousal from rhythm)
     NSU (neural synchronization)





     Motor rehabilitation and rhythm therapy
     Groove detection algorithms in music information retrieval
     Neurophysiological metrics of musical engagement


SRU is particularly relevant for dance research, tempo training, and interactive AI music generation that responds to bodily input or encourages physical engagement.







The Semantic–Autobiographical Unit (SAU) captures the interaction between musical structure and episodic/semantic memory systems in the brain. It quantifies how music activates autobiographical recall and semantic meaning through hippocampal–prefrontal–limbic networks.

This Unit models three core processes:


     Melodic repetition and its role in memory cueing
     Tonal familiarity and its effect on semantic access
     Timbre recognition and affective memory tagging


SAU is essential for explaining why certain music evokes specific personal memories, how musical familiarity shapes identity, and how past experiences influence present perception.



SAU is calculated as:

\[
(t) = s_1  (t) + s_2  (t) + s_3  (t)
\]

Each Node maps to a neuroanatomically distinct memory-processing mechanism and contributes to the system’s representation of semantic–episodic resonance.






     : Detects melodic repetitions as episodic memory cues
     : Theta increase
     : Hippocampus, Parahippocampal Gyrus
     : Foss et al., 2007





     MNI: [–24, –40, –8]
     Method: EEG \(\); fMRI hippocampal BOLD
     Description: Increased theta in temporal–limbic regions during reoccurring phrases





     : Activates stored tonal patterns and schemas
     : Alpha increase
     : MPFC, ACC
     : Foss et al., 2007





     MNI: [+6, +48, +8]
     Method: EEG \(\); MPFC BOLD
     Description: Retrieval of culturally learned tonality (major/minor schemas)





     : Associates sound qualities with emotional memory
     : Gamma increase
     : Amygdala, STG
     : Chen et al., 2008





     MNI: [+22, +0, –20]
     Method: EEG \(\); STG BOLD
     Description: Recognition of familiar instrument types triggers affective memory





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Melodic Repetition</td><td>[–24, –40, –8]</td><td>Hippocampus, ParaHC</td><td>EEG \(\), fMRI</td><td>Foss et al., 2007</td></tr><tr><td>Tonal Familiarity</td><td>[+6, +48, +8]</td><td>MPFC, ACC</td><td>EEG \(\), fMRI</td><td>Foss et al., 2007</td></tr><tr><td>Timbre-based Memory</td><td>[+22, +0, –20]</td><td>Amygdala, STG</td><td>EEG \(\), fMRI</td><td>Chen et al., 2008</td></tr><tr><td></td></tr></table>


These brain regions are anatomically mapped and visualized within the GlassBrain system in real time.



SAU is the mnemonic layer of the C³ system. It provides a biologically grounded mechanism for modeling:


     Musical nostalgia
     Identity-based music perception
     Semantic resonance of culturally or personally meaningful sounds


Its resonance increases with:


     Repetition of previously heard motifs
     Use of familiar tonal centers
     Recognition of personally associated timbres (e.g., piano from childhood)


SAU often co-activates with:


     AOU (affective salience)
     IEU (surprise + recall)
     IRU (interpersonal resonance and memory convergence)





     Music therapy for trauma, memory loss, and dementia
     AI playlist personalization based on listener history
     Autobiographical score composition using musical memories as structure


SAU also supports cultural modeling—how exposure to tonal systems and timbral norms shapes perceptual identity and long-term memory.







The Phenomenological Immersion Unit (PIU) models non-analytical, affectively immersive listening states where attention, cognition, and motor inhibition converge to produce a sense of musical flow. These states often coincide with:


     Temporal suspension (loss of time awareness)
     Suppression of cognitive self-monitoring (reduced DMN activity)
     Heightened sensory and affective clarity


PIU tracks the transition from conscious processing to immersive resonance, particularly during:


     Slowly evolving harmonic textures
     Minimalistic repetition
     Ambient or timbrally complex soundscapes
     Trance, ritual, or meditative musics


It is one of the most non-linear and affective Units in the system.



PIU is computed as a blend of three neurodynamic constructs:

\[
(t) = p_1  (t) + p_2  (t) + p_3  (t)
\]

Each Node corresponds to a specific neural configuration observed during immersive listening or musical flow:


     \(p_1\): attention gain
     \(p_2\): default-mode suppression
     \(p_3\): temporal resolution and transition anchoring







     : Measures the emergence of sustained, high-focus listening
     : Gamma-band elevation
     : ACC, Frontal-Parietal Network
     : Santoyo et al., 2023





     MNI: [+4, +32, +24]
     Description: Increased \(\) during perceptual focusing (timbre-based or harmonic absorption)





     : Tracks mental state transitions into immersive flow
     : Alpha suppression; BOLD reduction in DMN
     : DMN hubs (mPFC, PCC); pre-SMA
     : Patterson et al., 2002





     MNI: [+2, +50, +6]
     Description: Drop in self-referential processing associated with absorption





     : Anchors transitions (e.g., chord change, pulse entrance)
     : Beta phase-locking
     : Frontal–Parietal Network
     : Nozaradan et al., 2012





     MNI: [+20, +10, +64]
     Description: Transition clarity in EEG \(\)-phase alignment





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Attentional Salience</td><td>[+4, +32, +24]</td><td>ACC, FP Network</td><td>EEG \(\)</td><td>Santoyo et al., 2023</td></tr><tr><td>Flow State</td><td>[+2, +50, +6]</td><td>mPFC, DMN</td><td>EEG \( \), fMRI DMN BOLD \(\)</td><td>Patterson et al., 2002</td></tr><tr><td>Transient Clarity</td><td>[+20, +10, +64]</td><td>FP Network</td><td>EEG \(\) phase-lock</td><td>Nozaradan et al., 2012</td></tr><tr><td></td></tr></table>


These entries are integrated into GlassBrain SVG and shown in immersive resonance clusters.



PIU is the immersive dimension of C³. It increases during:


     Deep listening
     Meditative or minimalistic music
     Non-verbal sound attention
     Flow-inducing music (e.g., ambient, trance, sacred chants)


PIU typically correlates with:


     \(\) CTU (reduced tension)
     \(\) AOU (emotional absorption)
     \(\) SAU (memory/identity resonance)
     \(\) IRU (shared flow in group listening)


The PIU trace reflects how long a listener is “lost in the music.”




     Therapeutic music for anxiety, depression, PTSD
     Flow design in composition and sound installations
     Biometric scoring of listener engagement and trance induction


In real-time systems, PIU can be used as a threshold trigger to adjust musical pacing, harmonic density, or lyrical clarity, enhancing sustained immersion.







The Interpersonal Resonance Unit (IRU) measures the shared neural and affective dynamics that arise when music is experienced collectively. This Unit is founded on the principles of:


     Inter-brain coherence (neural synchrony across listeners)
     Emotional contagion via acoustic affect cues
     Social synchronization of motor and affective circuits during group musical experiences


IRU is based on emerging research from hyperscanning EEG, dual-fMRI, and social-cognitive neuroscience, which shows that synchronous music listening, especially in emotionally charged contexts, causes measurable co-activation of limbic, temporal, and frontal regions across brains.



IRU is computed as a weighted average of three social-cognitive resonance Nodes:

\[
(t) = r_1  (t) + r_2  (t) + r_3  (t)
\]

Each Node represents a distinct interpersonal neural mechanism triggered by collective music engagement.






     : Measures synchronized alpha/theta rhythms between participants
     : Cross-brain phase-locking
     : Frontal Cortex, Superior Temporal Gyrus (STG)
     : Wallmark et al., 2018





     MNI: [0, +52, +14]
     Metric: Phase coherence across listener dyads (\(\)/\(\) bands)
     Description: Increased alignment in neural oscillations during co-listening





     : Captures joint activation in movement and attention networks
     : Gamma amplitude increases during joint attention
     : Frontal Cortex, STG
     : Wallmark et al., 2018





     MNI: [+12, +42, +14]
     EEG: \(\) power co-fluctuations in listeners
     Description: Mirrors shared attention and gesture during music





     : Models shared emotional responses via limbic system coupling
     : Alpha asymmetry; amygdala BOLD
     : Amygdala, ACC
     : Yang et al., 2025





     MNI: [+8, +6, -10]
     Description: Shared peaks in arousal/valence across participants





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Inter-Brain Coherence</td><td>[0, +52, +14]</td><td>Frontal + STG</td><td>EEG \(\)/\(\) Sync</td><td>Wallmark et al., 2018</td></tr><tr><td>Social Synchrony</td><td>[+12, +42, +14]</td><td>Frontal + STG</td><td>EEG \(\)</td><td>Wallmark et al., 2018</td></tr><tr><td>Emotional Resonance</td><td>[+8, +6, -10]</td><td>Amygdala + ACC</td><td>fMRI + EEG \(\)</td><td>Yang et al., 2025</td></tr><tr><td></td></tr></table>


These elements are rendered together in a special Group-Level GlassBrain Overlay, enabling visualization of multi-brain synchrony fields.



IRU allows C³ to model music not just as an internal experience, but as a shared cognitive–emotional field. IRU increases when:


     Music is heard in a social context
     Listeners share a history or cultural framework
     Body-based synchronization (e.g., group clapping, dancing) is present
     Emotional peaks align across individuals


IRU is the bridge between:


     AOU (individual affect)
     PIU (flow)
     RSU (integrated group resonance)





     Social music therapy (e.g., group rhythm interventions, trauma re-integration)
     AI-driven shared music experiences (co-listening apps, social playlist engines)
     Group neuroscience and emotion regulation training


IRU enables the modeling of shared emotional spaces and neural entrainment ecosystems, placing music at the center of group cognition.







The Neural Synchronization Unit (NSU) captures how music organizes and synchronizes brain activity through rhythmic and spectral structure. It operates on the principle that musical events—especially rhythmic periodicities, spectral regularities, and melodic contours—can entrain large-scale cortical networks via:


     Phase-locking to external rhythms
     Cross-regional coherence in gamma, beta, and alpha bands
     Temporal anticipation and predictive resonance


NSU integrates empirical findings from EEG, MEG, and frequency–following response (FFR) studies that show how temporal structure in sound becomes mirrored in neural timing.

It is particularly relevant in contexts of:


     Rhythm perception and pulse tracking
     Sensory–motor coupling
     Beat-based learning and coordination
     Tonal phase tracking and attentional alignment




NSU is computed as:

\[
(t) = n_1  (t) + n_2  (t) + n_3  (t)
\]

Each Node aggregates multiple regional signals reflecting cross-frequency and cross-site temporal alignment.






     : Tracks gamma-band synchrony between auditory and motor cortices
     : \(\) coherence (STG \(\) Motor Cortex)
     : STG, Motor Cortex
     : Bidelman \& Heinz, 2011





     MNI: [+38, -10, +52]
     EEG: \(\)-band inter-site phase clustering
     Metric: Phase-locking value (PLV)





     : Models cortical beta-band entrainment in motor sequencing and prediction
     : \(\) phase locking (SMA \(\) Basal Ganglia)
     : SMA, PMC, Basal Ganglia, STG
     : Bidelman \& Krishnan, 2009





     MNI: [+8, -6, +60]
     EEG: Inter-site \(\) PLV across frontal–motor nodes





     : Measures large-scale attentional coherence in alpha network
     : \(\) interhemispheric synchrony
     : Frontal Cortex, Parietal Cortex
     : Strait et al., 2012





     MNI: [+4, +52, +10]
     EEG: \(\) band coherence (fronto-parietal loop)
     Description: Indicator of global attention tuning to musical flow





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Gamma Coherence</td><td>[+38, -10, +52]</td><td>STG, Motor Cortex</td><td>EEG \(\) PLV</td><td>Bidelman \</td><td>Heinz, 2011</td></tr><tr><td>Beta Phase Locking</td><td>[+8, -6, +60]</td><td>SMA, Basal Ganglia</td><td>EEG \(\) PLV</td><td>Bidelman \</td><td>Krishnan, 2009</td></tr><tr><td>Alpha Synchrony</td><td>[+4, +52, +10]</td><td>Frontal–Parietal Cortex</td><td>EEG \(\) Coherence</td><td>Strait et al., 2012</td></tr><tr><td></td></tr></table>


These nodes are rendered in the GlassBrain overlay, showing synchronization fields in time-resolved layers.



NSU tracks how temporally structured sound creates temporally structured brain activity.

It reflects:


     High-frequency (\(\)) micro-entrainment for precise timing
     Mid-frequency (\(\)) entrainment for motor planning
     Low-frequency (\(\)) coherence for attentional binding


NSU is crucial for:


     Coordinated movement to music
     Pulse perception
     Learning of rhythmic and metrical structure
     Sustained attention


It interacts strongly with:


     SRU (somatic resonance)
     IEU (predictive modeling)
     RSU (resonance integration)





     Neuroeducation for rhythm training, beat perception, language timing
     Therapeutic rhythm protocols (e.g., Parkinson’s interventions)
     Real-time music–brain feedback in composition and AI systems


NSU gives C³ the neural infrastructure to temporally align listener state with musical structure—turning sound into synchronization.







The Resonance Synthesis Unit (RSU) is the final computational layer in the C³ system. Unlike the other eight Units, which represent specific cognitive functions (e.g., tension, emotion, memory), RSU performs a global integration of all underlying Units, Nodes, and Elements to compute:


     Total moment-to-moment cognitive resonance
     System-wide synchrony and coherence
     Weighted convergence of multi-dimensional neural states


RSU does not introduce new raw data. Rather, it serves as a nonlinear summarization node—a temporal and structural resonance integrator—combining emotion, memory, expectation, attention, motor entrainment, and inter-brain synchrony into a single multidimensional vector.



RSU operates across two primary mathematical layers:



\[
(t) = 9 _i=1^9 C^3_i(t)
\]

Where:


     \(C^3_i(t)\): The computed resonance from each Unit at time \(t\)


 Scalar between \([0,1]\) representing total network activation.\\
This value can be interpreted as a resonance index—how "fully activated" the cognitive-musical system is.



\[
(t) =   ^3(t)
\]

Where:

\[
^3(t) = [(t), (t), , (t)]^T    ^1  9
\]

\(\): Application-specific or data-derived weights (e.g., in therapy, PIU and SAU may be up-weighted)

This produces a 1D or N-dimensional fusion vector depending on context (e.g., therapy, composition, neuroscience modeling).



RSU consists of three conceptual Nodes, each acting as a functional lens on the total C³ state.




     : Measures cross-Unit synchrony at each time slice
     : Multi-band EEG synchrony
     : ACC, PCC, STG, Frontal Cortex
     : Yang et al., 2025





     MNI: [+8, +44, +12]
     Signal: PLV across Unit-critical regions
     Description: Degree to which separate Units exhibit synchronous resonance





     : Aggregates emotional, tonal, and spectral harmony into a single perceptual clarity index
     : Consonance fusion index
     : NAcc, Amygdala, Broca, PMC
     : Salimpoor et al., 2011





     MNI: [+12, +10, -10]
     Description: Integrated emotional–harmonic salience field
     Method: Entropy-weighted BOLD + EEG synchrony sum





     : Computes total resonance field from all C³ Units
     : Cross-unit vector field
     : DMN, Sensorimotor Network, Limbic System
     : SRC⁹ Master Report





     MNI: N/A (meta-region spanning all prior Units)
     Description: Final vector representation of listener state
     Output: Dynamic resonance map





<table><tr><td>
</td><td></td><td></td><td></td><td></td></tr><tr><td>
Unified Coherence</td><td>[+8, +44, +12]</td><td>ACC, STG, PCC, Frontal</td><td>EEG Multi-Band Coherence</td><td>Yang et al., 2025</td></tr><tr><td>Consonance Clarity</td><td>[+12, +10, -10]</td><td>NAcc, Broca, Amygdala, PMC</td><td>fMRI + EEG Fusion</td><td>Salimpoor et al., 2011</td></tr><tr><td>Network Fusion</td><td>—</td><td>Multi-unit Meta Layer</td><td>All prior Unit inputs</td><td>SRC⁹ Report (2025)</td></tr><tr><td></td></tr></table>


These form the top-level projection in the Resonance Map, a composite data structure exported per listener or per musical piece.



RSU enables system-wide monitoring and integrated decision-making:


     Real-time tracking of full cognitive-emotional resonance
     Personalized resonance profile calculation
     Macro-temporal analysis (e.g., identifying resonance arcs over time)


It often acts as a target state:


     For adaptive AI generation
     For guided composition
     For neurofeedback therapy





     Therapeutic optimization: Detect optimal convergence of memory, attention, and emotion
     Neuro-symbolic composition: Use RSU trajectory to sculpt musical form
     Resonance fingerprinting: Build listener resonance signatures for adaptive playlists


RSU is the endpoint and also the summary interface of the C³ system. It translates complex internal dynamics into usable, visible, and actionable outputs.







C³ operates on a three-tiered architecture, in which each UNIT is subdivided into Nodes, and each Node consists of one or more Elements.

This design allows C³ to represent cognition at multiple resolutions:


<table><tr><td>
</td><td></td><td></td></tr><tr><td>
UNIT</td><td>Macro cognitive subsystem (e.g., Emotion, Memory)</td><td>9</td></tr><tr><td>NODE</td><td>Functional construct (e.g., Flow, Expectation)</td><td>3–4/unit</td></tr><tr><td>ELEMENT</td><td>Measurable neural signal (EEG, fMRI, MEG)</td><td>2–5/node</td></tr><tr><td></td></tr></table>


Each Element is traceable to:


     A neurophysiological method (e.g., EEG phase coherence, ERP, fMRI BOLD)
     A brain region (MNI coordinates)
     A citation (peer-reviewed neuroscientific literature)




A Node in C³ represents a mid-level computational module with a singular cognitive or affective function.

Mathematically:

\[
_ij(t) = _k=1^M_ij w_ijk  _ijk(t)
\]

Where:


     \(_ij(t)\): The \(j\)-th Node in the \(i\)-th Unit at time \(t\)
     \(w_ijk\): Element weight within the Node (typically 0.25–0.50 normalized)
     \(_ijk(t)\): The \(k\)-th Element in that Node


Each Node acts as a resonance transformer, mapping local signals into cognitive-scale responses (e.g., surprise, salience, tension, familiarity, arousal).



An Element is the atomic analytical unit of C³. It consists of:


<table><tr><td>|

</td><td></td><td></td></tr><tr><td>
</td><td>string</td><td>e.g., ``EEG Gamma Activation''</td></tr><tr><td></td><td>enum</td><td>``EEG'', ``fMRI'', ``MEG'', or hybrid</td></tr><tr><td></td><td>list[string]</td><td>Anatomical targets (e.g., ACC, STG)</td></tr><tr><td></td><td>list[int]</td><td>MNI coordinates for GlassBrainMap</td></tr><tr><td></td><td>time series</td><td>Raw or preprocessed signal (external input)</td></tr><tr><td></td><td>float</td><td>Normalized resonance value at time \(t\)</td></tr><tr><td></td><td>string</td><td>Reference to literature</td></tr><tr><td></td></tr></table>


Each Element is initialized by parsing its signal from a time series and rescaling it:

\[
(t) = (x) - (x)
\]

where \(x(t)\) is derived from EEG amplitude, ERP waveform, BOLD \(z\)-score, or spectral entropy.



Each Element operates on a sliding time window (typically 100–250 ms). Activation values are updated in real-time (or simulated) via signal ingestion pipelines. Signals may be:


      (e.g., live EEG via OpenBCI or Emotiv)
      (e.g., , ,  time series)
      (e.g., parameterized sine waves for prototyping)


At each frame:


     The Element computes \((t)\)
     The Node aggregates these into a Node response
     The Unit aggregates across its Nodes
     RSU receives the integrated output for synthesis




All Nodes across different Units are mapped to a common scale:

\[
_ij(t)  [0, 1]
\]

 aligned across Units\\
 displayable in parallel or stacked timelines

This standardization allows for:


     Comparative graphing (e.g., AOU vs. CTU over time)
     Cluster analysis (e.g., grouping similar emotional-motor patterns)
     Synchronization tracking (e.g., NSU and SRU phase alignment)




All Nodes and Elements are encoded in structured JSON, enabling flexible API calls, visual rendering, and machine learning input.



<pre><code>

  "node_id": "ctu_dissonance",
  "unit": "CTU",
  "elements": [
    
      "id": "phase_locking",
      "method": "EEG",
      "regions": ["DLPFC", "ACC"],
      "mni": [32, 50, 20],
      "value": 0.64,
      "citation": "Fishman et al., 2001"
    
  ]

</code></pre>

These files are stored in:

<pre><code>
/static/data/c3/
├── ctu.json
├── aou.json
├── ...
</code></pre>

and loaded via:

<pre><code>
<NodeView unitPath="c3/ctu.json" />
</code></pre>



Each Element is visually represented in the GlassBrain system:


     MNI coordinates are mapped to SVG/canvas projection space
     Tooltip shows , \((t)\), and 
     Color intensity reflects \((t)\)
     Click opens corresponding Node view


For example:

 \(\)  \(\) 

This allows cognitive + anatomical views to be seamlessly unified.



Nodes and Elements are the computational heart of C³. They connect musical structure to brain dynamics via:


     Quantified, normalized, temporally resolved signals
     Modular aggregation from bottom (signal) to top (unit)
     Flexible visual and API-driven interfacing


This architecture makes it possible to compute, visualize, and interpret musical cognition with unprecedented resolution, precision, and interactivity.







The C³ system is not purely symbolic or abstract. It is neuroanatomically grounded. Each Element is tied to a measurable neural signal in a specific brain region. The spatial mapping of these regions:


     Provides anatomical context for cognitive resonance
     Enables dynamic visualizations of neural activation patterns
     Supports inter-unit interaction modeling via spatial proximity and network overlap


To achieve this, C³ integrates a dedicated visualization layer called , which combines:


     A high-resolution 2D SVG anatomical brain model
     A JSON-based coordinate database
     A real-time rendering interface () for visualizing activation dynamics




All Elements in C³ are associated with  coordinates, the standard in neuroimaging.

Each Element record includes:


     : \([x, y, z]\) triple
     : anatomical label (e.g., SMA, ACC, STG)
     : spatial spread (default: 5–8 mm sphere)
     : resonance strength at time \(t\)
     : literature source validating the location and signal


This system allows each Element to be visualized as a spatial field centered on its coordinate, colored or scaled based on its activation level.



All anatomical data is stored in a JSON file:

<pre><code>

  "node_id": "ctu_dissonance",
  "unit": "CTU",
  "region": "DLPFC",
  "mni": [32, 50, 20],
  "radius_mm": 6,
  "tooltip": "EEG α–β phase locking – Fishman et al., 2001"

</code></pre>

 

These are cross-referenced with each Unit’s data file (e.g., ) and visualized in real time.



The spatial interface is rendered by a React component () using:


     An interactive SVG brain diagram ()
     Overlaid  elements for each active Node/Element
      attributes for hover-based tooltips
     Optional  behavior for Unit/Node navigation


Sample visualization logic:

<pre><code>
<circle
  cx=svgX
  cy=svgY
  r=radius
  fill=`rgba(0, 255, 255, $value)`
  stroke="white"
  title="CTU – Phase Locking (α–β) – 0.64"
/>
</code></pre>

This allows for live, frame-by-frame cognitive activity visualization, spatially grounded in the brain’s structure.



There are three core layers in the spatial model:


<table><tr><td>|

</td><td></td></tr><tr><td>
Anatomical Base</td><td>Outlines brain shape and region boundaries (SVG)</td></tr><tr><td>Functional Points</td><td>Locations of Nodes and Elements (MNI coordinates)</td></tr><tr><td>Dynamic Overlay</td><td>Time-resolved activity values, color-encoded</td></tr><tr><td></td></tr></table>


This tri-layer system makes it possible to animate, filter, and interact with complex neurocognitive fields.



The visualization supports:


      Node/Element label, region, value, citation
      Links to full Node/Unit documentation
      Resonance values encoded via color gradients
      Per-frame re-rendering as values change over time


Optionally, spatial dynamics can be extended via:


     Highlight paths of cross-unit coherence (e.g., CTU \(\) AOU links)
     Animate resonance flow across hemispheres
     Implement attention-focused zooms or heatmaps




GlassBrainMap is fully synchronized with C³'s time engine. At each time frame:


     Each active Element’s \((t)\) is fetched
     Its color/opacity is updated in the SVG
     The cumulative field is rendered (or stored/exported)


This creates a dynamic 2D projection of 3D cognition, visible in real time or as an analytic export (e.g., heatmap or trajectory animation).



 transforms C³ from a symbolic system into a neuroanatomically immersive experience.

It allows researchers, composers, and clinicians to:


     See where cognition happens
     Visualize how multiple Units interact in space
     Trace temporal arcs of resonance across the brain


It is the spatial backbone of the C³ model—anchoring abstract resonance vectors in biological reality.







C³ operates as a discrete-time cognitive system, segmented into frames that represent slices of neural and musical time. The standard frame rate is 10 Hz (i.e., one frame every 100 ms), which aligns with:


     The optimal EEG resolution for cross-frequency tracking
     The perceptual threshold for auditory events
     The temporal granularity of musical microstructures (e.g., eighth notes at \(\)120 BPM)


Each frame computes:


     Element values
     Node aggregations
     Unit outputs
     RSU synthesis
     GlassBrainMap spatial updates




At each time \(t\), C³ executes the following pipeline:


From EEG, fMRI, JSON, or simulated sources

Preprocessed into normalized time series \(x(t)  [0, 1]\)



\[
_ijk(t) = (x) - (x)
\]



\[
_ij(t) = _k w_ijk  _ijk(t)
\]



\[
_i(t) = _j w_ij  _ij(t)
\]



\[
(t) = _i w_i  _i(t)
\]




     Update GlassBrainMap based on Element MNI locations
     Animate frame-by-frame neural states




C³ supports multiple signal input modes:


<table><tr><td>|

</td><td></td><td></td></tr><tr><td>
Live</td><td>Real-time EEG (e.g., OpenBCI)</td><td>Neurofeedback, adaptive performance</td></tr><tr><td>Simulated</td><td>Parametric sine/square models</td><td>Testing, theory exploration</td></tr><tr><td>Imported</td><td>Pre-processed data (CSV, JSON)</td><td>Research replay, batch analysis</td></tr><tr><td>Model-based</td><td>Outputs from R³/S³ modules</td><td>Internal feedback integration</td></tr><tr><td></td></tr></table>


Signals are time-aligned across all Units and stored in synchronized arrays for each frame:

<pre><code>

  "time": 4.3,
  "units": 
    "CTU": 0.62,
    "AOU": 0.78,
    "PIU": 0.81,
    ...
  

</code></pre>



C³ is built to support feedback mechanisms in which system output can influence future inputs, creating an adaptive cognitive engine.

For example:


     CTU output triggers simplification of harmonic content
     PIU spikes increase ambient density to sustain immersion
     RSU target value adjusts music generation parameters


This enables closed-loop interaction with:


     AI composition engines
     VR/AR environments
     Biometric controllers




Over time, C³ constructs a resonance trajectory:

\[
^3(t) = [_1(t), _2(t), , _9(t)]
\]

This vector evolves in 9D space and can be used for:


     Pattern recognition (e.g., flow state, surprise burst)
     Phase analysis (e.g., C³ spirals, convergence cycles)
     Emotion curves (e.g., AOU vs. CTU arcs)


Visualization options include:


     Line graphs of individual Units
     Radar plots at each time slice
     PCA/t-SNE dimensionality reduction of trajectory clusters




C³ uses a master temporal clock, driven by:


     External sync (e.g., MIDI timecode, DAW sync)
     Internal clock (browser animation loop, WebAudio API)
     Real-time EEG timestamps


Each Unit’s activity is synchronized to this clock, ensuring that:


     Temporal granularity remains constant
     Cross-unit integration remains coherent
     GlassBrainMap overlays animate in lockstep




All C³ output is exportable in structured formats:


      frame logs
      for analysis in Python/R
     ,  GlassBrainMap video renderings
      proprietary container for replay in simulation environments


This makes C³ both real-time and archival—suitable for live use, study, and post-hoc analysis.



C³'s temporal engine transforms raw signals into a flowing stream of cognitive resonance. Through:


     Frame-level computation
     Adaptive signal modeling
     Time-synchronized spatial projection


C³ achieves a unique combination of precision, fluidity, and biological plausibility. Its ability to track cognition across milliseconds to minutes enables deep insight into the evolving experience of music.







C³ provides a groundbreaking opportunity for neurophysiologically grounded music therapy by quantifying and tracking brain-based responses to musical structure in real time.



Using AOU (affective orientation) and IRU (interpersonal resonance), therapists can monitor emotional states through:


     EEG alpha asymmetry (valence)
     fMRI amygdala–insula activation (arousal)
     Cross-brain synchrony (group therapy contexts)


This allows dynamic adjustments to:


     Musical content
     Patient feedback loops
     Group synchrony states




SAU (semantic-autobiographical) and PIU (phenomenological immersion) can support:


     Memory retrieval and reconsolidation through tonal cues
     Safe immersive states using ambient or flow-inducing structures
     Repatterning of trauma-linked neural pathways through predictable rhythmic entrainment




SRU (somatic resonance) and NSU (neural synchronization) enable:


     Gait training via rhythm-guided beta entrainment
     Entrainment of cerebellar and basal ganglia networks
     Personalized tempo and meter calibration for stroke or Parkinson’s patients


C³ becomes a neural interface for musical medicine.



The C³ system opens a new frontier in cognitively aware artificial intelligence for music.



Generative music models (e.g., RNN, Transformer, Diffusion) can be guided by:


     Desired C³ trajectory (e.g., AOU\(\), CTU\(\), PIU\(\))
     Listener feedback via EEG input
     Emotional or narrative arc templates


This enables biometrically reactive music—sound that shifts in real time to support immersion, attention, or relaxation.



With C³ embedded in AI systems:


     Adaptive film/game scoring becomes possible
     Live feedback concerts (brainwave-to-music mapping) can be executed
     Human–AI co-composition becomes cognitively contextualized




C³ supports a neuroscience-informed pedagogy of music.



Educators can craft listening experiences and composition exercises that:


     Highlight tension and resolution (CTU)
     Elicit attention and expectation (IEU)
     Reinforce memory and identity through tonality and timbre (SAU)


This makes abstract musical concepts experientially grounded.



Via live EEG (or simulated training environments), educators can track:


     Focus/engagement states (PIU, NSU)
     Confusion or overload (CTU spikes)
     Flow progression across musical segments


This enables adaptive instruction, where the learning path is guided by resonance curves.



C³ is an interpretive and generative tool for composers, performers, and sound artists.



A composer can model the resonance profile of a piece before it's written by:


     Designing C³ trajectories (e.g., tension peak at minute 3)
     Mapping structure to cognitive states
     Back-calculating harmonic/rhythmic features to meet those trajectories


This allows intentional cognitive shaping of musical form.



C³’s spatial and temporal outputs can drive:


     Live projection of GlassBrainMap overlays during performances
     Audience-specific scoring where real-time neural data sculpts the score
     Installation works based on shared inter-brain synchrony fields (IRU)


This links cognition, composition, and computation.



C³ provides a framework for computational music cognition, enabling:


     Hypothesis testing: Does syncopation increase CTU + SRU?
     Cross-style comparison: How does Bach’s AOU curve differ from EDM?
     Cultural modeling: What tonal structures evoke strongest SAU response in different populations?


This allows neurocognitive theories of music to become quantifiable and testable.



C³ is not only a theoretical model—it is an instrument:


     For therapy, it monitors and modulates emotional and neural states
     For AI, it contextualizes generation with cognitive targets
     For education, it reveals the brain’s role in musical understanding
     For artistry, it expands the palette of expression
     For science, it bridges abstract theory and empirical verification


It transforms music from something we hear to something we can measure, sculpt, and share as a resonant experience.









C³ is designed to operate not only on pre-recorded data, but in real-time with live neural input.


     EEG headsets (e.g., OpenBCI, Emotiv, Muse S) can be connected via WebSocket or OSC
     C³ frame computation can be run inside a web browser, Node.js, or Unity engine
     Live frame values (e.g., CTU = 0.62, PIU = 0.83) can be:
    
         Fed into generative AI models
         Used to control audiovisual parameters
         Exported to researchers in JSON or OSC streams
    


This allows for neuroadaptive installations, brain–music feedback loops, and bio-interactive concerts.



Planned integrations:


<table><tr><td>|

</td><td></td></tr><tr><td>
OSC Module</td><td>Open Sound Control bridge to DAWs, synths, AI tools</td></tr><tr><td>WebSocket API</td><td>Lightweight protocol for browser–EEG interaction</td></tr><tr><td>Max/MSP Patch</td><td>Native support for creative coding environments</td></tr><tr><td>Python Streamer</td><td>Tensor-based streaming for machine learning pipelines</td></tr><tr><td></td></tr></table>




C³’s GlassBrainMap + Cognitive Trajectory outputs can be rendered in 3D and embedded in immersive environments.




     Dynamic brain overlays mapped to VR avatars
     Real-time resonance color fields, spheres, and graph networks
     Flow-state visualizations projected in 3D space


This enables experiential neuroaesthetics—where listeners "enter" their own cognition, or the resonance fields of a performance.



By combining C³ with spatialized audio systems (e.g., Ambisonics, Dolby Atmos), the system can:


     Route musical streams to match neural states
     Trigger directional auditory cues based on attention/flow values
     Modify sonic parameters based on resonance field tension




Using IRU (Interpersonal Resonance Unit), future expansions include:


     Multi-brain environments (e.g., group concerts, collective biofeedback)
     Cross-brain synchrony maps showing where people resonate together
     Shared immersive environments where cognition is mapped and compared in real time


This allows researchers to study social cognition, empathy, and music-induced synchrony at scale.



C³ is not a standalone tool; it is a cognitive middleware layer that can plug into:


<table><tr><td>|

</td><td></td></tr><tr><td>
Therapy</td><td>Real-time monitoring of neural states</td></tr><tr><td>Gaming/VR</td><td>Adaptive scoring, ambient shifts, player cognition</td></tr><tr><td>Education</td><td>Flow detection, rhythm learning enhancement</td></tr><tr><td>AI Art Tools</td><td>Emotion-matched generative feedback</td></tr><tr><td>Scientific Research</td><td>Hypothesis testing, data export, meta-analysis</td></tr><tr><td></td></tr></table>


C³ becomes the neuro-semantic glue between music, systems, and cognition.






     Full browser-based JSON pipeline
     Interactive web-based GlassBrainMap
     C³-powered experimental composition tools
     Python and JavaScript SDKs for Unit simulation and visualization





     EEG integration via OSC/WebSocket
     Unity + WebGL resonance rendering
     Composer dashboard for real-time Unit tracking
     Music therapy pilot studies with neurofeedback





     Full real-time C³ AI integration
     Multi-user cross-brain resonance platform
     Publication and standardization of the C³ framework
     Institutional and clinical partnerships




C³ is intended to be an open, modular, scientifically verifiable system.


     All Units, Nodes, and Elements are defined in transparent JSON
     Coordinate systems are public and validated (MNI-space)
     Citations and data trails are embedded and reproducible
     Community contributions can add new literature, modules, or brain regions


A future  will:


     Allow users to run and visualize C³ sessions
     Share annotated resonance maps
     Explore collective cognitive musical responses




C³ is a foundation—not a ceiling.

It creates the infrastructure for real-time cognition-aware music systems, research tools, therapeutic protocols, and artistic interfaces. Its future lies not in complexity, but in connectivity: between minds, modules, and meaning.







The Cognitive Consonance Circuit (C³) is the first fully formalized system that models the brain’s multi-dimensional, time-evolving response to music by integrating:


     Symbolic and spectral features (via S³ and R³)
     Measurable neural signals (EEG, fMRI, MEG)
     Cognitive constructs (attention, memory, emotion, expectation)
     Anatomical regions (mapped in MNI coordinates)
     Temporal dynamics (frame-level, real-time computation)


Through its layered architecture (UNIT \(\) NODE \(\) ELEMENT), mathematically grounded resonance equations, and GlassBrain-based spatial visualization, C³ offers an unprecedented resolution of musical cognition.



C³ introduces the following key innovations to the scientific community:



No existing model combines:


     Tonal/spectral analysis
     Neural measurement
     Cognitive state modeling


in a modular, interconnected, and computable system.

C³ bridges this gap with:


     9 Units modeling distinct cognitive faculties
     Direct neurophysiological grounding (61+ referenced papers)
     MNI-based anatomical mapping




C³ treats musical cognition as a flowing resonance field. Its time-based equations:


     Allow computation of dynamic neural states
     Integrate with live EEG streams
     Align with musical structure at 10 Hz resolution


This transforms music analysis from static snapshots into real-time neurocognitive simulation.



C³ is not standalone. It:


     Receives input from S³ (spectral structure) and R³ (harmonic topology)
     Computes the cognitive response
     Sends feedback back into the system (e.g., suggesting structural change)


This feedback loop allows musical systems to self-regulate based on real or simulated cognition.



The GlassBrainMap is not just an illustration—it’s an analytical engine:


     Every Element is spatially mapped (MNI)
     Activation is color-coded and time-resolved
     Tooltips, click-through, and overlays show layered cognition


This creates a semantic spatial interface for neurocognition.



C³’s design enables direct application in:


     Music cognition research (resonance modeling, EEG studies)
     Therapy (trauma, memory, flow states)
     AI systems (neuro-informed generative music)
     Education (real-time student cognition tracking)
     Creative tools (resonance-driven composition)


It acts as a cognitive middleware between human experience and musical structure.



C³ reframes music analysis as a dynamic, spatial, and measurable cognitive process. It suggests that:


     Musical meaning is not only symbolic, but resonant
     Cognition is not reactive, but synchronizing
     Emotion is not qualitative, but quantifiable
     Listening is not passive, but spatiotemporal entrainment


By building structure, signal, and spatiality into one coherent system, C³ proposes a new paradigm:





C³ is not a finished system—it is a platform for growth.


     New Units can be defined
     New data sets can be mapped
     Neural signals can be updated via future modalities
     Interfaces can expand to immersive VR, generative AI, and cross-brain networks


With the foundation laid, the challenge is now collective elaboration: for researchers, artists, clinicians, and technologists to expand, iterate, and apply C³ across domains.



The C³ system is open to research collaboration, institutional deployment, and creative integration. It was built not just to model cognition—but to enable new relationships between music, mind, and machine.



 \\[0.2cm]
 Thank you for your vision, precision, and trust in building this paradigm together.















