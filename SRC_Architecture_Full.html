<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SRC Architecture (Full)</title>
    <style>
        body {
            background-color: #111;
            color: #eee;
            font-family: monospace;
            padding: 2rem;
        }
        pre {
            white-space: pre-wrap;
            word-wrap: break-word;
        }
    </style>
</head>
<body>
    <pre>\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{hyperref}

\geometry{margin=1in}

\title{\textbf{SRC$^{9}$ MASTER TECHNICAL REPORT}}
\date{}
\begin{document}

\maketitle
\section*{Chapter I — What is SRC$^{9}$?}

\subsection*{1.1 Definition and Rationale}

SRC$^{9}$, or the \textbf{Spectral–Resonance–Cognitive} system, is an integrated, multi-layered computational framework that models how music transforms from raw sound into perceived structure and cognitive meaning. It is composed of three tightly coupled modules:

\begin{itemize}
    \item \textbf{S$^{3}$ – Spectral Sound Space}: A high-resolution spectral analysis engine that extracts the fundamental acoustic content of music, including partials, harmonics, microtonal deviations, and time–frequency–amplitude relations.
    
    \item \textbf{R$^{3}$ – Resonance-Based Relational Reasoning}: A harmonic reasoning engine that interprets spectral data through scalar field theory, resonance topology, and psychoacoustic principles to model musical structure without symbolic assumptions.
    
    \item \textbf{C$^{3}$ – Cognitive Consonance Circuit}: A neurophysiologically grounded model of perceptual resonance and emotional impact, structured into nine cognitive units derived from EEG/fMRI literature and organized into measurable, time-varying neural signatures.
\end{itemize}

The superscript 9 ($^{9}$) denotes the system's expansion into \textbf{nine cognitive dimensions}, each modeled as a unique \textit{Unit} in the C$^{3}$ architecture, and accessible from the resonance outputs of R$^{3}$ and the acoustic signals of S$^{3}$.

\subsection*{1.2 Motivation: Why SRC$^{9}$?}

Traditional music theory frameworks start from notation and seek meaning through pre-defined symbolic systems. Meanwhile, modern machine learning models such as Jukebox or Magenta generate musical output with no semantic or cognitive interpretability. Neuroscientific studies, though rich in EEG/fMRI data, lack a bridge to music-theoretical relevance.

SRC$^{9}$ was created to solve this cross-domain disconnect by:

\begin{enumerate}
    \item Starting with physical audio (not symbolic input)
    \item Modeling resonance fields and energy topologies, not abstract pitch classes
    \item Mapping these fields into time-aligned neural signatures rooted in empirical neuroscience
\end{enumerate}

\subsection*{1.3 System Overview: A Perception-to-Structure Pipeline}

SRC$^{9}$ operates as a real-time or batch-based pipeline:

\[
\text{Audio Signal (Waveform)} 
\rightarrow \textbf{S$^{3}$} \rightarrow \textbf{R$^{3}$} \rightarrow \textbf{C$^{3}$} 
\rightarrow \text{Feedback}
\]

\begin{itemize}
    \item \textbf{S$^{3}$} produces framewise spectral data: $\text{partials}_t = \{\text{freq}, \text{dB}, \text{symbol}, \text{harmonic index}\}$
    \item \textbf{R$^{3}$} computes: Phantom Root (PR), Resonance Potential ($\Phi$), Resonance Field Map (RFM), Cognitive Resonance Vector (CRV)
    \item \textbf{C$^{3}$} receives these values to drive:
    \begin{itemize}
        \item Tension mapping (CTU)
        \item Affective modeling (AOU)
        \item Memory alignment (SAU)
        \item Expectation violation (IEU)
        \item Group synchrony and attention modeling (IRU, NSU)
    \end{itemize}
\end{itemize}

Each stage of SRC$^{9}$ is aligned to a common temporal resolution (typically 0.1s), enabling multimodal synchronization with EEG, fMRI, audio playback, or Unity-based interactive scenes.

\subsection*{1.4 Formal Notation}

Let $x(t)$ be the input waveform segmented into overlapping frames.

From S$^{3}$:
\[
\text{Frame}_i = \{f_0, f_1, ..., f_{16}\}, \quad \text{where each } f_n = \text{partial}_{i,n}
\]

From R$^{3}$:
\[
\text{PR}_i, \Phi_i, \text{RFM}_i(f), \text{CRV}_i = \langle \text{TPS}_i, \text{TFI}_i, \text{NSF}_i \rangle
\]

From C$^{3}$:
\[
C^{3}(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t), \quad \text{Unit}_i(t) = \sum_j w_{ij} \cdot \text{Node}_j(t)
\]

Each \textbf{Node} is an EEG/fMRI-derived observable (e.g., $\beta$ phase-locking, MMN, BOLD z-score), anchored anatomically using MNI coordinates in a 3D GlassBrainMap interface.

\subsection*{1.5 Multimodal Feedback}

The architecture supports \textbf{bidirectional modulation}:

\[
\text{C$^{3}$ feedback} \Rightarrow \text{R$^{3}$ weighting} \Rightarrow \text{S$^{3}$ filter adjustment}
\]

For example:

\begin{itemize}
    \item High CTU (tension) output may modulate the harmonic weights in R$^{3}$'s $\Phi$ calculation
    \item Strong PIU (immersion) may suppress noisy partials in S$^{3}$’s visualization layer
    \item SAU (semantic memory) may trigger audio annotations or dynamic re-sequencing
\end{itemize}

This loop forms the foundation of responsive, perceptually aware music analysis, generation, or education platforms.

\subsection*{1.6 Implementation Philosophy}

SRC$^{9}$ is:

\begin{itemize}
    \item \textbf{Scientific}: Grounded in psychoacoustics, computational neuroscience, and acoustic theory
    \item \textbf{Modular}: Built with independently executable units, fully API-controllable
    \item \textbf{Visual}: Every layer has an interpretable, dynamic output (HTML, PNG, 3D)
    \item \textbf{Interactive}: Exports to Unity, GlassBrainMap, OSC/VR platforms
    \item \textbf{Cognitively honest}: Models not just structure, but perception
\end{itemize}

\section*{Chapter II — Modular Dimensions of SRC$^{9}$}

SRC$^{9}$ is divided into three orthogonal modules: S$^{3}$ (Spectral), R$^{3}$ (Resonance), and C$^{3}$ (Cognitive). Each functions as an independent layer in the signal–structure–perception continuum, while maintaining tight alignment through shared temporal schemas, compatible data models, and reciprocal feedback.

\subsection*{2.1 S$^{3}$ — Spectral Sound Space}

\paragraph{Function:}  
Transforms raw audio into high-resolution spectral frames by extracting:

\begin{itemize}
  \item Fundamental frequency ($f_0$)
  \item Harmonic partials (1–16)
  \item Amplitude in dBFS
  \item Microtonal pitch symbols (e.g., A4⁺¹, C3⁻²)
\end{itemize}

\paragraph{Output Format:}  
JSON array of frames at 0.1s intervals:

\begin{verbatim}
{
  "time": 1.2,
  "partials": [
    { "freq": 440.0, "db": -12.4, "isFundamental": true, "symbol": "A4⁰" },
    { "freq": 880.0, "db": -18.2, "harmonic_index": 2, "symbol": "A5⁰" }
  ]
}
\end{verbatim}

\paragraph{Tools Used:}

\begin{itemize}
  \item CREPE (f₀ extraction)
  \item librosa (STFT, RMS, cent conversion)
  \item Custom Python pipeline with modular scripts
\end{itemize}

\paragraph{Scientific Rationale:}  
Inspired by spectral music theory (Grisey, Murail) and auditory physiology (tonotopic mapping), S$^{3}$ treats the frequency domain as the true substrate of musical identity — discarding staff notation and tuning system assumptions.

\subsection*{2.2 R$^{3}$ — Resonance-Based Relational Reasoning}

\paragraph{Function:}  
Processes S$^{3}$ outputs to identify and model harmonic structure, not via tonal syntax, but via energetic interaction between partials.

\paragraph{Core Units:}

\begin{itemize}
  \item PRU — Phantom Root Unit: Detects implied fundamentals from overtone sets
  \item RPU — Resonance Potential Unit: Computes scalar coherence $\Phi$ per frame
  \item RFMU — Resonance Field Modeling Unit: Generates Gaussian field over frequency
  \item CRVU — Cognitive Resonance Vectoring Unit: Extracts TPS, TFI, NSF metrics
\end{itemize}

\paragraph{Field Representation:}

\[
\text{RFM}(f, t) = \sum_i A_i(t) \cdot e^{-\frac{(f - f_i(t))^2}{2\sigma^2}}
\]

This formula defines a scalar resonance density field over log-frequency space.

\paragraph{Resonance Vector:}
\[
\vec{\text{CRV}} = [\text{TPS}, \text{TFI}, \text{NSF}] \in [0,1]^3
\]

Used as the summary output of all R$^{3}$ activity and as input to C$^{3}$ modules.

\paragraph{Scientific Basis:}  
Builds on psychoacoustic roughness theory (Plomp & Levelt), neural entrainment (Bidelman), and just intonation topology (Sethares, Tymoczko).

\subsection*{2.3 C$^{3}$ — Cognitive Consonance Circuit}

\paragraph{Function:}  
Models how humans perceive, evaluate, and emotionally respond to the harmonic signals computed in R$^{3}$. Each response is neurophysiologically grounded and structured by a 9-Unit circuit.

\paragraph{C$^{3}$ Units:}

\begin{itemize}
  \item CTU — Cognitive Tension
  \item AOU — Affective Orientation
  \item IEU — Intuitive Expectation
  \item SRU — Somatic Resonance
  \item SAU — Semantic-Autobiographical
  \item PIU — Phenomenological Immersion
  \item IRU — Interpersonal Resonance
  \item NSU — Neural Synchronization
  \item RSU — Resonance Synthesis (summary vector)
\end{itemize}

\paragraph{Equation:}
\[
C^{3}(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t)
\quad \text{where} \quad
\text{Unit}_i(t) = \sum_j w_{ij} \cdot \text{Node}_j(t)
\]

Each Node is mapped to EEG/fMRI features (e.g., alpha asymmetry, gamma coherence, BOLD z-scores) and anatomically located via MNI coordinates in the \textit{GlassBrainMap}.

\paragraph{Data Flow:}
CRVU $\rightarrow$ CTU, AOU, PIU, NSU \\
Neural feedback loops modify RFM weighting, Φ computation, and partial salience in S$^{3}$.

\paragraph{Scientific Integration:}  
Combines computational music cognition (Lerdahl, Huron), neuroaesthetics (Zatorre, Koelsch), and brain–music entrainment literature.

\section*{Chapter III — Mathematical Foundations of SRC$^{9}$}

SRC$^{9}$ formalizes music cognition through a hierarchy of equations, resonance functions, and vector spaces that bridge physical sound, psychoacoustic interaction, and perceptual abstraction.

\subsection*{3.1 Frame-Based Signal Model}

Let the raw audio input be a continuous time-domain waveform $x(t)$. SRC$^{9}$ processes this waveform in fixed-length, overlapping windows:

\begin{equation}
x_i(t) = x(t + iH), \quad \text{for frame } i
\end{equation}

Where:
\begin{itemize}
  \item $H$ = hop size (e.g., 10 ms)
  \item $x_i(t)$ = time-domain windowed signal
\end{itemize}

Each frame is then passed into CREPE or equivalent pitch tracking module to estimate:

\begin{equation}
f_{0i}, \quad A_i, \quad \text{partials } \{f_{in}\}_{n=1}^{16}
\end{equation}

These define the fundamental + harmonic space used across the system.

\subsection*{3.2 Resonance Potential Equation (Φ)}

The scalar measure $\Phi$ represents the instantaneous coherence of all spectral partials within a frame:

\begin{equation}
\Phi(t) = \sum_{i<j} \frac{A_i(t) \cdot A_j(t)}{|f_i(t) - f_j(t)| + \epsilon}
\end{equation}

Where:
\begin{itemize}
  \item $f_i(t), A_i(t)$ = frequency and amplitude of partial $i$
  \item $\epsilon$ = small constant to avoid division by zero
\end{itemize}

Interpretation:
\begin{itemize}
  \item Higher amplitude → more weight
  \item Smaller interval → stronger resonance
\end{itemize}

This equation generalizes roughness and consonance models using continuous frequency data.

\subsection*{3.3 Resonance Field (RFM)}

To represent spectral resonance topographically, a Gaussian kernel density is applied across a log-frequency grid:

\begin{equation}
\text{RFM}(f, t) = \sum_i A_i(t) \cdot e^{-\frac{(f - f_i(t))^2}{2\sigma^2}}
\end{equation}

This converts discrete spectral data into a continuous scalar field — a kind of “terrain map” of resonance.

\subsubsection*{Gradient Operator:}
To compute directionality of tonal pull:

\begin{equation}
\nabla \text{RFM}(f, t) = \frac{\partial \text{RFM}(f, t)}{\partial f}
\end{equation}

Used in CRVU → TFI to model spectral fusion.

\subsection*{3.4 Cognitive Vector (CRV)}

The final cognitive resonance vector is defined as:

\begin{equation}
\vec{\text{CRV}} = [\text{TPS}, \text{TFI}, \text{NSF}]
\end{equation}

Each metric is defined as:

\paragraph{TPS — Temporal Perceptual Stability:}
\begin{equation}
\text{TPS} = \frac{1}{1 + \sigma_\Phi(t)}
\end{equation}

\paragraph{TFI — Tonal Fusion Index:}
\begin{equation}
\text{TFI} = \frac{1}{1 + \langle |\nabla \text{RFM}(f, t)| \rangle}
\end{equation}

\paragraph{NSF — Neural Synchronization Field:}
\begin{equation}
\text{NSF} = \sum_t \Phi(t) \cdot e^{-\alpha t}
\end{equation}

Where $\alpha$ is a decay constant modeling attention/memory trace.

\subsection*{3.5 PR Estimation (Phantom Root)}

Let $\{f_1, f_2, ..., f_n\}$ be a group of detected pitch events. Then, the phantom root $r$ is the frequency that minimizes mean harmonic error:

\begin{equation}
r^* = \arg\min_r \left( \frac{1}{n} \sum_i \left| \frac{f_i - r \cdot h_i}{r \cdot h_i} \right| \right)
\end{equation}

Where $h_i$ are harmonic template integers (e.g., [1,2,3,4]).

\subsection*{3.6 Just Intonation Vector Representation}

Each frequency can be projected into prime-exponent vector space:

\begin{equation}
\vec{v}_i = (x_2, x_3, x_5, x_7, ...)
\quad \text{where } f_i = 2^{x_2} \cdot 3^{x_3} \cdot 5^{x_5} \cdots
\end{equation}

Mean vector yields:

\begin{equation}
\vec{v}_{PR} = \frac{1}{N} \sum_i \vec{v}_i
\end{equation}

Which is mapped back to the frequency domain to compute phantom root in symbolic-harmonic space.

\subsection*{3.7 Summary Table of Key Equations}

\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Metric} & \textbf{Equation} \\\hline
$\Phi$ (Resonance Potential) & $\Phi(t) = \sum_{i<j} \frac{A_i A_j}{|f_i - f_j| + \epsilon}$ \\\hline
RFM (Field) & $\text{RFM}(f, t) = \sum_i A_i e^{-(f - f_i)^2 / 2\sigma^2}$ \\\hline
Gradient & $\nabla \text{RFM} = \partial \text{RFM} / \partial f$ \\\hline
CRV & $[\text{TPS}, \text{TFI}, \text{NSF}]$ \\\hline
PR Estimation & $r = \arg\min_r \sum_i |f_i - r h_i| / r h_i$ \\\hline
NSF & $\sum_t \Phi(t) e^{-\alpha t}$ \\\hline
\end{tabular}
\end{center}

\section*{Chapter IV — Temporal Architecture and Synchronization}

Time is not merely a parameter in SRC$^{9}$; it is a structural axis along which all modules are synchronized, integrated, and compared. Every analytical unit, perceptual frame, and visual output is aligned to a common frame-based time grid, enabling coherence between real-time interaction, dynamic modeling, and retrospective analysis.

\subsection*{4.1 Frame Resolution}

All SRC$^{9}$ operations are executed in frames of fixed temporal resolution.

\begin{itemize}
  \item \textbf{Standard Frame Duration:} $\Delta t = 0.1$ seconds
  \item \textbf{Frames per 20-second audio:} $200$ frames
  \item \textbf{Aligned Across:} S$^3$ → R$^3$ → C$^3$
\end{itemize}

This resolution provides a compromise between cognitive relevance (auditory segmentation and beat-level processes) and computational tractability.

\subsection*{4.2 Temporal Data Schema}

Each frame is indexed and timestamped explicitly:

\begin{verbatim}
{
  "time": 3.2,
  "partials": [...],
  "phi": 2.83,
  "crv": {
    "TPS": 0.814,
    "TFI": 0.652,
    "NSF": 0.042
  }
}
\end{verbatim}

Additional time-windowed metrics (e.g., windowed Φ, RFM segments) use labeled intervals:

\begin{verbatim}
{
  "window": "5.0–8.0",
  "phi": 9.183,
  "window_size": 3
}
\end{verbatim}

All time-based data are synchronized via integer multiples of $\Delta t$.

\subsection*{4.3 Window-Based Aggregation}

Many perceptual phenomena operate on time windows rather than individual frames (e.g., expectancy, stability, modulation). To simulate this:

\begin{equation}
\Phi_T = \sum_{t=t_0}^{t_1} \Phi(t)
\quad \text{where } T = [t_0, t_1]
\end{equation}

Window lengths are configurable: 1s, 3s, 5s, or 7s (10–70 frames). 

These windows feed CRVU and symbolic inference layers, and provide smoothed curves for visualization.

\subsection*{4.4 Inter-Unit Time Sharing}

Each SRC$^9$ unit reads or writes data at the same temporal resolution, ensuring:

\begin{itemize}
    \item Synchrony between harmonic events and cognitive metrics
    \item Real-time overlay of Φ, RFM, PR, and CRV
    \item Accurate PRU segment demarcation based on CentTracker ($\pm49$c deviation)
\end{itemize}

Example: frame 38 at $t=3.8$s will contain:

\begin{itemize}
    \item 17 partials from S$^3$
    \item 1 $\Phi$ scalar from RPU
    \item RFM density array from RFMU
    \item 3-element CRV vector from CRVU
    \item Segment label if included in a PRU group
\end{itemize}

\subsection*{4.5 Real-Time Execution Model}

To support live streaming or reactive composition, each frame can be evaluated asynchronously. A frame handler listens for input, processes data, and stores results:

\textbf{Frame Pipeline:}

\[
\text{Frame}_t \Rightarrow \text{S$^3$ extract} \Rightarrow \text{R$^3$ process} \Rightarrow \text{C$^3$ interpret} \Rightarrow \text{Output + Feedback}
\]

Latency budget: $< 50$ ms per frame.

\subsection*{4.6 Timeline Synchronization with Audio/Video}

SRC$^9$ includes support for timeline-aligned playback and export:

\begin{itemize}
    \item \textbf{Unity integration:} \texttt{Time.time} $\leftrightarrow$ frame index
    \item \textbf{Audio export:} Link frame analysis to audio segments
    \item \textbf{Plotly visualizations:} Frame-aligned curves, scrollable graphs
\end{itemize}

Visual overlays are rendered in rasterized layers, each 216px tall, stacked into a 2160px 4K vertical space. These layers include:

\begin{itemize}
    \item RawSpectrum
    \item PRU
    \item RPU
    \item RFMU
    \item CRVU
\end{itemize}

\subsection*{4.7 Frame Integrity and Diagnostics}

Each frame includes metadata for traceability:

\begin{verbatim}
{
  "time": 12.3,
  "frame_id": 123,
  "source": "RawSpectrum01",
  "checksum": "ae347ac1...",
  "validated": true
}
\end{verbatim}

This ensures reproducibility and integrity in batch pipelines or dynamic environments.

\subsection*{4.8 Temporal Modeling Summary}

SRC$^9$ temporal architecture transforms time from a passive marker to an active modeling dimension. It enables:

\begin{itemize}
    \item Segment-based cognition modeling (e.g., tonal drift, root migration)
    \item Layer-aligned visualization of concurrent harmonic and perceptual states
    \item Real-time reactivity and temporal learning models
\end{itemize}

\section*{Chapter V — Data Structures and Interface Formats}

The analytical power of SRC$^{9}$ depends not only on its internal computations, but on its ability to represent, store, and exchange data in structured, interpretable, and extensible formats. This chapter outlines the file architectures, symbolic systems, and cross-platform export mechanisms that enable integration across scientific, educational, and creative platforms.

\subsection*{5.1 JSON Frame Format (Canonical)}

All unit processing in SRC$^{9}$ is time-aligned to frames in the following structure:

\begin{verbatim}
{
  "time": 3.2,
  "partials": [
    { "freq": 261.63, "amplitude": 0.84, "symbol": "C4⁰", "harmonic_index": 0, "isFundamental": true },
    { "freq": 523.25, "amplitude": 0.51, "symbol": "C5⁰", "harmonic_index": 1, "isFundamental": false }
  ],
  "phi": 2.83,
  "crv": { "TPS": 0.812, "TFI": 0.694, "NSF": 0.039 }
}
\end{verbatim}

\paragraph{Specifications:}
\begin{itemize}
    \item \texttt{time}: Timestamp in seconds
    \item \texttt{partials}: List of harmonic components with symbolic pitch
    \item \texttt{phi}: Frame-level $\Phi$ scalar
    \item \texttt{crv}: Cognitive resonance vector output (from CRVU)
\end{itemize}

\subsection*{5.2 Unit-Specific Outputs}

Each SRC$^{9}$ unit outputs a structured file:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit} & \textbf{File} & \textbf{Contents} \\
\hline
PRU & PR-unit-temporal.json & PR frequency, symbol, harmonic group, fusion score \\
RPU & RP-framewise.json, RP-windowed.json & $\Phi$ per frame or time window \\
RFMU & RFM-unit.json & Resonance field grid + gradient vector \\
CRVU & CRV-unit.json & Cognitive vector: TPS, TFI, NSF \\
\hline
\end{tabular}
\end{center}

All outputs are timestamp-aligned at 0.1s resolution.

\subsection*{5.3 Symbolic Microtonal Encoding}

SRC$^{9}$ uses a compact symbolic system to encode pitch with microtonal precision:

\begin{itemize}
    \item Format: \texttt{[PitchClass][Octave][Superscript]}
    \item Superscripts denote deviation in cents:
    \begin{itemize}
        \item $⁰$ = 0 cent
        \item $⁺¹$ = +25 cent
        \item $⁻¹$ = –25 cent
        \item $⁺²$ = +50 cent
    \end{itemize}
\end{itemize}

\textbf{Examples:}

\begin{itemize}
    \item \texttt{C4⁰} = C4 at 0c
    \item \texttt{A4⁺¹} = A4 +25 cents
    \item \texttt{G3⁻²} = G3 –50 cents
\end{itemize}

This allows symbolic readability while preserving microtonal resolution from S$^{3}$ partial tracking.

\subsection*{5.4 CSV Export for Unity and Visual Systems}

Unity and WebGL-based environments operate on line-by-line streaming. Each partial is exported as:

\begin{verbatim}
time,freq,amplitude,isFundamental,harmonic_index,symbol
1.0,220.0,0.82,True,0,G3⁰
1.0,440.0,0.51,False,1,G4⁰
\end{verbatim}

Used to instantiate prefabs or terrain meshes in:

\begin{itemize}
    \item \texttt{CSVLoader.cs}
    \item \texttt{SpectrumVisualizer.cs}
    \item \texttt{RFM Terrain Generator.cs}
\end{itemize}

\subsection*{5.5 Matrix and Vector Data Structures}

Internally, each frame can also be represented in matrix form for ML pipelines:

\[
\text{PartialMatrix}_t = 
\begin{bmatrix}
f_0 & A_0 & h_0 \\
f_1 & A_1 & h_1 \\
\vdots & \vdots & \vdots \\
f_{16} & A_{16} & h_{16}
\end{bmatrix}
\quad
\text{CRV}_t = 
\begin{bmatrix}
\text{TPS}_t \\
\text{TFI}_t \\
\text{NSF}_t
\end{bmatrix}
\]

This supports CRV-based AI composition, harmonic fingerprint learning, or real-time ML inference.

\subsection*{5.6 File Structure Conventions}

\begin{itemize}
    \item \texttt{../data/raw/} — RawSpectrum-unit.json
    \item \texttt{../data/output/PR/} — Phantom root segments
    \item \texttt{../data/output/RP/} — Framewise/windowed Φ
    \item \texttt{../data/output/RFM/} — Grid + gradient fields
    \item \texttt{../data/output/CRV/} — Cognitive vector layers
\end{itemize}

All files are UTF-8 encoded and stored in flat JSON or CSV formats for interoperability with scientific tools and frontend visual platforms.


---

```latex
\section*{Chapter VI — Visualization Layers and Multimodal Rendering}

SRC$^{9}$ is designed not only to compute resonance and cognition, but to render it visually. Every analytical layer, from raw spectral frames to cognitive vectors, is projected into a coherent visual system aligned across time and frequency. These visualizations are not cosmetic: they serve as cognitive tools, allowing researchers, composers, and users to intuitively see the dynamics of harmonic structure.

\subsection*{6.1 Layer-Based Stack Design}

SRC$^{9}$ visualization output is composed of vertically stacked unit layers, each aligned to a shared horizontal timeline (0–20s). Each layer occupies 216px vertical space, with the RawSpectrum occupying 1080px as base.

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer} & \textbf{Visual Form} & \textbf{Height (px)} \\
\hline
RawSpectrum (S$^3$) & colored partial markers & 1080 \\
PRU & red bars with pitch labels & 216 \\
RPU & $\Phi$ line + window overlays & 216 \\
RFMU & resonance field heatmap & 216 \\
CRVU & RGB stacked bars & 216 \\
\hline
Total & 2160 px (4K) & \\
\hline
\end{tabular}
\end{center}

\subsection*{6.2 Visual Encoding Principles}

\paragraph{Frequency:} Y-axis (log scale)  
\paragraph{Amplitude:} Marker size, object scale, emission intensity  
\paragraph{Time:} X-axis (0–20s, 0.1s resolution)  
\paragraph{Color:}  
\begin{itemize}
  \item Frequency class (e.g., pitch class palette)
  \item Functional role (e.g., PR = red, $\Phi$ = gray, field = inferno colormap)
  \item CRV: red = TPS, green = TFI, blue = NSF
\end{itemize}

\paragraph{Symbolic labels:} Microtonal notations (e.g., G2⁺¹) appear on PR bars and RawSpectrum points.

\subsection*{6.3 Plotting Tools}

\begin{itemize}
    \item \textbf{Plotly (Python):} for interactive HTML visualizations, hoverable markers, frame-aligned curves
    \item \textbf{Matplotlib:} for static PNG exports, segment overlays, symbol-annotated graphs
    \item \textbf{Unity (C\#):} for 3D mesh-based field rendering and partial animation
\end{itemize}

\subsection*{6.4 Master Overlay Generation}

The file \texttt{visualize_overlay_all.py} combines all unit visual outputs into a single 3840×2160 image or HTML frame.

\textbf{Features:}

\begin{itemize}
    \item Frame-synchronized overlays
    \item Independent vertical scaling per layer
    \item Interactive time cursor
    \item Toggleable layers
\end{itemize}

\subsection*{6.5 Cognitive Color Mapping}

CRVU’s cognitive outputs (TPS, TFI, NSF) are displayed as stacked colored bars:

\begin{itemize}
    \item \textcolor{red}{Red:} TPS — stability
    \item \textcolor{green}{Green:} TFI — fusion
    \item \textcolor{blue}{Blue:} NSF — memory encoding
\end{itemize}

The bar height corresponds to value magnitude $[0,1]$.

\subsection*{6.6 RFM Surface Rendering in Unity}

RFMU’s scalar field data are converted into 3D terrain meshes:

\begin{itemize}
    \item X = time
    \item Z = frequency (log scale)
    \item Y = field strength $\rightarrow$ terrain height
    \item Emission map = normalized $\Phi$
    \item Overlay: peak paths, PR lines, curvature ridges
\end{itemize}

Implemented using Unity’s \texttt{MeshFilter}, \texttt{MaterialPropertyBlock}, and shader-based vertex displacement.

\subsection*{6.7 Animation and Playback Features}

\begin{itemize}
    \item Timeline scrubbing (linked to frame index)
    \item Real-time playback at 10 FPS (0.1s/frame)
    \item Sound-reactive visuals (optional)
    \item Dynamic camera tracking (e.g., PR curve follower)
\end{itemize}

\subsection*{6.8 Use Cases}

\begin{itemize}
    \item \textbf{Education:} visually teach harmonic fields, voice leading, polyphony
    \item \textbf{Analysis:} detect modulation, PR shift, dissonance zones
    \item \textbf{Performance:} display resonance terrain live in VR
    \item \textbf{Composition:} use RFM as a topographic canvas for generative tools
\end{itemize}

\section*{Chapter VII — Intermodular Feedback and Adaptive Control}

A defining feature of SRC$^{9}$ is its recursive structure: each module not only feeds into the next but also receives feedback from downstream layers. This transforms the system from a static analyzer into a dynamic resonance engine — capable of adaptive learning, reweighting, and perceptually informed transformation.

\subsection*{7.1 Loop Architecture}

The primary communication loop of SRC$^{9}$ is:

\[
\text{S$^3$} \rightarrow \text{R$^3$} \rightarrow \text{C$^3$} \rightarrow \text{Feedback to R$^3$ or S$^3$}
\]

\paragraph{Forward Path:}
\begin{itemize}
    \item S$^3$ $\rightarrow$ R$^3$: partial frames $\rightarrow$ harmonic reasoning
    \item R$^3$ $\rightarrow$ C$^3$: CRV vector + $\Phi$ + RFM data
\end{itemize}

\paragraph{Feedback Path:}
\begin{itemize}
    \item C$^3$ $\rightarrow$ R$^3$: attention, immersion, memory modulation
    \item R$^3$ $\rightarrow$ S$^3$: spectral filtering, dynamic rescaling, visualization tuning
\end{itemize}

\subsection*{7.2 C$^3$ $\rightarrow$ R$^3$ Feedback Mechanisms}

\paragraph{Affective Salience (AOU, PIU):}
High immersion scores increase weight on core partials in RFM generation:

\[
A_i^{*} = A_i \cdot (1 + \lambda_{\text{PIU}})
\]

\paragraph{Tension Focus (CTU):}
RPU’s $\Phi$ calculation uses tension-weighted denominators:

\[
\Phi'(t) = \sum_{i<j} \frac{A_i A_j}{|f_i - f_j| + \epsilon} \cdot \omega_{\text{CTU}}
\]

\paragraph{Memory Anchoring (SAU):}
SAU can extend windowed Φ integration across prior phrases to model phrase re-entry or long-term attractor stabilization.

\subsection*{7.3 C$^3$ $\rightarrow$ S$^3$ Modulation}

\begin{itemize}
    \item \textbf{Spectral Masking:} Hide partials with low salience or low CRV
    \item \textbf{Symbol Injection:} Annotate or override f₀ labels with C$^3$-informed symbolic tags
    \item \textbf{Display Scaling:} Increase opacity/size of key partials if memory/affect signal is high
\end{itemize}

\subsection*{7.4 Modulation Example (Narrative Music)}

Assume a phrase begins with stable CRV:

\[
\vec{\text{CRV}}_1 = [0.91, 0.88, 0.82]
\]

The system increases visual brightness of corresponding partials and amplifies RFM terrain peaks.

A modulation or PR shift occurs:

\[
\vec{\text{CRV}}_2 = [0.41, 0.33, 0.17]
\]

This results in:
\begin{itemize}
    \item Sharpened $\nabla$RFM contours
    \item Increase in partial flicker effect in Unity
    \item CRV bars collapse → signaling cognitive destabilization
\end{itemize}

\subsection*{7.5 Feedback API Specification}

\paragraph{Feedback Packet (JSON):}

\begin{verbatim}
{
  "time": 4.3,
  "feedback": {
    "CTU": 0.87,
    "PIU": 0.76,
    "NSU": 0.41
  }
}
\end{verbatim}

Received by:

\begin{itemize}
    \item RFM filter generator
    \item Partial weighting engine
    \item Visual modulation manager
\end{itemize}

\subsection*{7.6 Live Feedback and Loop Safety}

\begin{itemize}
    \item Feedback modulation is clamped between $\pm 25\%$ per frame
    \item Frame history buffers used to prevent oscillation artifacts
    \item Async-safe handlers allow interruption or override at runtime
\end{itemize}

\subsection*{7.7 Toward Resonance-Centric Interactivity}

The feedback loop is not just for refinement. It enables new applications:

\begin{itemize}
    \item Interactive composition: CRV $\rightarrow$ generative seed adjustment
    \item Brain–music co-evolution: EEG $\rightarrow$ C$^3$ $\rightarrow$ R$^3$ reshaping
    \item Self-regulating installations: perception $\rightarrow$ structure $\rightarrow$ perception
\end{itemize}

\section*{Chapter VIII — Scientific Contribution and Comparative Positioning}

SRC$^{9}$ is more than a computational toolkit — it is a conceptual shift in how music is understood, analyzed, and linked to perception. This chapter contextualizes SRC$^{9}$ within existing scientific disciplines and explains its novel contribution to music theory, auditory neuroscience, cognitive modeling, and AI.

\subsection*{8.1 Bridging Fragmented Disciplines}

\begin{itemize}
    \item \textbf{Traditional Music Theory:} Offers symbolic, style-specific models (e.g., Roman numerals, keys) that lack generalizability to non-Western, microtonal, or electronically produced music.
    \item \textbf{Auditory Neuroscience:} Describes neural encoding of sound, but lacks structural models of music capable of predicting EEG/fMRI response.
    \item \textbf{AI/ML Music Systems:} Generate convincing audio, but are black-box and devoid of interpretability or symbolic grounding.
\end{itemize}

\textbf{SRC$^{9}$ bridges these silos} by combining spectral analysis, resonance modeling, and cognitive simulation into a unified framework.

\subsection*{8.2 Novel Contributions by Module}

\paragraph{S$^3$ — Spectral Extraction}
\begin{itemize}
    \item Sub-cent partial tracking with harmonic identification
    \item Microtonal symbolic pitch encoding ($\pm$25c steps)
    \item 4K-resolution time–frequency mapping
\end{itemize}

\paragraph{R$^3$ — Resonance Modeling}
\begin{itemize}
    \item Real-valued $\Phi$ coherence computation
    \item RFM field topography and attractor mapping
    \item Phantom root detection without symbolic grammar
    \item Resonance-based perception modeling (TPS, TFI, NSF)
\end{itemize}

\paragraph{C$^3$ — Cognitive Circuitry}
\begin{itemize}
    \item Unit–Node–Element hierarchy with EEG/fMRI anchors
    \item Integration with real-time neural interfaces (e.g., OpenBCI, Emotiv)
    \item Emotional salience, memory encoding, and inter-brain synchrony modeling
\end{itemize}

\subsection*{8.3 Epistemological Reversal}

Traditionally:

\[
\text{Notation} \rightarrow \text{Structure} \rightarrow \text{Sound}
\]

SRC$^{9}$ reverses this:

\[
\text{Sound} \rightarrow \text{Structure} \rightarrow \text{Perception}
\]

This change recognizes that human listeners don’t hear scores — they hear waveforms, from which meaning emerges through spectral convergence, not grammatical rule sets.

\subsection*{8.4 Relationship to Existing Models}

\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Model} & \textbf{Comparison to SRC$^9$} \\
\hline
Lerdahl/Jackendoff GTTM & Symbolic-only, lacks spectral realism \\
Huron’s ITPRA & Predictive cognition, no spectral grounding \\
Bregman’s Auditory Scene Analysis & Compatible perceptually, no structural formalism \\
Schenkerian Analysis & Hierarchical tonality, score-dependent \\
Tonnetz/Neo-Riemannian Theory & Static topology, lacks time/frequency axes \\
MusicLM / Magenta & Non-interpretable deep generative models \\
\hline
\end{tabular}
\end{center}

\subsection*{8.5 Empirical Grounding}

\begin{itemize}
    \item $\Phi$ aligns with neural synchrony and EEG FFR data (Bidelman 2011)
    \item CRV mirrors attention and memory indices in fMRI studies (Zatorre et al. 2013)
    \item Microtonal segmentation reflects known auditory thresholds (Moore 2012)
    \item Temporal frame length matches auditory ERP resolution (MMN, P300)
\end{itemize}

\subsection*{8.6 Scientific Use Cases}

\begin{itemize}
    \item Neurocognitive analysis of music listening
    \item Dynamic modeling of musical form without score
    \item Empirical testing of tension, memory, or absorption in real time
    \item Cross-cultural harmonic modeling (e.g., gamelan, maqam, drone music)
    \item Tonotopic map visualization from real audio
\end{itemize}

\subsection*{8.7 Artistic Use Cases}

\begin{itemize}
    \item AI composition using CRV trajectories
    \item VR installations guided by RFM terrain
    \item Generative systems with real-time PR feedback
    \item Improvisation interfaces using $\Phi$ heatmaps and modulation vectors
\end{itemize}

\subsection*{8.8 Educational Use Cases}

\begin{itemize}
    \item Teaching spectral vs. symbolic harmony
    \item Visualizing modulation, drift, tension, and resolution
    \item Exploring affective resonance in sound
    \item Multisensory music learning through waveform → field → cognition
\end{itemize}

\subsection*{8.9 Future Research Integration}

SRC$^{9}$ can integrate with:

\begin{itemize}
    \item EEG systems (OpenBCI, Emotiv) — real-time feedback to CRVU
    \item Machine learning — CRV as feature vector for emotion or form prediction
    \item Neuroscientific experiments — auditory-cognitive mapping under stimuli
    \item Notational systems — hybrid symbolic–spectral scores
\end{itemize}

\section*{Chapter IX — Implementation Architecture and Development Overview}

While SRC$^{9}$ is rooted in scientific theory and cognitive models, it is also a practical software system: a set of coordinated Python, JSON, CSV, Unity, and WebGL components that form a modular, executable pipeline.

This chapter describes the engineering architecture of SRC$^{9}$ — the file structures, runtime logic, APIs, and execution modes that bring its resonance engine to life.

\subsection*{9.1 System Overview}

SRC$^{9}$ is composed of three primary code layers:

\begin{itemize}
    \item \textbf{Core Analysis Layer:} Python modules for S$^3$, R$^3$, and C$^3$ computations
    \item \textbf{Visualization Layer:} Plotly, Matplotlib, and WebGL-based rendering scripts
    \item \textbf{Interaction Layer:} Unity scene controllers, OSC interfaces, and data streaming tools
\end{itemize}

\subsection*{9.2 Folder and File Structure}

\begin{verbatim}
/src/
  /s3/
    extract_frequencies_crepe.py
    harmonics_matching.py
  /r3/
    PR_unit_temporal.py
    RP_unit_combined.py
    RFM_unit_analysis.py
    CRV_unit_analysis.py
  /c3/
    CTU_compute.py
    AOU_compute.py
    ...
  /visualize/
    visualize_PR_temporal.py
    visualize_RFM_unit.py
    visualize_overlay_all.py
  run_SRC9_pipeline.py
/data/
  /raw/
    RawSpectrum-unit.json
  /output/
    /PR/
    /RP/
    /RFM/
    /CRV/
/output/
  R3-overlay.html
  *.png
/unity/
  CSVLoader.cs
  SpectrumVisualizer.cs
  CRVOverlayHUD.cs
\end{verbatim}

\subsection*{9.3 Execution Pipeline}

Execution can occur:

\begin{itemize}
    \item \textbf{Sequentially} — via \texttt{run_SRC9_pipeline.py}
    \item \textbf{Manually} — unit-by-unit for debugging
    \item \textbf{Real-time} — via live frame ingestion, under development
\end{itemize}

\paragraph{Standard Pipeline Order:}

\begin{enumerate}
    \item S$^3$: CREPE $\rightarrow$ base spectrum
    \item R$^3$: PRU, RPU, RFMU, CRVU
    \item C$^3$: All 9 Units $\rightarrow$ summary JSON
    \item Visualizer: Generate overlays and interactive outputs
    \item Unity export: CSV + animation parameters
\end{enumerate}

\subsection*{9.4 API Design Philosophy}

\textbf{Input:} always JSON

\textbf{Output:} JSON + PNG + HTML (Plotly) + CSV (Unity)

Each function or script is:

\begin{itemize}
    \item stateless (idempotent)
    \item reusable (called by other pipelines)
    \item visually testable (through plot outputs)
\end{itemize}

\subsection*{9.5 Modularity Map}

\begin{center}
\includegraphics[width=0.75\textwidth]{modular_block_diagram.pdf}
\end{center}

Modules are black-box compatible — meaning any layer (e.g., RFMU) can be replaced or extended with an alternate implementation without breaking upstream/downstream logic.

\subsection*{9.6 Runtime Profiling (Batch Mode)}

On a standard system (Intel i7, 16 GB RAM):

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Unit} & \textbf{Analysis Time (200 frames)} & \textbf{Visualization Time} \\
\hline
PRU & 2.1 s & 1.4 s \\
RPU & 4.0 s & 2.2 s \\
RFMU & 5.2 s & 3.1 s \\
CRVU & 1.0 s & 1.0 s \\
C$^3$ full unit set & $\sim$9 s & — \\
Overlay (HTML) & — & 3–5 s \\
\hline
\end{tabular}
\end{center}

\textbf{Total batch time:} ~20–25 seconds per 20s audio input.

\subsection*{9.7 Unity Engine Integration}

\textbf{File Format:} CSV  
\textbf{Core Classes:}
\begin{itemize}
    \item \texttt{Partial.cs} — object representation
    \item \texttt{CSVLoader.cs} — parser and loader
    \item \texttt{SpectrumVisualizer.cs} — prefab instantiation
    \item \texttt{CRVOverlayHUD.cs} — affective bar display
\end{itemize}

\textbf{Render Modes:}
\begin{itemize}
    \item glowing spheres for partials
    \item PR curves via \texttt{LineRenderer}
    \item terrain mesh for RFM via \texttt{MeshFilter}
\end{itemize}

\subsection*{9.8 Platform Compatibility}

\begin{itemize}
    \item \textbf{Python:} 3.9+
    \item \textbf{Unity:} 2021.3 LTS
    \item \textbf{Jupyter:} for experiment notebooks
    \item \textbf{Web:} HTML + Plotly/Three.js (experimental)
    \item \textbf{VR/OSC:} WebSocket-ready
\end{itemize}

\subsection*{9.9 Distribution and Open Source}

SRC$^{9}$ is open-source under the MIT license. Code, data samples, visual exports, and Unity demos are available at:

\texttt{\url{https://github.com/src9-framework/src9}}

Contributors are invited to fork units, extend field models, or contribute to future modules such as OL (Overtone Locking) or GMI (Global Musical Inference).

\section*{Chapter X — Final Reflections and Theoretical Outlook}

SRC$^{9}$ is not merely a framework, a pipeline, or a set of scripts. It is a paradigm: a new way of conceptualizing, measuring, and interacting with musical structure. It brings together physics, perception, cognition, and computation into a unified temporal field theory of music.

\subsection*{10.1 A New Definition of Harmony}

Harmony is traditionally defined symbolically — as triads, scales, or functional progressions. SRC$^{9}$ proposes a redefinition:

\textbf{Harmony is a time-varying field of structured resonance, shaped by energy, weighted by perception, and embedded in cognition.}

Instead of working in discrete steps (e.g., I–IV–V), SRC$^{9}$ defines harmony as an evolving topology:

\begin{itemize}
    \item \textbf{Attractors:} Phantom roots, perceptual centers
    \item \textbf{Gradients:} Tonal pull, dissonance slope
    \item \textbf{Fusion zones:} $\Phi$ coherence regions
    \item \textbf{Modulation:} Topographic drift in RFM space
\end{itemize}

\subsection*{10.2 Cognitive Resonance as Musical Logic}

Through C$^{3}$, harmony becomes measurable not only in acoustics, but in brain-space:

\begin{itemize}
    \item TPS $\rightarrow$ perceived stability
    \item TFI $\rightarrow$ spectral coherence
    \item NSF $\rightarrow$ memory encoding strength
\end{itemize}

This transforms music analysis into neuroperceptual logic — one grounded in actual listener behavior, emotion, and timing.

\subsection*{10.3 Generalization Across Styles and Cultures}

Because SRC$^{9}$ is not bound to Western notation, keys, or 12-TET tuning, it generalizes to:

\begin{itemize}
    \item Drone-based music (e.g., Indian raga, Tibetan chant)
    \item Just intonation and spectralism
    \item Electronic soundscapes and ambient textures
    \item Improvised music, microtonal works, non-metered environments
\end{itemize}

Its mathematical core — $\Phi$, RFM, CRV — is culturally neutral but perceptually rich.

\subsection*{10.4 Toward a New Science of Sound}

SRC$^{9}$ invites a reimagination of music cognition as a form of field-based reasoning:

\[
\text{Cognition is not symbolic parsing. It is real-time entrainment to dynamic energy structures.}
\]

This claim opens doors to:

\begin{itemize}
    \item New theories of musical time and memory
    \item Biofeedback systems that respond to sonic states
    \item Emotion-aware generative music engines
    \item Aesthetic theories rooted in resonance, not style
\end{itemize}

\subsection*{10.5 Final Statement}

SRC$^{9}$ is designed to evolve.

It is not only a tool for analyzing the music of the past — it is a language for the music of the future. A music that is:

\begin{itemize}
    \item Spectrally informed
    \item Resonantly grounded
    \item Cognitively engaged
    \item Mathematically coherent
    \item Visually immersive
\end{itemize}

The resonance field is the new score.

The CRV vector is the new expressive arc.

The mind–ear interface is the new stage.

\textbf{This is the architecture of SRC$^{9}$.}

\section*{Chapter X — Final Reflections and Theoretical Outlook}

SRC$^{9}$ is not merely a framework, a pipeline, or a set of scripts. It is a paradigm: a new way of conceptualizing, measuring, and interacting with musical structure. It brings together physics, perception, cognition, and computation into a unified temporal field theory of music.

\subsection*{10.1 A New Definition of Harmony}

Harmony is traditionally defined symbolically — as triads, scales, or functional progressions. SRC$^{9}$ proposes a redefinition:

\textbf{Harmony is a time-varying field of structured resonance, shaped by energy, weighted by perception, and embedded in cognition.}

Instead of working in discrete steps (e.g., I–IV–V), SRC$^{9}$ defines harmony as an evolving topology:

\begin{itemize}
    \item \textbf{Attractors:} Phantom roots, perceptual centers
    \item \textbf{Gradients:} Tonal pull, dissonance slope
    \item \textbf{Fusion zones:} $\Phi$ coherence regions
    \item \textbf{Modulation:} Topographic drift in RFM space
\end{itemize}

\subsection*{10.2 Cognitive Resonance as Musical Logic}

Through C$^{3}$, harmony becomes measurable not only in acoustics, but in brain-space:

\begin{itemize}
    \item TPS $\rightarrow$ perceived stability
    \item TFI $\rightarrow$ spectral coherence
    \item NSF $\rightarrow$ memory encoding strength
\end{itemize}

This transforms music analysis into neuroperceptual logic — one grounded in actual listener behavior, emotion, and timing.

\subsection*{10.3 Generalization Across Styles and Cultures}

Because SRC$^{9}$ is not bound to Western notation, keys, or 12-TET tuning, it generalizes to:

\begin{itemize}
    \item Drone-based music (e.g., Indian raga, Tibetan chant)
    \item Just intonation and spectralism
    \item Electronic soundscapes and ambient textures
    \item Improvised music, microtonal works, non-metered environments
\end{itemize}

Its mathematical core — $\Phi$, RFM, CRV — is culturally neutral but perceptually rich.

\subsection*{10.4 Toward a New Science of Sound}

SRC$^{9}$ invites a reimagination of music cognition as a form of field-based reasoning:

\[
\text{Cognition is not symbolic parsing. It is real-time entrainment to dynamic energy structures.}
\]

This claim opens doors to:

\begin{itemize}
    \item New theories of musical time and memory
    \item Biofeedback systems that respond to sonic states
    \item Emotion-aware generative music engines
    \item Aesthetic theories rooted in resonance, not style
\end{itemize}

\subsection*{10.5 Final Statement}

SRC$^{9}$ is designed to evolve.

It is not only a tool for analyzing the music of the past — it is a language for the music of the future. A music that is:

\begin{itemize}
    \item Spectrally informed
    \item Resonantly grounded
    \item Cognitively engaged
    \item Mathematically coherent
    \item Visually immersive
\end{itemize}

The resonance field is the new score.

The CRV vector is the new expressive arc.

The mind–ear interface is the new stage.

\textbf{This is the architecture of SRC$^{9}$.}

\title{S3_MasterTechnicalReport_}
\author{Amac Erdem}
\date{May 2025}
\section*{S³ Module – Master Technical Report (Enhanced)}

\subsection*{Spectral Sound Space (S³) Master Report Roadmap}

\subsection*{I. Purpose and Position of the Report}

\paragraph{Core Objective:}  
The S³ Module is the spectral analysis and fundamental sound modeling layer of the SRC⁹ system. This report documents all theoretical, algorithmic, mathematical, engineering, and aesthetic structures of the module on both scientific and applied levels.

\paragraph{Role in the System:}

\begin{itemize}
    \item Within SRC⁹, S³ forms the first layer of the S³–R³–C³ system.
    \item This report establishes the data foundation for R³ and C³ integration.
\end{itemize}

\subsection*{II. Main Report Sections}

\begin{enumerate}
    \item Introduction and General Overview
    \item Theoretical Foundations
    \item Mathematical and Algorithmic Model
    \item Signal Processing Pipeline Architecture
    \item Data Structures and Formats
    \item Visualization Layers
    \item R³ and C³ Integration
    \item Optimization and Performance
    \item References and Sources
    \item Appendices and Code Examples
\end{enumerate}

\subsection*{III. Detailed Roadmap for Each Section}

\paragraph{1. Introduction and General Overview}
\begin{itemize}
    \item General structure of the SRC⁹ system
    \item Role of S³ in the system
    \item Scientific and aesthetic goals
    \item Development history
    \item Use cases: music analysis, composition, AI, cognitive science
\end{itemize}

\paragraph{2. Theoretical Foundations}

\subparagraph{2.1 Spectral Music Theory}
\begin{itemize}
    \item Summary of theoretical frameworks from Schaeffer, Grisey, Murail
    \item Frequency-based harmony
    \item Role of harmonic series in music
\end{itemize}

\subparagraph{2.2 Psychoacoustic Foundations}
\begin{itemize}
    \item Critical bands
    \item Harmonic fusion
    \item Perceptual resonance
\end{itemize}

\subparagraph{2.3 Musical Time and Frequency as a Unified Domain}
\begin{itemize}
    \item Time–frequency duality
    \item Limitations of Fourier theory
\end{itemize}

\paragraph{3. Mathematical and Algorithmic Model}

\subparagraph{3.1 Frequency Space and Microtonality}
\begin{itemize}
    \item Frequency distributions beyond 12TET
    \item Cent and pitch class calculations
\end{itemize}

\subparagraph{3.2 Harmonic Structures}
\begin{itemize}
    \item Concept of partials
    \item Harmonic series tolerance (±50 cents)
    \item Partial grouping algorithms
\end{itemize}

\subparagraph{3.3 Fundamental Frequency Estimation (f₀)}
\begin{itemize}
    \item Use of CREPE
    \item Comparison with pYIN
    \item Viterbi optimization
\end{itemize}

\subparagraph{3.4 JSON Data Modeling}

\paragraph{4. Signal Processing Pipeline Architecture}

\subparagraph{4.1 File Structure and Module Organization}
\begin{itemize}
    \item \texttt{audio/}, \texttt{json/}, \texttt{scripts/}, \texttt{output/}, \texttt{utils/}
\end{itemize}

\subparagraph{4.2 Main Scripts}
\begin{itemize}
    \item \texttt{extract\_frequencies\_crepe.py}
    \item \texttt{harmonics\_matching.py}
    \item \texttt{s3\_visualization.py}
\end{itemize}

\subparagraph{4.3 Step-by-Step Workflow}
\begin{itemize}
    \item Audio → f₀ + dB → harmonic matching → color coding → visualization
\end{itemize}

\subparagraph{4.4 Execution and Automation}
\begin{itemize}
    \item \texttt{start\_pipeline.sh} structure
    \item Command examples
\end{itemize}

\paragraph{5. Data Structures and Formats}

\subparagraph{5.1 \texttt{partials.json}}
\begin{itemize}
    \item Frame structure
    \item Partial fields: freq, dB, isFundamental
\end{itemize}

\subparagraph{5.2 \texttt{harmonics.json} (optional)}
\begin{itemize}
    \item Extended data post harmonic matching
\end{itemize}

\subparagraph{5.3 \texttt{resonances.json}}  
\begin{itemize}
    \item Prepared output for R³
\end{itemize}

\subparagraph{5.4 \texttt{color\_map.json}}  
\begin{itemize}
    \item Optional: record of spectral color mapping
\end{itemize}

\paragraph{6. Visualization Layers}

\subparagraph{6.1 2D Map}
\begin{itemize}
    \item Time–Frequency map (3840×2160)
    \item Microtonal grid
    \item Donut Spectrum color transitions
\end{itemize}

\subparagraph{6.2 3D Map}
\begin{itemize}
    \item Time–Frequency–Amplitude (x, y, z)
    \item Unity and Plotly usage
    \item Mesh and dot modes
\end{itemize}

\subparagraph{6.3 Unity Integration}
\begin{itemize}
    \item CSV export format
    \item Prefab system
    \item Real-time camera and lighting setup
\end{itemize}

\paragraph{7. R³ and C³ Integration}

\subparagraph{7.1 Output for R³}
\begin{itemize}
    \item JSON → Resonance Potential (Φ), Harmonic Distance (HD), Phantom Root (PR)
\end{itemize}

\subparagraph{7.2 Preparation for C³}
\begin{itemize}
    \item Microtonal notation + amplitude → cognitive resonance estimation
\end{itemize}

\subparagraph{7.3 Feedback Loops}
\begin{itemize}
    \item R³ → S³ color changes
    \item C³ → S³ attention level simulation
\end{itemize}

\paragraph{8. Optimization and Performance}

\subparagraph{8.1 Pipeline Performance}
\begin{itemize}
    \item JSON file size
    \item CREPE runtime
    \item Viterbi duration
\end{itemize}

\subparagraph{8.2 Visualization Performance}
\begin{itemize}
    \item 4K render times
    \item Number of prefabs in Unity
\end{itemize}

\subparagraph{8.3 GPU Acceleration (Future)}

\paragraph{9. References and Sources}
\begin{itemize}
    \item Grisey, Murail, Schaeffer
    \item Lerdahl, Tymoczko, Sethares
    \item Cognitive modeling: Zatorre, Patel, Koelsch
    \item CREPE (Kim et al., 2018)
    \item Librosa and pYIN documentation
    \item Plotly, Unity technical docs
\end{itemize}

\paragraph{10. Appendices and Code Examples}
\begin{itemize}
    \item Code blocks and annotations
    \item JSON examples
    \item Color spectrum image (C → B transition)
    \item Unity scene settings (camera, light, prefab connections)
    \item Screenshots and final outputs
\end{itemize}

\section*{I.1 – Introduction and Overview}

\subsection*{Spectral Sound Space (S³) in the Architecture of SRC⁹}

The Spectral Sound Space (S³) module constitutes the foundational analytic layer of the SRC⁹ framework—an interdisciplinary system that unites spectral acoustics, resonance modeling, and cognitive neuroscience. Positioned as the first pillar in the tripartite structure of SRC⁹ (S³–R³–C³), S³ provides the fundamental data structures and perceptual primitives from which harmonic reasoning (R³) and neural interaction modeling (C³) emerge.

Whereas traditional music analysis systems typically begin from symbolic notation (e.g., MIDI, scores), S³ reverses the process: it operates directly on audio waveforms to extract a detailed, high-resolution representation of acoustic content in both time and frequency. This bottom-up approach ensures that the analytic foundation is directly grounded in the physical properties of sound, enabling it to generalize across styles, cultures, tuning systems, and performance modalities.

\subsection*{Motivations for S³: Why a Spectral Module?}

The design of S³ is motivated by three fundamental observations:

\begin{itemize}
    \item \textbf{Sound is inherently spectral.}\\
    Any auditory event can be decomposed into partials—individual frequency components that change over time in pitch, amplitude, and phase. These partials are the building blocks of tone perception and harmonic structure.

    \item \textbf{Spectral representations precede symbolic ones.}\\
    The human auditory system does not “hear notes,” but rather detects frequency patterns and intensity envelopes. Notation is a cultural abstraction layered atop an auditory substrate. Therefore, analysis should start at the spectral level if it aims to reflect perceptual and cognitive realities.

    \item \textbf{Harmony is a physical phenomenon before it is a theoretical one.}\\
    The perception of consonance, root, resonance, and tonality emerges from interactions between partials, not from theoretical scales. S³ enables the measurement and visualization of these interactions in their raw, physical form.
\end{itemize}

\subsection*{The Role of S³ in the Full System}

In the architecture of SRC⁹, S³ performs three critical roles:

\subsubsection*{Data Generator}

S³ converts raw audio into structured data representations including:

\begin{itemize}
    \item Fundamental frequencies (f₀) over time
    \item Partial tracks (harmonics and inharmonic components)
    \item Amplitude (in dB) of each component
    \item Microtonal symbolic notations and pitch-class mappings
    \item Spectral centroids, energy envelopes, and entropy values
\end{itemize}

\subsubsection*{Perceptual Filter}

It selectively isolates acoustically meaningful content from noise, silences, and irrelevant transients using amplitude thresholds, frame-based smoothing, and overtone filters.

\subsubsection*{Visual Engine}

S³ produces high-resolution 2D and 3D visualizations of the spectral data:

\begin{itemize}
    \item 2D time–frequency maps at 3840×2160 resolution
    \item 3D spectrograms with frequency–amplitude towers
    \item Interactive spectral canvases for analysis and composition
\end{itemize}

These outputs form the primary input for the R³ module (Resonance-Based Relational Reasoning), where spectral data is analyzed for harmonic structure, resonance potential, overtone locking, and phantom root phenomena.

\subsection*{Interdisciplinary Relevance}

The S³ module draws simultaneously from:

\begin{itemize}
    \item \textbf{Spectral Music Theory (e.g., Grisey, Murail, Schaeffer):}\\
    Emphasizing sound itself as the basis of musical form, S³ adopts this premise and extends it computationally.

    \item \textbf{Signal Processing and Machine Learning:}\\
    Tools such as CREPE (deep learning pitch estimator), librosa (audio analysis library), and FFT algorithms are integrated to allow frame-level pitch and amplitude extraction with sub-millisecond precision.

    \item \textbf{Cognitive Neuroscience and Psychoacoustics:}\\
    Through microtonal accuracy and overtone modeling, S³ mirrors how the auditory cortex processes complex sound structures.

    \item \textbf{Music Technology and Visualization:}\\
    S³ interfaces directly with Unity, WebGL, and VR environments, producing real-time interactive spectral landscapes for education, analysis, and artistic use.
\end{itemize}

\subsection*{Conclusion: A Foundational Layer}

The Spectral Sound Space (S³) module serves as the foundation of the SRC⁹ system. It provides the raw material—both data and perceptual structure—from which musical reasoning and cognitive mapping can emerge. Unlike traditional systems that operate on abstracted notation or symbolic input, S³ roots its analysis in the physical substance of sound.

Its capacity to extract, quantify, and visualize meaningful partials across styles and tuning systems makes it a uniquely versatile tool. Whether applied to Renaissance counterpoint, spectral composition, or neural music analysis, S³ enables a new form of bottom-up, physics-based music understanding.

\section*{I.2 – Theoretical Foundations}

\subsection*{I.2.1 Sound Before Symbol: Epistemology of Sonic Analysis}

The S³ module is predicated on a fundamental epistemological shift: that music analysis should begin not with the score, but with the sound itself. Western music theory has historically prioritized symbolic abstraction (notation, keys, chords), often detaching the study of music from the phenomena it arises from—vibrations in air, shaped by instruments, perceived by human bodies.

This module challenges that precedence.

Rather than assuming that sound serves as a mere carrier for symbolic content, S³ posits the inverse: symbolic constructs are interpretations of an underlying spectral substrate. Frequencies, amplitudes, and overtones are not peripheral—they are the music.

This shift aligns with the work of spectral composers (e.g., Gérard Grisey, Tristan Murail), auditory cognition researchers (e.g., Diana Deutsch, Albert Bregman), and philosophers of music (e.g., Pierre Schaeffer). S³ bridges their insights with modern computation.

\subsection*{I.2.2 Spectralism and Acoustical Foundations}

The theoretical roots of S³ are deeply informed by spectralism: a movement in contemporary composition that foregrounds the timbral and acoustical properties of sound over traditional harmonic systems.

\paragraph{Key spectralist premises:}

\begin{itemize}
    \item Sound is a complex spectrum of partials, not a fixed pitch.
    \item Harmony arises from the overtone series, not from abstract interval systems.
    \item The orchestration of spectra defines form and tension more than functional harmony.
\end{itemize}

S³ translates these ideas into data structures:

\begin{itemize}
    \item Every partial is tracked as an individual frequency–amplitude event.
    \item No assumption of equal temperament is made—frequencies are real-valued, microtonal.
    \item Harmonic relationships are computed, not assumed.
\end{itemize}

Through its modular design, S³ formalizes the intuition of the spectralists into a machine-readable format.

\subsection*{I.2.3 Psychoacoustics: Human Hearing and Spectral Perception}

S³ is not merely mathematically accurate; it is psychoacoustically meaningful. Its architecture reflects how human auditory perception operates:

\begin{itemize}
    \item \textbf{Critical Band Theory:} S³ models perceptual overlap via overtone locking mechanisms.
    \item \textbf{Auditory Scene Analysis:} Each partial is assigned an identity, allowing for grouping and source separation.
    \item \textbf{Temporal Integration:} Partial tracks are evaluated over time, reflecting how we perceive tone continuity.
\end{itemize}

These principles are embedded in the frame-based, high-resolution analysis pipeline. The system's sensitivity to cent-level frequency shifts, amplitude decay curves, and overtone fusion makes it not just accurate but also cognitively plausible.

\subsection*{I.2.4 Microtonality and Continuous Pitch Space}

Unlike traditional pitch class systems which operate on discrete steps (12TET), S³ operates in continuous pitch space. Every frequency is stored as a floating-point value, and pitch class labeling is optional, reversible, and tolerant to cent deviations.

\begin{itemize}
    \item Pitch is mapped not via quantization, but through continuous cent distance metrics.
    \item Microtonal variations (±5 to ±25 cents) are preserved and visualized explicitly.
    \item Symbolic mappings (e.g., C4⁺¹) are generated only for readability—not as assumptions.
\end{itemize}

This enables the analysis of music outside the bounds of Western tuning: just intonation, 24-TET, gamelan pelog/slendro, non-octave repeating scales, or even completely aleatoric sound structures.

\subsection*{I.2.5 From Spectrum to Structure: Towards Resonance}

Finally, the theoretical foundation of S³ leads naturally into R³, the Resonance-Based Relational Reasoning module.

Where S³ represents the raw materials of sound, R³ interprets those materials relationally:

\begin{itemize}
    \item How do partials converge?
    \item Which phantom roots emerge?
    \item What is the resonance potential of a harmonic field?
\end{itemize}

S³ provides the data; R³ provides the reasoning.

This progression mirrors human musical experience:

\begin{itemize}
    \item First, we hear sound (S³).
    \item Then, we infer structure and coherence (R³).
    \item Finally, we respond cognitively and emotionally (C³).
\end{itemize}

In this chain, S³ is the anchor: a physically-grounded, perceptually-aligned, computationally robust representation of what music is, before it is interpreted.

\section*{I.3 – Mathematical and Algorithmic Model}

\subsection*{I.3.1 Overview of the Computational Framework}

The S³ module translates raw acoustic signals into structured, symbolic, and interpretable representations. Its algorithmic core centers around three dimensions:

\begin{itemize}
    \item Frequency (Hz)
    \item Amplitude (dB)
    \item Time (ms resolution)
\end{itemize}

These three variables are extracted and tracked for each partial—a distinct sinusoidal component of a sound. Unlike traditional pitch-tracking systems that detect only the fundamental ($f_0$), S³ treats the full overtone field as first-class data. This makes it possible to model harmonic content, resonance, and spectral evolution with great precision.

\subsection*{I.3.2 Signal Preprocessing and Frame Segmentation}

The input signal $x(t)$ is divided into overlapping frames for analysis. Typical frame parameters are:

\begin{itemize}
    \item Sampling rate: $f_s = 16000$ Hz (CREPE optimal)
    \item Frame length: 1024 samples ($\sim$64 ms)
    \item Hop size: 160 samples ($\sim$10 ms)
\end{itemize}

Let:

\[
x_i(t) = x(t + i \cdot H), \quad \text{for frame } i
\]

where $H$ is the hop size. Frames are then passed into the frequency estimation pipeline.

\subsection*{I.3.3 Fundamental Frequency Extraction using CREPE}

The system uses CREPE (Kim et al., 2018), a deep convolutional network trained to estimate the fundamental frequency of monophonic signals with high accuracy.

Given an audio frame $x_i(t)$, CREPE returns:

\begin{itemize}
    \item $f_{0i}$: estimated fundamental frequency
    \item $c_i \in [0,1]$: confidence score
    \item optionally, $a_i$: activation maps (ignored in S³)
\end{itemize}

The result is a time series:

\[
\{ (t_i, f_{0i}, c_i) \}_{i=1}^N
\]

If $c_i < \theta$ (confidence threshold), the value is discarded or interpolated.

\subsection*{I.3.4 Amplitude Estimation and Normalization}

Each frame’s amplitude is derived via RMS energy or CREPE confidence:

\[
A_i = 20 \cdot \log_{10}(\text{RMS}(x_i)) \quad \text{(dB)}
\]

or, if using confidence:

\[
A_i = 20 \cdot \log_{10}(c_i + \epsilon)
\]

All amplitudes are normalized between $[-50 \text{ dB}, 0 \text{ dB}]$ and clipped accordingly.

\subsection*{I.3.5 Harmonic Field Construction}

S³ computes not only the fundamental $f_0$, but constructs a harmonic field:

\[
H_i = \{ n \cdot f_{0i} \mid n = 1, 2, \ldots, N_h \}
\]

where $N_h$ is the number of harmonics tracked (typically 16). Each harmonic is stored with:

\[
f_{i,n} = n \cdot f_{0i}
\]

\[
A_{i,n} = A_i - \delta(n) \quad \text{(attenuated by harmonic order)}
\]

\texttt{isFundamental} is flagged \texttt{True} if $n = 1$.

These are saved per frame in the following format:

\begin{verbatim}
{
  "time": 0.010,
  "partials": [
    {"freq": 440.0, "db": -21.0, "isFundamental": true},
    {"freq": 880.0, "db": -31.0, "isFundamental": false},
    {"freq": 1320.0, "db": -35.0, "isFundamental": false}
  ]
}
\end{verbatim}

\subsection*{I.3.6 Pitch and Cent Calculation}

For each partial, the module calculates the pitch in cents relative to A4 (440 Hz):

\[
\text{cents}(f) = 1200 \cdot \log_2 \left( \frac{f}{440} \right)
\]

This allows:

\begin{itemize}
    \item Microtonal deviation tracking (e.g., +8 cents)
    \item Conversion to symbolic pitch classes (e.g., A4⁺⁸)
\end{itemize}

This symbolic mapping is computed but not quantized; the raw frequency is retained as canonical.

\subsection*{I.3.7 Data Structures and Resolution}

S³ maintains full resolution in time (10 ms), frequency (floating-point Hz), and amplitude (floating-point dB).

\begin{itemize}
    \item \textbf{JSON storage:} arrays of frames with timestamps and partials
    \item \textbf{Data size:} $\sim$10,000 frames per 100 seconds of audio, $\sim$160,000 partials
\end{itemize}

\subsection*{I.3.8 Harmonic Matching and Partial Clustering}

An optional phase identifies recurring partials and clusters them. This supports later stages in R³ (e.g., overtone locking and phantom root detection).

Matching is done by:

\begin{itemize}
    \item Nearest-neighbor search across frames using frequency proximity
    \item Harmonic number estimation:
    \[
    n = \text{round} \left( \frac{f}{f_0} \right)
    \]
\end{itemize}

\subsection*{Summary}

The S³ module translates sound into a structured lattice of time–frequency–amplitude events. Its mathematical model ensures:

\begin{itemize}
    \item Sub-millisecond temporal resolution
    \item Cent-level pitch accuracy
    \item Frame-wise harmonic field tracking
    \item Explicit microtonal notation
    \item Compatibility with resonance modeling in R³
\end{itemize}

\section*{I.4 – Signal Processing Architecture (Pipeline Design)}

\subsection*{I.4.1 Overview}

The signal processing architecture of the S³ module is designed as a modular, multi-stage pipeline that transforms raw audio into structured, symbolically annotated, and visually renderable spectral data.

This architecture balances three key principles:

\begin{itemize}
    \item \textbf{High resolution:} Time frames in 10 ms steps, frequency in cent-level precision, amplitude in dB.
    \item \textbf{Modularity:} Each step is encapsulated as an independent script with defined input/output formats (typically JSON).
    \item \textbf{Extensibility:} The system supports integration with other modules (R³, C³), external libraries (CREPE, librosa), and interactive engines (Unity, Plotly).
\end{itemize}

\subsection*{I.4.2 Directory Structure}

The pipeline operates within a clearly defined project structure:

\begin{verbatim}
S3-Module/
├── audio/                     # Input audio files (.wav or .mp3)
│   └── cello_suite_no1.wav
├── json/                      # Intermediate and final data outputs
│   ├── base_frequencies.json
│   └── partials.json
├── output/                    # Visualization exports
│   ├── s3_visualization.png
│   └── s3_visualization.html
├── scripts/                   # Pipeline scripts
│   ├── extract_frequencies_crepe.py
│   ├── harmonics_matching.py
│   └── s3_visualization.py
├── utils/                     # Utility modules (shared functions)
│   ├── freq_to_rgb.py
│   └── freq_to_microtonal.py
└── requirements.txt
\end{verbatim}

This structure ensures reproducibility and separation of concerns across computation, data, and display.

\subsection*{I.4.3 Pipeline Stages}

\paragraph{Stage 1 – Fundamental Frequency Extraction (CREPE)}  
\textbf{Script:} \texttt{extract\_frequencies\_crepe.py}

\begin{itemize}
    \item \textbf{Input:} \texttt{.wav} or \texttt{.mp3} audio file  
    \item \textbf{Parameters:} frame size, hop size, duration (default: 10 seconds)
    \item \textbf{Process:}
    \begin{itemize}
        \item Load audio using librosa
        \item Segment into overlapping frames
        \item Pass frames to \texttt{crepe.predict()} for $f_0$ estimation
        \item Estimate amplitude via RMS or CREPE confidence
    \end{itemize}
    \item \textbf{Output (JSON):}
\end{itemize}

\begin{verbatim}
{
  "frames": [
    {
      "time": 0.010,
      "partials": [
        {"freq": 440.0, "db": -21.0, "isFundamental": true}
      ]
    },
    ...
  ]
}
\end{verbatim}

\paragraph{Stage 2 – Harmonic Field Construction}  
\textbf{Script:} \texttt{harmonics\_matching.py}

\begin{itemize}
    \item \textbf{Input:} \texttt{base\_frequencies.json} from Stage 1
    \item \textbf{Process:}
    \begin{itemize}
        \item For each frame, extract fundamental $f_0$
        \item Construct harmonics: $H_n = n \cdot f_0$ (typically for $n = 2$ to $16$)
        \item Assign decreasing amplitude per harmonic (e.g., $-10$ dB per step)
        \item Merge harmonics with original fundamental
    \end{itemize}
    \item \textbf{Output (JSON):} \texttt{partials.json} — contains full harmonic field per frame with \texttt{isFundamental} flags
\end{itemize}

\paragraph{Stage 3 – Spectral Visualization (2D/3D)}  
\textbf{Script:} \texttt{s3\_visualization.py}

\begin{itemize}
    \item \textbf{Input:} \texttt{partials.json} from Stage 2
    \item \textbf{Process:}
    \begin{itemize}
        \item Convert frequency to log-scale (Hz $\rightarrow$ cent)
        \item Normalize amplitude to dB $\rightarrow$ size and opacity
        \item Assign color using \texttt{freq\_to\_rgb()} (Donut Spectrum with 7-note color wheel)
        \item Add microtonal labels using \texttt{freq\_to\_microtonal()}
    \end{itemize}
    \item \textbf{Output:}
    \begin{itemize}
        \item PNG at 3840×2160
        \item Optional: HTML (Plotly interactive)
    \end{itemize}
\end{itemize}

\subsection*{I.4.4 Execution Commands}

\begin{verbatim}
# Step 1: Extract fundamentals
python3 scripts/extract_frequencies_crepe.py audio/Debussy.mp3 json/base_frequencies.json

# Step 2: Build harmonic field
python3 scripts/harmonics_matching.py json/base_frequencies.json json/partials.json

# Step 3: Visualize
python3 scripts/s3_visualization.py json/partials.json output/s3_visualization.png
\end{verbatim}

\textbf{Optional interactive output:}

\begin{verbatim}
python3 scripts/s3_visualization.py json/partials.json output/s3_visualization.html
\end{verbatim}

\subsection*{I.4.5 Automation: Master Script (Optional)}

You may include a master pipeline script (\texttt{start\_pipeline.sh}) that automates all stages:

\begin{verbatim}
#!/bin/bash
echo "Running S³ Pipeline..."
python3 scripts/extract_frequencies_crepe.py audio/input.wav json/base_frequencies.json
python3 scripts/harmonics_matching.py json/base_frequencies.json json/partials.json
python3 scripts/s3_visualization.py json/partials.json output/s3_visualization.png
echo "Done."
\end{verbatim}

\subsection*{I.4.6 Error Handling and Logging}

Each script includes:

\begin{itemize}
    \item Usage help when arguments are missing
    \item File existence checks for inputs
    \item JSON schema validation (planned)
    \item Logging system for progress and warnings
\end{itemize}

\subsection*{I.4.7 Modular API Design}

The functions in each script can also be exposed via an internal API for integration with:

\begin{itemize}
    \item Jupyter Notebooks (for educational/research use)
    \item Unity (real-time input/output via OSC or TCP)
    \item R³ module (resonance analysis integration)
\end{itemize}

\subsection*{Summary}

The S³ module's signal processing architecture forms a clean, high-resolution, and extensible pipeline from sound to structure. It is engineered to allow spectral data to be transformed into meaningful musical structures, ready for further analysis by R³ and C³. The use of standard tools, modular scripts, and clear data formats ensures that the system is not only scientifically robust but also developer-friendly and future-proof.

\section*{I.5 – Data Structures and Formats}

\subsection*{I.5.1 Overview}

The Spectral Sound Space (S³) module operates on a carefully designed set of data structures to ensure maximum flexibility, resolution, and interoperability. These formats serve as both internal data representations and external interfaces for downstream modules (R³, C³), visualizations, and interactive environments (e.g., Unity).

The primary data structure is JSON, chosen for its human readability, machine parsability, and web compatibility. All spectral events—frequencies, amplitudes, time steps, and symbolic annotations—are encoded in JSON using standardized field names and schema.

\subsection*{I.5.2 Frame-Based Structure}

At its core, S³ stores information as a sequence of time-ordered frames, each corresponding to a small window of the input signal (typically every 10 ms).

\paragraph{Structure:}
\begin{verbatim}
{
  "frames": [
    {
      "time": 0.010,
      "partials": [
        {
          "freq": 440.0,
          "db": -21.0,
          "isFundamental": true,
          "note": "A4+0",
          "rgb": [1.0, 0.0, 0.0],
          "harmonicIndex": 1
        },
        {
          "freq": 880.0,
          "db": -31.0,
          "isFundamental": false,
          "note": "A5+0",
          "rgb": [0.0, 0.0, 1.0],
          "harmonicIndex": 2
        }
      ]
    },
    ...
  ]
}
\end{verbatim}

\paragraph{Each frame contains:}
\begin{itemize}
    \item \texttt{time} (in seconds)
    \item \texttt{partials}: an array of objects describing frequency components
\end{itemize}

\paragraph{Each partial includes:}

\begin{center}
\begin{tabular}{|l|l|p{8cm}|}
\hline
\textbf{Field} & \textbf{Type} & \textbf{Description} \\
\hline
\texttt{freq} & float & Frequency in Hz \\
\texttt{db} & float & Amplitude in dBFS (normalized between -50 and 0) \\
\texttt{isFundamental} & bool & Whether this is the frame’s $f_0$ \\
\texttt{note} & string & Symbolic label, e.g. \texttt{"C4+7"} \\
\texttt{rgb} & list & RGB color derived from freq (used in visualization) \\
\texttt{harmonicIndex} & int & 1 for fundamental, 2+ for overtones \\
\hline
\end{tabular}
\end{center}

\subsection*{I.5.3 Data Resolution}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Dimension} & \textbf{Resolution} & \textbf{Example} \\
\hline
Time & 10 ms (adjustable) & 100 frames per second \\
Frequency & Float (cent-level precision) & 442.7 Hz \\
Amplitude & Float (-50 to 0 dB) & -27.3 dB \\
Pitch Notation & Microtonal (cent offset) & A4⁺⁷ \\
\hline
\end{tabular}
\end{center}

This high resolution enables perceptual modeling (via R³) and microtonal music analysis.

\subsection*{I.5.4 Supplementary Formats}

\paragraph{a. CSV for Unity or WebGL:}

Used in 3D interactive rendering:

\begin{verbatim}
time,frequency,dB,r,g,b
0.010,440.0,-21.0,255,0,0
0.010,880.0,-31.0,0,0,255
\end{verbatim}

\paragraph{b. Mesh Export (grid-based):}

For 3D terrain mapping:

\begin{verbatim}
x (time), y (frequency), z (normalized dB)
0.010, 440.0, 0.58
0.010, 880.0, 0.32
\end{verbatim}

\paragraph{c. OSC / Real-Time Formats:}

(Optional) For integration with Unity or Max/MSP:

\begin{verbatim}
/s3/frame 0.010 440.0 -21.0 1.0 0.0 0.0
\end{verbatim}

\subsection*{I.5.5 Schema Definitions (Formal)}

You may enforce structure via JSON schema:

\paragraph{Frame Schema:}

\begin{verbatim}
{
  "type": "object",
  "properties": {
    "time": { "type": "number" },
    "partials": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "freq": { "type": "number" },
          "db": { "type": "number" },
          "isFundamental": { "type": "boolean" },
          "note": { "type": "string" },
          "rgb": {
            "type": "array",
            "items": { "type": "number" },
            "minItems": 3,
            "maxItems": 3
          },
          "harmonicIndex": { "type": "integer" }
        },
        "required": ["freq", "db"]
      }
    }
  },
  "required": ["time", "partials"]
}
\end{verbatim}

This formalism ensures forward compatibility and guards against malformed data.

\subsection*{I.5.6 Storage Considerations}

For a 10-second file with 10 ms frames and 16 partials per frame:

\begin{itemize}
    \item 1,000 frames × 16 partials = 16,000 data points
    \item Approx. 2–5 MB as compressed JSON
    \item Easily processed in memory on modern systems
\end{itemize}

You can compress JSON with gzip or use a binary format (e.g., MessagePack) for lower latency streaming.

\subsection*{I.5.7 Reusability and Interoperability}

\begin{itemize}
    \item \texttt{partials.json} is the canonical format for R³ analysis
    \item \texttt{s3\_visualization.py} reads from it to generate high-res plots
    \item \texttt{export\_for\_unity.py} converts it to CSV for interactive 3D display
    \item Optional mapping to \texttt{.S3N} format (SRC standard, in progress)
\end{itemize}

\subsubsection*{Summary}

The data structures used by S³ are designed for high-resolution spectral modeling, downstream integration with resonance/cognitive modules, and compatibility with artistic, analytical, and interactive applications.

The JSON-based frame–partial model ensures that time, frequency, amplitude, and pitch data are tightly coupled and fully traceable. This standardization supports rigorous analysis, intuitive visualization, and machine readability.

\section*{I.6 – Visualization Layers and Aesthetic Design Principles}

\subsection*{I.6.1 Overview}

Visualization in the S³ module is not simply a graphical rendering of spectral data—it is a perceptually-aligned, musically meaningful, and aesthetically optimized representation of sound. The purpose of visualizing partials is twofold:

\begin{itemize}
    \item \textbf{Analytic Insight:} Allow researchers, theorists, and composers to examine the spectral and temporal structure of sound at microtonal and microtemporal resolution.
    \item \textbf{Cognitive Alignment:} Mirror how auditory structures are perceived, allowing visual artifacts to stand in for psychoacoustic phenomena such as overtone fusion, resonance, and vibrato.
\end{itemize}

The design principles of S³ visualizations stem from an integrated philosophy of scientific legibility, musical interpretation, and visual minimalism.

\subsection*{I.6.2 Two-Tier Visualization Architecture}

The system employs two synchronized visual layers:

\paragraph{Tier 1: Spectral Density Map (2D or 3D)}

\begin{itemize}
    \item \textbf{X:} Time (seconds), linear scale
    \item \textbf{Y:} Frequency (Hz), log scale
    \item \textbf{Z:} Amplitude (only in 3D mode)
\end{itemize}

\textbf{Units:}
\begin{itemize}
    \item Resolution: 3840 px × 2160 px (default)
    \item Frame step: 10 ms
    \item Frequency precision: cent-level ($\sim$0.58 px per cent)
\end{itemize}

Each partial is visualized as a dot or micro-line, positioned at its (time, frequency) coordinate, and styled according to amplitude and pitch-class-based color.

\paragraph{Tier 2: Symbolic Notation Layer}

\begin{itemize}
    \item Aligned on X (time) with Tier 1
    \item Contains labels for:
    \begin{itemize}
        \item Fundamental pitches (e.g., A4, C5⁺⁸)
        \item Onset durations (shown as segment lengths)
        \item Microtonal deviations (in cent format)
    \end{itemize}
    \item Can be toggled or overlaid for interpretive use
\end{itemize}

This layer enables mapping from physical spectrum to musical language (e.g., score-independent notation).

\subsection*{I.6.3 Color Mapping: Donut Spectrum}

Instead of static color coding (e.g., red = high freq), S³ implements a musically cyclic, frequency-based color system:

\paragraph{Principle:}
Colors cycle with octaves, not linear Hz.

\textbf{Base hue anchors:}

\begin{itemize}
    \item C: Red
    \item D: Orange
    \item E: Yellow
    \item F: Green
    \item G: Light Blue
    \item A: Blue
    \item B: Violet
\end{itemize}

Intermediate tones interpolate between anchors.

Repeat per octave: $\log_2(f/f_{\text{ref}}) \mod 1$

\paragraph{Example:}
\begin{itemize}
    \item 261.6 Hz (C4) $\rightarrow$ Red
    \item 440 Hz (A4) $\rightarrow$ Blue
    \item 1046 Hz (C6) $\rightarrow$ Red again
\end{itemize}

\paragraph{Mathematical Mapping:}

\[
\theta(f) = (\log_2(f/f_{\text{ref}}) \mod 1) \cdot 360^\circ
\]

Converted to HSV hue $\rightarrow$ RGB

This mapping aids intuitive identification of tonal color, enhances spectral grouping perception, and allows visual equivalence across octaves.

\subsection*{I.6.4 Amplitude and Visual Emphasis}

Amplitude (in dB) is mapped to:

\begin{itemize}
    \item Dot size (larger = louder)
    \item Opacity (higher dB = more solid)
    \item Optional Z-height (in 3D mesh)
\end{itemize}

Amplitude range is normalized between -50 dB and 0 dB.

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{dB} & \textbf{Size Range} & \textbf{Visual} \\
\hline
-50 & 1.0 px & barely visible \\
-30 & 3.5 px & translucent \\
-10 & 6.5 px & prominent \\
0   & 8.0 px & fully saturated \\
\hline
\end{tabular}
\end{center}

\subsection*{I.6.5 Hover and Interaction Design (Plotly/Unity)}

In 2D interactive views (Plotly), each partial responds to hover:

\paragraph{Display:}

\begin{verbatim}
Time: 3.140 s
Frequency: 445.6 Hz
Amplitude: -23.4 dB
Note: A4⁺¹⁵
Harmonic Index: 1
\end{verbatim}

This ensures each data point is interpretable in real time and can be cross-referenced with musical structure.

In Unity, similar hover behavior is achieved using Raycast + Tooltip systems.

\subsection*{I.6.6 Aesthetic Philosophy}

S³ visualizations are governed by three design maxims:

\begin{itemize}
    \item \textbf{Clarity over Colorfulness:} Every color and glyph must carry meaning—no arbitrary or decorative elements.
    \item \textbf{Sonic Minimalism:} Reflect the sparsity of partials, allow negative space, and avoid visual clutter.
    \item \textbf{Cognitive Load Management:} Layer complexity gradually (e.g., hide partials $<$ -40 dB by default), allow user toggles.
\end{itemize}

\subsection*{I.6.7 Multi-format Outputs}

\begin{itemize}
    \item \textbf{PNG:} Static, high-resolution (e.g., for papers, print). Rendered at 3840×2160 using Plotly + Kaleido.
    \item \textbf{HTML:} Interactive hover-capable plots. Zoom, layer toggle, export options.
    \item \textbf{CSV (Unity):} For 3D rendering (frequency $\rightarrow$ height). Used in VR/AR sound-space explorations.
\end{itemize}

\subsection*{I.6.8 Future Features}

\begin{itemize}
    \item Temporal motion blur to represent vibrato or tremolo
    \item Animated playback with cursor-following time marker
    \item 2.5D stacked pitch-class visualization (similar to piano roll, but spectral)
\end{itemize}

\subsubsection*{Summary}

Visualization in the S³ module is not a cosmetic feature—it is a tool for perception, cognition, and musical logic. Every graphical element is tied to a psychoacoustic or musical principle, and the rendering stack ensures that physical properties of sound are transposed into visually legible, interpretable, and aesthetically powerful structures.

\section*{I.7 – R³ and C³ Integration Architecture}

\subsection*{I.7.1 Introduction}

While S³ provides a high-resolution, physically-grounded representation of sound, it is only the first layer of the broader SRC⁹ system. The modules that follow—R³ (Resonance-Based Relational Reasoning) and C³ (Cognitive Consonance Circuit)—rely on the structured spectral data output by S³ to compute:

\begin{itemize}
    \item Harmonic relationships (R³)
    \item Resonance fields and perceptual centers (R³)
    \item Neural correlates of sound structure (C³)
    \item Bounded attention, emotional salience, and cognitive load (C³)
\end{itemize}

This section describes how data flows from S³ into R³ and C³, and how architectural compatibility is maintained across analytical, real-time, and interactive systems.

\subsection*{I.7.2 Data Flow Overview}

\textbf{From S³ to R³:}

\begin{itemize}
    \item \textbf{Input:} \texttt{partials.json}
    \item R³ extracts for each frame:
    \begin{itemize}
        \item $f_0$ (fundamental)
        \item Full harmonic field
        \item Frequency ratios
        \item Harmonic intervals
        \item Microtonal deviations
    \end{itemize}
\end{itemize}

\textbf{From R³ to C³:}

\begin{itemize}
    \item R³ outputs for each frame:
    \begin{itemize}
        \item Resonance potential ($\Phi$)
        \item Harmonic distance (HD)
        \item Phantom root (PR)
        \item Overtone locking (OL)
        \item Vectorized harmonic embeddings
    \end{itemize}
    \item C³ computes:
    \begin{itemize}
        \item Temporal Perceptual Stability (TPS)
        \item Tonal Fusion Index (TFI)
        \item Neural Synchronization Fields (NSF)
    \end{itemize}
\end{itemize}

\subsection*{I.7.3 Interface Specification}

\paragraph{JSON Format Standards:}
Each module reads and writes time-aligned frame arrays with shared conventions.

\textbf{Input from S³:}
\begin{verbatim}
{
  "frames": [
    {
      "time": 0.010,
      "partials": [
        { "freq": 440.0, "db": -21.0, "isFundamental": true }
      ]
    }
  ]
}
\end{verbatim}

\textbf{Output from R³:}
\begin{verbatim}
{
  "frames": [
    {
      "time": 0.010,
      "phantom_root": 65.4,
      "resonance_potential": 0.92,
      "harmonic_vectors": [[1, 0, -1], [0, 1, -2]],
      "harmonic_distances": [
        { "pair": [0, 1], "hd": 0.25 }
      ]
    }
  ]
}
\end{verbatim}

\textbf{Output from C³:}
\begin{verbatim}
{
  "frames": [
    {
      "time": 0.010,
      "tps": 0.82,
      "tfi": 0.61,
      "nsf": 0.45
    }
  ]
}
\end{verbatim}

\subsection*{I.7.4 Modular Code Interface (Python)}

Each module exposes a consistent API:

\paragraph{Example (R³):}
\begin{verbatim}
from r3_engine import compute_resonance_metrics
r3_out = compute_resonance_metrics(s3_data)  # JSON in, JSON out
\end{verbatim}

\paragraph{Example (C³):}
\begin{verbatim}
from c3_engine import compute_cognitive_metrics
c3_out = compute_cognitive_metrics(r3_out)
\end{verbatim}

This chainable interface design enables automated pipelines, real-time computation, and batch processing.

\subsection*{I.7.5 Streaming Compatibility (Optional)}

S³ frames can be streamed in real time (e.g., OSC or WebSocket) and passed frame-by-frame to downstream modules.

\textbf{Example OSC message:}
\begin{verbatim}
/s3/frame 0.012 441.3 -22.5 1.0 0.0 0.0
\end{verbatim}

\begin{itemize}
    \item R³ computes $\Phi$ and PR on the fly.
    \item C³ updates perceptual stability fields.
\end{itemize}

This architecture supports use in:

\begin{itemize}
    \item VR/AR environments
    \item Generative composition engines
    \item Real-time performance analytics
\end{itemize}

\subsection*{I.7.6 Synchronization and Latency}

To ensure downstream module alignment:

\begin{itemize}
    \item All frames are timestamped with exact onset time
    \item Optional global clock source can synchronize external sensors (e.g., EEG, motion)
    \item Each module can interpolate, pad, or drop frames to maintain temporal consistency
    \item Maximum allowable latency for inter-module propagation: $<$ 20 ms
\end{itemize}

\subsection*{I.7.7 Use Cases}

\begin{center}
\begin{tabular}{|l|l|p{8cm}|}
\hline
\textbf{Use Case} & \textbf{Modules Involved} & \textbf{Description} \\
\hline
Harmonic Field Tracking & S³ $\rightarrow$ R³ & Spectral components to resonance models \\
Tonal Center Estimation & S³ $\rightarrow$ R³ $\rightarrow$ C³ & Phantom root influences TPS and NSF \\
Real-Time Attention Feedback & S³ $\rightarrow$ C³ & EEG alignment with spectral events \\
Audio–Visual Synchronization & S³ $\rightarrow$ R³ + Unity & Spectral peaks drive color, shape, camera cues \\
\hline
\end{tabular}
\end{center}

\subsection*{I.7.8 Technical Stack and Dependencies}

\begin{itemize}
    \item \textbf{S³:} CREPE, librosa, numpy, Plotly, Unity (visual output)
    \item \textbf{R³:} Custom harmonic analysis engine, fractional prime decomposition, Euler Tonnetz geometry
    \item \textbf{C³:} numpy, scipy, TensorFlow (optional for neural modeling), EEG data pipelines
\end{itemize}

\subsection*{I.7.9 Logging and Diagnostics}

Each module logs:

\begin{itemize}
    \item Frame time
    \item Input checksum (for verification)
    \item Output integrity (range checks)
    \item Processing time (profiling)
    \item Synchronization offsets
\end{itemize}

A central dashboard (\texttt{src9\_monitor.py}) allows tracking of inter-module health and alignment.

\subsubsection*{Summary}

The integration of S³ with R³ and C³ establishes a vertically layered architecture:

\begin{itemize}
    \item S³ $\rightarrow$ raw perceptual primitives
    \item R³ $\rightarrow$ harmonic structure and resonance logic
    \item C³ $\rightarrow$ cognitive and emotional interpretation
\end{itemize}

The clear API design, standardized data structures, and optional real-time compatibility ensure that all modules communicate reliably, maintain synchronization, and can evolve independently while sharing a unified foundation.

\section*{I.8 – Optimization and Performance}

\subsection*{I.8.1 Overview}

Due to its high-resolution time–frequency modeling, microtonal accuracy, and support for large-scale audio datasets, the S³ module requires careful performance tuning. This section details the computational characteristics of each pipeline stage and outlines optimization strategies at multiple levels:

\begin{itemize}
    \item Algorithmic
    \item Architectural
    \item Real-time constraints
    \item Cross-platform compatibility (desktop, embedded, Unity)
\end{itemize}

\subsection*{I.8.2 Bottlenecks by Pipeline Stage}

\begin{center}
\begin{tabular}{|l|p{4.2cm}|p{3.2cm}|p{4.2cm}|}
\hline
\textbf{Stage} & \textbf{Description} & \textbf{Typical Bottleneck} & \textbf{Mitigation} \\
\hline
f₀ Extraction (CREPE) & Neural net inference & CPU-bound & Batch inference or GPU acceleration \\
Amplitude Estimation & RMS over frames & I/O bound & Pre-slice audio, frame caching \\
Harmonic Field Generation & Harmonic expansion per frame & Memory/compute & Vectorized array ops (NumPy) \\
Symbolic Mapping & Cents + RGB + note assignment & None (fast) & Already optimized \\
Visualization & Plotly rendering & GPU/UI bottleneck & Static output or throttled rendering \\
Unity Export & CSV generation & Disk I/O & Streamed JSON $\rightarrow$ buffer cache \\
\hline
\end{tabular}
\end{center}

\subsection*{I.8.3 Temporal and Spectral Resolution}

\paragraph{Time} 10 ms hop (100 FPS): sufficient for most music. Adjustable to 5 ms (high accuracy) or 20 ms (fast).

\paragraph{Frequency} Floating point (e.g. 442.76 Hz): cent-level ($\sim$0.6 px) precision. No quantization unless explicitly requested.

\paragraph{Amplitude} Normalized between -50 dB and 0 dB. Visualization supports amplitude-dependent size and color mapping.

\subsection*{I.8.4 Memory Footprint}

Assuming a 10-second clip at 100 FPS, 16 partials per frame:

\begin{itemize}
    \item Total frames: 1,000
    \item Total partials: $\sim$16,000
    \item Typical JSON size: 2–5 MB uncompressed
    \item In-memory size: $\sim$8–12 MB (with symbolic fields)
\end{itemize}

\textbf{�� Optimization Tip:} For long-form analysis, stream partials per segment (e.g., 100 frames) into memory, then flush.

\subsection*{I.8.5 Code-Level Optimizations}

Use NumPy for harmonic expansion:

\begin{verbatim}
harmonics = f0 * np.arange(1, N + 1)
\end{verbatim}

Use list comprehensions and avoid deeply nested loops.

Batch compute cent/pitch/RGB fields per frame.

Avoid recalculating pitch class mappings if frequency hasn’t changed.

\subsection*{I.8.6 Visualization Optimization}

\paragraph{2D (Plotly or Matplotlib)}

\begin{itemize}
    \item Throttle marker size and opacity for very low dB
    \item Hide partials < -40 dB (optional toggle)
    \item Use Kaleido for static rendering instead of Orca (faster)
\end{itemize}

\paragraph{3D (Unity)}

\begin{itemize}
    \item Use \texttt{DrawMeshInstanced()} instead of \texttt{GameObject} clones
    \item Use object pooling
    \item Export only "significant" partials (e.g., fundamental + harmonics up to -35 dB)
\end{itemize}

\subsection*{I.8.7 Real-Time Constraints}

To enable interactive use (e.g., in VR or live audio streams):

\begin{itemize}
    \item Use frame queues to pre-load analysis windows
    \item Perform $f_0$ + harmonic expansion in a separate thread
    \item Use audio input ring buffers (with PyAudio or SoundDevice)
\end{itemize}

\textbf{⚙️ Target latency budget:} $<$ 50 ms end-to-end

\subsection*{I.8.8 Cross-Platform Performance}

\begin{center}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Platform} & \textbf{Strategy} \\
\hline
Desktop (Python) & Full pipeline with CREPE + Plotly \\
Unity (C\#) & CSV import + GPU mesh render \\
Web (WebGL) & Pre-rendered \texttt{.html} or WASM visualizer \\
Embedded (Raspberry Pi) & Use downsampled audio + partial-only export \\
\hline
\end{tabular}
\end{center}

\subsection*{I.8.9 Performance Logging and Metrics}

S³ includes performance tracking hooks:

\begin{verbatim}
import time
start = time.time()
# ... processing ...
print(f"Step completed in {time.time() - start:.2f} seconds.")
\end{verbatim}

Future improvement: a dashboard that reports:

\begin{itemize}
    \item FPS throughput
    \item Memory usage
    \item Partial density over time
    \item Processing heatmap
\end{itemize}

\subsection*{I.8.10 Future Optimizations}

\begin{itemize}
    \item Use GPU-accelerated libraries (e.g. CuPy, TensorRT for CREPE)
    \item Parallel frame analysis with \texttt{multiprocessing} or \texttt{joblib}
    \item Use lightweight binary formats (\texttt{.msgpack} or \texttt{.protobuf}) for JSON
\end{itemize}

\subsubsection*{Summary}

S³ is designed for high fidelity and extensibility, but with attention to efficiency at each level. By combining frame-wise processing, vectorized operations, intelligent filtering, and GPU/Unity export strategies, the system remains responsive and scalable—from short musical phrases to full-length performances, from desktop analysis to embedded playback.

\section*{I.9 – References and Source Integration}

\subsection*{I.9.1 Overview}

The S³ module is grounded in a rich body of theoretical, technical, and scientific literature. This section documents the foundational sources that inform the system’s design, including references from music theory, signal processing, psychoacoustics, and cognitive neuroscience. It also provides integration notes for each cited source, detailing how the ideas have been translated into algorithmic and computational form within S³.

\subsection*{I.9.2 Spectral Music and Acoustic Theory}

\paragraph{Gérard Grisey \& Tristan Murail – Spectral Aesthetics}

\textbf{Reference:} Grisey, G. (1996). \textit{Did You Say Spectral?}

\textbf{Contribution:}

\begin{itemize}
    \item Rejection of abstract harmonic systems in favor of the overtone series
    \item Advocacy for time–frequency as a compositional space
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Partial tracking over time
    \item Harmonic field construction
    \item Overtone-based pitch logic
\end{itemize}

\paragraph{Pierre Schaeffer – Acousmatic Perception}

\textbf{Reference:} Schaeffer, P. (1966). \textit{Traité des objets musicaux}

\textbf{Contribution:}

\begin{itemize}
    \item Classification of sonic objects based on spectral content
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Time-framed spectral analysis
    \item Object-based spectral segmentation
\end{itemize}

\subsection*{I.9.3 Psychoacoustics and Human Perception}

\paragraph{Albert Bregman – Auditory Scene Analysis}

\textbf{Reference:} Bregman, A. S. (1990). \textit{Auditory Scene Analysis}

\textbf{Contribution:}

\begin{itemize}
    \item Stream segregation, grouping of partials by proximity
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Harmonic clustering
    \item Fundamental and overtone coherence tracking
\end{itemize}

\paragraph{Diana Deutsch – Perception of Pitch and Illusions}

\textbf{Reference:} Deutsch, D. (1982). \textit{The Psychology of Music}

\textbf{Contribution:}

\begin{itemize}
    \item Illusory pitch perception, frequency grouping
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Microtonal deviation representation
    \item Phantom root preparation for R³
\end{itemize}

\paragraph{Moore, B. – Critical Bands and Masking}

\textbf{Reference:} Moore, B. C. J. (2003). \textit{An Introduction to the Psychology of Hearing}

\textbf{Contribution:}

\begin{itemize}
    \item Modeling auditory filters and perceptual interference
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Overtone locking zone threshold (used in OL module of R³)
\end{itemize}

\subsection*{I.9.4 Signal Processing and Pitch Estimation}

\paragraph{Kim et al. – CREPE Pitch Tracker}

\textbf{Reference:} Kim, J., Salamon, J., Li, P., \& Bello, J. P. (2018). \textit{CREPE: A Convolutional Representation for Pitch Estimation}

\textbf{Contribution:}

\begin{itemize}
    \item Deep learning-based frame-by-frame pitch estimation
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Used for extracting $f_0$ from raw audio at 10 ms resolution
\end{itemize}

\paragraph{Librosa – Python Audio Toolkit}

\textbf{Reference:} McFee, B., Raffel, C., Liang, D., et al. (2015). \textit{librosa: Audio and Music Signal Analysis in Python}

\textbf{Contribution:}

\begin{itemize}
    \item Audio loading, STFT, RMS, cent mapping
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Core utility for RMS estimation, time axis construction
\end{itemize}

\subsection*{I.9.5 Mathematical and Symbolic Systems}

\paragraph{Lerdahl \& Jackendoff – Generative Theory of Tonal Music}

\textbf{Reference:} Lerdahl, F., \& Jackendoff, R. (1983). \textit{A Generative Theory of Tonal Music}

\textbf{Contribution:}

\begin{itemize}
    \item Cognitive modeling of grouping, metric structure
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Inspired tiered visualization (partials vs. symbolic layer)
\end{itemize}

\paragraph{Tymoczko, D. – Geometric Music Theory}

\textbf{Reference:} Tymoczko, D. (2011). \textit{A Geometry of Music}

\textbf{Contribution:}

\begin{itemize}
    \item Mapping musical structures to geometric topologies
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Inspires geometric pitch-space design (future S³–R³ integration)
\end{itemize}

\paragraph{Sethares, W. – Tuning, Timbre, Spectrum, Scale}

\textbf{Reference:} Sethares, W. A. (2005). \textit{Tuning, Timbre, Spectrum, Scale}

\textbf{Contribution:}

\begin{itemize}
    \item Non-Western tuning systems and perceptual consonance
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Support for non-12TET frequencies and just intonation
\end{itemize}

\subsection*{I.9.6 Cognitive and Neural Foundations}

\paragraph{Zatorre, Koelsch, Patel – Music and the Brain}

\textbf{References:}

\begin{itemize}
    \item Zatorre, R. J. (2002). \textit{Structure and function of auditory cortex}
    \item Koelsch, S. (2011). \textit{Towards a neural basis of music perception}
    \item Patel, A. D. (2008). \textit{Music, Language, and the Brain}
\end{itemize}

\textbf{Contribution:}

\begin{itemize}
    \item Mapping music perception to cortical activity
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Informing the downstream design of C³ module
    \item Supporting microtemporal precision as neurologically relevant
\end{itemize}

\subsection*{I.9.7 Visualization Systems}

\paragraph{Plotly – Interactive Graphics}

\textbf{Reference:} Plotly (open-source docs)

\textbf{Contribution:}

\begin{itemize}
    \item High-resolution, log-frequency 2D plotting
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Used for \texttt{s3\_visualization.py}, hover data, export to PNG/HTML
\end{itemize}

\paragraph{Unity – 3D Audio Visualization}

\textbf{Reference:} Unity Technologies (docs)

\textbf{Contribution:}

\begin{itemize}
    \item Mesh, prefab, and real-time rendering
\end{itemize}

\textbf{S³ Integration:}

\begin{itemize}
    \item Unity receives CSV exports for real-time 3D partial landscapes
\end{itemize}

\subsection*{I.9.8 Future Integrations}

\begin{itemize}
    \item Open Sound Control (OSC): For live audio input/output into S³ pipeline.
    \item VR/AR Extensions: Using Unity WebXR for immersive spectral interaction.
    \item EEG Integration APIs: Connecting S³ output with neural data for use in C³.
\end{itemize}

\subsubsection*{Summary}

The S³ module is deeply grounded in a wide range of academic sources, and every computational decision is anchored in at least one theoretical or empirical reference. From pitch tracking to visualization, from psychoacoustics to symbolic abstraction, S³ reflects a comprehensive and scholarly synthesis of the last century of acoustical, musical, and perceptual research.

\section*{I.10 – Appendices and Code Examples}

\subsection*{I.10.1 Overview}

This section provides detailed technical examples, supplemental illustrations, and code snippets to support the core content of the S³ module. These resources are designed for developers, researchers, and artists seeking to extend, test, or embed S³ into larger computational or artistic environments.

Each appendix includes a fully functional code excerpt, JSON schema, visualization samples, and system integration examples.

\subsection*{I.10.2 Appendix A: CREPE Extraction Script (\texttt{extract\_frequencies\_crepe.py})}

\begin{verbatim}
import crepe
import librosa
import numpy as np
import json
import sys

def extract_frequencies(audio_path, output_json, step_size=10, duration=10.0):
    y, sr = librosa.load(audio_path, sr=None, duration=duration)
    time, frequency, confidence, _ = crepe.predict(
        y, sr, step_size=step_size, model_capacity='full', viterbi=True
    )
    frames = []
    for t, f, c in zip(time, frequency, confidence):
        frames.append({
            "time": float(t),
            "partials": [
                {"freq": float(f), "db": 20 * np.log10(c + 1e-6), "isFundamental": True}
            ]
        })
    with open(output_json, 'w') as f:
        json.dump({"frames": frames}, f, indent=2)

if __name__ == "__main__":
    audio_path = sys.argv[1]
    output_json = sys.argv[2]
    extract_frequencies(audio_path, output_json)
\end{verbatim}

\subsection*{I.10.3 Appendix B: Harmonics Expansion Script (\texttt{harmonics\_matching.py})}

\begin{verbatim}
import json
import sys

def add_harmonics(input_json, output_json, max_harmonics=16):
    with open(input_json) as f:
        data = json.load(f)
    for frame in data["frames"]:
        base_freq = next((p["freq"] for p in frame["partials"] if p.get("isFundamental")), None)
        if base_freq:
            for n in range(2, max_harmonics + 1):
                frame["partials"].append({
                    "freq": n * base_freq,
                    "db": -30,
                    "isFundamental": False
                })
    with open(output_json, 'w') as f:
        json.dump(data, f, indent=2)

if __name__ == "__main__":
    add_harmonics(sys.argv[1], sys.argv[2])
\end{verbatim}

\subsection*{I.10.4 Appendix C: Frequency to RGB (Donut Spectrum Color Mapping)}

\begin{verbatim}
import math

NOTE_COLORS = [
    (255, 0, 0),       # C
    (255, 128, 0),     # D
    (255, 255, 0),     # E
    (0, 255, 0),       # F
    (0, 255, 255),     # G
    (0, 0, 255),       # A
    (128, 0, 255),     # B
]

def freq_to_rgb(freq):
    if freq <= 0:
        return (0, 0, 0)
    midi = 69 + 12 * math.log2(freq / 440.0)
    semitone = midi % 12
    anchor = [0, 2, 4, 5, 7, 9, 11]
    i = max([j for j in range(len(anchor)-1) if semitone >= anchor[j]])
    ratio = (semitone - anchor[i]) / (anchor[i+1] - anchor[i])
    r1, g1, b1 = NOTE_COLORS[i]
    r2, g2, b2 = NOTE_COLORS[(i+1) % len(NOTE_COLORS)]
    r = r1 + (r2 - r1) * ratio
    g = g1 + (g2 - g1) * ratio
    b = b1 + (b2 - b1) * ratio
    return (r / 255.0, g / 255.0, b / 255.0)
\end{verbatim}

\subsection*{I.10.5 Appendix D: Microtonal Notation Generator}

\begin{verbatim}
def freq_to_microtonal(freq):
    import math
    A4 = 440.0
    NOTES = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
    midi_float = 69 + 12 * math.log2(freq / A4)
    note_index = int(round(midi_float))
    cent_offset = int(round((midi_float - note_index) * 100))
    octave = (note_index // 12) - 1
    note = NOTES[note_index % 12]
    return f"{note}{octave}{cent_offset:+}"
\end{verbatim}

\subsection*{I.10.6 Appendix E: Plotly 2D Visualization Script (\texttt{s3\_visualization.py})}

\begin{verbatim}
import json, plotly.graph_objs as go, numpy as np

def amplitude_norm(db): return max(0.0, min(1.0, (db + 50) / 50))

def draw_s3(json_path, png_out):
    with open(json_path) as f: data = json.load(f)
    X, Y, C, S, T = [], [], [], [], []
    for frame in data["frames"]:
        t = frame["time"]
        for p in frame["partials"]:
            X.append(t)
            Y.append(p["freq"])
            amp = amplitude_norm(p["db"])
            S.append(2 + amp * 8)
            rgb = tuple(int(c * 255) for c in freq_to_rgb(p["freq"]))
            C.append(f"rgb({rgb[0]},{rgb[1]},{rgb[2]})")
            T.append(f"{p['freq']:.2f} Hz @ {p['db']:.1f} dB")
    fig = go.Figure(go.Scatter(x=X, y=Y, mode='markers', marker=dict(color=C, size=S), text=T, hoverinfo='text'))
    fig.update_layout(width=3840, height=2160, yaxis=dict(type='log', title='Frequency (Hz)'), xaxis=dict(title='Time (s)'))
    fig.write_image(png_out)

# Example:
# draw_s3("partials.json", "s3_visualization.png")
\end{verbatim}

\subsection*{I.10.7 Appendix F: JSON Schema Snippet}

\begin{verbatim}
{
  "type": "object",
  "properties": {
    "frames": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "time": { "type": "number" },
          "partials": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "freq": { "type": "number" },
                "db": { "type": "number" },
                "isFundamental": { "type": "boolean" }
              },
              "required": ["freq", "db"]
            }
          }
        },
        "required": ["time", "partials"]
      }
    }
  }
}
\end{verbatim}

\subsection*{I.10.8 Appendix G: Screenshots and Diagrams}

Omitted in text version – included in report PDF or interactive Jupyter companion notebook.

\begin{itemize}
    \item Spectrogram at 3840×2160 resolution
    \item Donut color wheel illustration
    \item Partial track overlays
    \item Unity-based 3D cube field
\end{itemize}

\subsubsection*{Summary}

The \textbf{S³ MasterTechnicalReport(Enhanced)} concludes its first part with complete technical references, reusable code snippets, and implementation-ready structures. These examples empower researchers, developers, and artists to reconstruct the entire pipeline, audit its logic, or embed it into larger analytical or creative ecosystems.

This foundation now fully supports the integration of R³ (Resonance-Based Relational Reasoning) and C³ (Cognitive Consonance Circuit), enabling next-generation modeling of music as a spectrum–resonance–consciousness continuum.

\title{R3-MasterTechnicalReport-Enhanced}
\author{Amac Erdem}
\date{May 2025}

\begin{document}

\maketitle

\section{Introduction}
\section*{I.1(1) – Motivation and Origins (Enhanced)}

Music theory, for centuries, has been dominated by symbolic and categorical thinking. Chords are labeled; keys are named; progressions are prescribed. These abstractions, while elegant and effective in many stylistic contexts, fail to reflect the continuous, physical nature of acoustic phenomena and the non-discrete, probabilistic mechanisms of perception. This misalignment between how music is theorized and how it is actually heard is a foundational problem that motivates the creation of R³.

In recent decades, both scientific and artistic movements have exposed the limits of traditional pitch-class and functional harmony systems:

\begin{itemize}
    \item \textbf{Spectral music}, led by composers such as Gérard Grisey and Tristan Murail, focused on the actual overtone content of sound rather than idealized chords.
    \item \textbf{Psychoacoustic studies} (Terhardt, 1974; Plomp \& Levelt, 1965) demonstrated that perceived pitch and consonance are emergent properties of partial alignment, not of symbolic classification.
    \item \textbf{Neuroscience findings}, including frequency-following response (FFR) and brainstem phase-locking studies (cf. Bidelman et al., 2011), showed that the brain tracks periodicity and overtone structures even without conscious musical attention.
    \item \textbf{Just intonation and extended harmonic systems} (Doty, 2002; Sethares, 1998) provided mathematical models of tuning that highlight resonance, not abstraction, as the organizing principle of pitch space.
\end{itemize}

The convergence of these fields leads to a necessary shift in harmonic reasoning: from symbolic theory to resonance-based computation.

\textbf{R³ (Resonance-Based Relational Reasoning)} is designed to model music not as a grammar of discrete signs, but as a flow of structured vibrational energy. It treats pitch as a function of spectral gravity, energy proximity, and temporal anchoring, rather than categorical labeling.

At the core of R³ lie three principles:

\begin{itemize}
    \item \textbf{Tonal centers are emergent}\\
    $\rightarrow$ They result from statistical convergence of overtones, not pre-defined labels.
    
    \item \textbf{Resonance is continuous}\\
    $\rightarrow$ Harmonic coherence is not binary (consonant vs. dissonant), but a gradient, computable via $\Phi$.
    
    \item \textbf{Perception is relational}\\
    $\rightarrow$ Tonal stability is derived from how partials interact — in time, in frequency, and in amplitude — not from isolated entities.
\end{itemize}

This paradigm is grounded not only in music but in broader systems theory, signal processing, and neuroacoustics. The resonance field becomes the new tonal map, where perception, meaning, and emotion are drawn not on a grid of pitches, but across a topology of dynamic acoustic pressure.

As Zatorre and Salimpoor (2013) observed, musical reward correlates with prediction and violation in time-dependent structures — R³ provides the computational substrate for such dynamics. Through $\Phi$, PR, and RFM, it becomes possible to chart the internal logic of sound as it unfolds, not as a score, but as a fluid field of tonal potential.

\subsubsection*{Summary}

The motivation for R³ is not merely the refinement of harmony theory. It is the redefinition of what harmony is: no longer a symbolic artifact, but a topological, energetic, and cognitive resonance surface.

\section*{I.2(1) – Integration within SRC⁹ Architecture (Enhanced)}

SRC⁹ is designed as a modular, multi-domain cognitive-auditory system with three principal modules:

\begin{itemize}
    \item \textbf{S³:} Spectral Sound Space – acoustic extraction, microtonal analysis
    \item \textbf{R³:} Resonance-Based Relational Reasoning – harmonic topology, field modeling
    \item \textbf{C³:} Cognitive Consonance Circuit – perceptual synthesis, memory, valuation
\end{itemize}

R³ sits at the exact center of this architecture — mathematically, informationally, and conceptually.

It functions as the Y-axis of the SRC⁹ lattice, where:

\begin{itemize}
    \item X = spectral frequency space (S³)
    \item Y = resonant interaction space (R³)
    \item Z = perceptual response dimension (C³)
\end{itemize}

This triaxial topology is not metaphorical — it is computationally enforced through data routing, inter-unit feedback, and shared time/frequency schemas.

\paragraph{1. Input from S³}

R³’s input is the fully expanded harmonic spectrum:

\begin{verbatim}
{
  "time": 2.1,
  "partials": [
    {
      "freq": 196.0,
      "amplitude": 0.84,
      "isFundamental": true,
      "harmonic_index": 0,
      "symbol": "G3⁰"
    },
    ...
  ]
}
\end{verbatim}

Sampling: 0.1s frames (200 per 20s session)

Per Frame: 1 f₀ + 16 harmonics

Extras: Microtonal symbol mapping, cent deviation flags

Each R³ unit extracts features relevant to its function (e.g., PR focuses on $f_0$ trajectory; RFM on amplitude–frequency density).

\paragraph{2. Internal Resonance Processing (R³)}

Within R³, each unit runs in temporal and spectral parallel. However, their semantic roles are different:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit} & \textbf{Function} & \textbf{Outputs To} \\
\hline
PRU & Virtual root estimation & RP, RFM, CRV \\
RPU & Framewise \& windowed $\Phi$ & CRV, RFM \\
RFMU & Field topology (RFM, $\nabla$RFM) & CRV, visualization \\
CRVU & Vector summary [TPS, TFI, NSF] & C³ – attention \& memory \\
\hline
\end{tabular}
\end{center}

Together, these units transmute the raw spectrum into a dynamic resonance surface — a structure rich in perceptual cues, mathematical relations, and cognitive triggers.

\paragraph{3. Output to C³}

R³’s final output — the CRV vector — is transmitted to C³ for use in:

\begin{itemize}
    \item Attention anchoring: TPS stability scores influence focus
    \item Fusion response modeling: TFI reflects perceptual unity
    \item Temporal expectation modeling: NSF controls time-weighted relevance
\end{itemize}

These values modulate cognitive consonance curves and neural resonance gating within higher-order evaluative modules.

\paragraph{4. Bidirectional Modulation}

Though R³ receives data from S³ and passes to C³, the flow is not strictly feedforward.

R³ can also be:

\begin{itemize}
    \item Influenced by C³ feedback, adjusting $\Phi$ weighting or field granularity based on attention level
    \item Trigger of S³ re-analysis, when PR or TFI instability passes a threshold
\end{itemize}

This recurrent loop allows SRC⁹ to behave like a resonant cognitive engine — not just analyzing, but reacting to musical content dynamically.

\paragraph{5. Implementation Topology}

The data flow between S³, R³, and C³ can be visualized as:

\begin{verbatim}
             [ S³ ]  →  Raw Spectrum (partials)
               ↓
         ┌─────────────┐
         │     R³      │
         │ PR → Φ → RFM│
         └────┬────────┘
              ↓
           [ CRV ]
              ↓
            [ C³ ]
       (Attention, Valuation, Memory)
\end{verbatim}

Each arrow represents a JSON/array structure or vector of numerical values, all time-aligned, standardized, and validated.

\paragraph{6. Architectural Significance}

R³ enables SRC⁹ to:

\begin{itemize}
    \item Move beyond symbolic analysis
    \item Embrace probabilistic, topological, and energy-based models
    \item Bridge physics (S³) with cognition (C³)
    \item Operate in real-time or offline batch analysis
    \item Output data suitable for visualization, sonification, or interactivity (Unity)
\end{itemize}

\section*{II.1(2) – Domain Definitions: Traditional vs. Resonance-Based Theory}

In the architecture of SRC⁹, a domain is not merely a categorical label — it defines the epistemic framework by which musical structure is interpreted. Domains provide the foundational assumptions, theoretical orientation, and algorithmic style of all downstream unit processing.

Two such domains are currently defined within the R³ module architecture:

\subsection*{A. Traditional-Based Theory (Reserved Domain)}

\textbf{Philosophical Basis:}
\begin{itemize}
    \item Music is a symbolic grammar of signs.
    \item Tonality arises from hierarchical relations between named pitch classes.
    \item Harmony is a sequence of discrete, functional progressions (e.g., tonic $\rightarrow$ dominant $\rightarrow$ subdominant).
\end{itemize}

\textbf{Historical Sources:}
\begin{itemize}
    \item Rameau’s fundamental bass theory
    \item Roman numeral analysis
    \item Common-practice tonality (1600–1900)
    \item Schenkerian structuralism
\end{itemize}

\textbf{Computational Analogy:}
\begin{itemize}
    \item Rule-based systems
    \item Symbol-to-symbol transitions
    \item State machines over pitch-class sets
\end{itemize}

\textbf{Current Status in SRC⁹:}

\begin{itemize}
    \item Unpopulated. The domain is intentionally preserved for comparative or pedagogical use but no active units are assigned.
\end{itemize}

\textbf{Future Possibility:}

\begin{itemize}
    \item Incorporating classical root analysis
    \item Mapping Roman numeral logic to PR output
    \item Providing symbolic contrast to field-theoretic R³ outputs
\end{itemize}

\subsection*{B. R³ – Resonance-Based Theory (Active Domain)}

\textbf{Philosophical Basis:}
\begin{itemize}
    \item Music is a flow of interacting energy fields.
    \item Tonality emerges from the statistical convergence of partials.
    \item Harmony is a dynamic topology shaped by amplitude, frequency, and time.
\end{itemize}

\textbf{Scientific Foundations:}
\begin{itemize}
    \item Psychoacoustics (Plomp \& Levelt, 1965; Terhardt, 1974)
    \item Spectral composition (Grisey, Murail)
    \item Just intonation and tuning theory (Doty, Sethares)
    \item Auditory neuroscience (Zatorre, Bidelman, FFR)
\end{itemize}

\textbf{Computational Strategy:}
\begin{itemize}
    \item Vector calculus in spectral domains
    \item Probabilistic detection of resonance attractors
    \item Topographic field modeling (RFM)
    \item Temporal variance and cognitive vector construction (CRV)
\end{itemize}

\subsection*{Comparison Table: Symbolic vs. Resonant Modeling}

\begin{center}
\begin{tabular}{|l|p{4.5cm}|p{5.5cm}|}
\hline
\textbf{Aspect} & \textbf{Traditional Theory} & \textbf{R³ – Resonance-Based Theory} \\
\hline
Pitch Type & Discrete pitch classes & Continuous frequency space \\
Harmony Basis & Root progression (symbolic) & Spectral proximity \& convergence \\
Time Treatment & Syntactic units (measures) & Framewise + windowed resonance maps \\
Tonal Center Definition & Key, tonic function & Phantom root via overtone intersection \\
Mathematical Form & Rule-based grammar & Scalar field + differential operators \\
Cognitive Link & Abstract schema & Auditory energy alignment (FFR) \\
Adaptivity & Static system & Real-time dynamic modeling \\
\hline
\end{tabular}
\end{center}

\subsection*{Why Dual Domains?}

SRC⁹ includes both domains to support:

\begin{itemize}
    \item Comparative modeling
    \item Pedagogical clarity
    \item Theoretical transparency
\end{itemize}

Although the Traditional-Based Theory domain is empty, its presence allows users to compare symbolic models vs. resonance models, test different theoretical assumptions, and use R³ as both an analytical tool and a research platform.

Eventually, units such as:

\begin{itemize}
    \item Tonic Function Classifier (TFC)
    \item Symbolic Root Evaluator (SRE)
    \item Key Probabilistic Mapper (KPM)
\end{itemize}

...may populate the Traditional domain for hybrid analysis.

\subsection*{Domain Assignment Summary}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Unit} & \textbf{Domain} \\
\hline
PRU & R³ – Resonance-Based \\
RPU & R³ – Resonance-Based \\
RFMU & R³ – Resonance-Based \\
CRVU & R³ – Resonance-Based \\
OL (future) & R³ – Resonance-Based \\
\hline
\textbf{Traditional-Based Theory} & (empty) \\
\hline
\end{tabular}
\end{center}

\section*{II.2(1) – PRU: Phantom Root Unit (Enhanced)}

The Phantom Root Unit (PRU) models one of the most perceptually paradoxical yet experientially central aspects of musical cognition: the ability to perceive a fundamental pitch even when it is not acoustically present. This unit formalizes and operationalizes the phenomenon of “virtual pitch” by detecting statistical convergence across harmonic spectra and time.

\subsection*{1. Theoretical Background and Cognitive Justification}

Since the 1970s, experimental psychoacoustics has demonstrated that listeners can identify the “missing fundamental” of a complex tone purely from overtone relationships (Terhardt, 1974; Houtsma \& Goldstein, 1972). These percepts are non-linear and emergent: they do not result from individual partials, but from their alignment in log-frequency space.

Auditory scene analysis research (Bregman, 1990) shows that virtual pitch perception is guided by temporal stability, harmonic simplicity, and statistical regularity. PRU unifies these dimensions using an intersection-based harmonic model, a vector-space fusion score, and symbolic microtonal tagging.

In neurophysiological terms, virtual pitch tracking is associated with brainstem-level phase locking (cf. Bidelman \& Krishnan, 2011), where neurons track periodicity of inferred tones even when the spectral content is incomplete. PRU aligns with this processing architecture.

\subsection*{2. Input Requirements}

\begin{itemize}
    \item \textbf{Source:} \texttt{RawSpectrum-unit.json}
    \item \textbf{Frame resolution:} 0.1s
    \item \textbf{Each frame includes:} fundamental + 16 harmonics
    \item \textbf{Symbolic labeling:} cent-deviation-based (e.g., G3⁺¹)
\end{itemize}

Partial data are first segmented into fundamental frequency sequences, grouped by $\pm49$ cent stability window, and then processed through pattern recognition templates.

\subsection*{3. Core Algorithmic Stages}

\paragraph{a. CentTracker}

Tracks whether the current fundamental frequency remains within a $\pm49$ cent band. Once a deviation exceeds threshold, a note boundary is declared.

Let $f_n$ be the current $f_0$ value:

\[
\text{if } \left| \text{cents}(f_n, f_{n-1}) \right| > 49 \Rightarrow \text{new note}
\]

This creates time-stable pitch clusters, e.g., G2 (1.4s–2.1s), D3 (2.2s–3.3s), G3 (3.4s–4.2s)

\paragraph{b. GroupMatcher}

Matches these sequential pitch groups to harmonic templates such as:

\begin{itemize}
    \item [1, 2, 3] $\rightarrow$ simple harmonic stack
    \item [1, 2, 3, 4, 5] $\rightarrow$ extended overtone set
    \item [1–11] $\rightarrow$ full resonance model (A4 group)
\end{itemize}

Given $N$ consecutive note frequencies $\{f_1, f_2, \ldots, f_N\}$, PRU seeks a scalar $r$ such that:

\[
f_i \approx r \cdot h_i \quad \text{for harmonic template } H = \{h_1, h_2, \ldots, h_N\}
\]

The best-fit $r$ becomes the phantom root candidate.

\paragraph{c. Harmonic Intersection Scoring (HIS)}

To evaluate whether partials align at a common origin, PRU computes:

\[
\text{HIS}(r) = \sum_{i<j} \#(H_i \cap_\epsilon H_j)
\]

Where:

\begin{itemize}
    \item $H_i = \{k \cdot f_i\}$: harmonic set of note $i$
    \item $\cap_\epsilon$: fuzzy intersection under cent-tolerance $\epsilon$
\end{itemize}

This gives a resonance-weighted score of harmonic “compatibility.”

\paragraph{d. Fusion Metric: Harmonic Information Score (HIM)}

As an alternative or supplement, PRU may also compute the HIM:

\[
\text{HIM}\left(\frac{b}{a}\right) = \frac{a \cdot b}{a + b - 1}
\]

This models perceptual fusion likelihood (cf. Sethares, 1998) and allows comparing different PR candidates.

\paragraph{e. Prime-Exponent Averaging (Optional)}

For symbolic systems like Just Intonation or Prime-Limit modeling, PRU represents each note as a vector:

\[
\vec{v}_i = (x_2, x_3, x_5, x_7, \ldots), \quad \text{where } f_i = 2^{x_2} \cdot 3^{x_3} \cdot 5^{x_5} \cdots
\]

The mean vector:

\[
\vec{v}_{PR} = \frac{1}{N} \sum_i \vec{v}_i
\]

...is projected back into frequency space to suggest a symbolic root in prime-vector space.

\paragraph{f. SymbolMapper}

Final phantom root frequency is mapped into symbolic pitch notation:

\begin{itemize}
    \item Cent bins: $\pm$25 cent steps
    \item Output: \texttt{C3⁺²}, \texttt{A\#2⁻¹}, etc.
\end{itemize}

\subsection*{4. Output Format}

\begin{verbatim}
{
  "time_range": [2.1, 4.2],
  "phantom_root": 130.81,
  "symbol": "C3⁺¹",
  "group": "A2",
  "fusion_score": 0.431
}
\end{verbatim}

Each record corresponds to a stable perceptual segment.

\subsection*{5. Integration & Downstream Use}

\begin{itemize}
    \item \textbf{Feeds RPU:} defines resonance weighting center
    \item \textbf{Feeds RFMU:} sets initial attractor in field
    \item \textbf{Feeds CRVU:} primary input for TPS (temporal stability)
\end{itemize}

\section*{II.2(2) – RPU: Resonance Potential Unit (Φ)}

The Resonance Potential Unit (RPU) provides a scalar measure of spectral coherence — quantifying how “tightly” the partials within a time window resonate with one another. The $\Phi$ metric is a mathematically continuous and perceptually grounded proxy for what listeners describe as "harmonic richness," "fusion," or "tonal gravity."

\subsection*{1. Scientific Foundation and Perceptual Basis}

Consonance perception is not binary but continuous. Listeners experience certain spectra as more stable or fused depending on how close and strong partials are in frequency and amplitude. This insight, originating from psychoacoustic studies by Plomp \& Levelt (1965), forms the perceptual core of $\Phi$.

Further evidence from Bidelman et al. (2011) and Leman (2016) suggests that harmonic coherence triggers neural entrainment and reward systems. High $\Phi$ values are theorized to activate neural structures such as the nucleus accumbens (associated with musical pleasure) and generate higher predictive certainty in auditory processing streams.

\subsection*{2. Input Model}

\begin{itemize}
    \item Input file: \texttt{RawSpectrum-unit.json}
    \item Frame duration: 0.1 seconds
    \item Each frame: list of partials (freq, amp)
\end{itemize}

All $\Phi$ computations are frame-aligned, amplitude-weighted, and cent-aware.

\subsection*{3. Core Algorithmic Flow}

\paragraph{a. Pairwise $\Phi$ Matrix}

For every frame $t$, a full pairwise distance matrix is constructed across all partials:

\[
\Phi(t) = \sum_{i<j} \frac{A_i \cdot A_j}{|f_i - f_j| + \epsilon}
\]

Where:

\begin{itemize}
    \item $A_i$, $A_j$: amplitudes
    \item $f_i$, $f_j$: frequencies
    \item $\epsilon$: small constant (1e⁻⁶)
\end{itemize}

This formulation ensures:

\begin{itemize}
    \item Higher amplitude = higher resonance impact
    \item Smaller frequency distance = stronger spectral fusion
\end{itemize}

Each $\Phi(t)$ is a scalar representing the spectral “density” or “pull” within that frame.

\paragraph{b. Time-Windowed Integration ($\Phi_T$)}

To capture longer-term coherence, sliding time windows (1s, 3s, 5s, 7s) are defined:

\[
\Phi_T = \sum_{t \in T} \Phi(t)
\]

This models harmonic pressure across time — analogous to field density in physics.

\textbf{Used for:}
\begin{itemize}
    \item Detecting modulation
    \item Smoothing over local instability
    \item Feeding into NSF (memory-weighted) metrics in CRVU
\end{itemize}

\paragraph{c. Optional Field-Weighted $\Phi$}

Future extensions may modulate $\Phi$ with field attractor weights (from RFM):

\[
\Phi_{RFM}(t) = \sum_{i<j} \frac{A_i \cdot A_j \cdot \gamma(f_i, f_j)}{|f_i - f_j| + \epsilon}
\]

Where $\gamma$ is a Gaussian function centered on current PR or spectral centroid.

\subsection*{4. Fluctuation Modeling}

Variance in $\Phi(t)$ over time is highly correlated with tonal anchoring. A stable tonal segment will show low variance; modulation or dissonance increases fluctuation.

\[
\sigma_\Phi(t) = \frac{1}{N} \sum_{i=1}^N \left( \Phi_i - \bar{\Phi} \right)^2
\]

This index feeds into CRVU's TPS node (Temporal Perceptual Stability).

\subsection*{5. Output Formats}

\paragraph{Framewise:}
\begin{verbatim}
{
  "time": 4.1,
  "phi": 3.142
}
\end{verbatim}

\paragraph{Windowed:}
\begin{verbatim}
{
  "window": "6.0–9.0",
  "phi": 9.762,
  "window_size": 3
}
\end{verbatim}

Both forms are visually overlaid in multi-colored $\Phi$ curves (e.g., red = 1s, blue = 7s).

\subsection*{6. Integration Points}

\begin{itemize}
    \item \textbf{Feeds CRVU:} TPS (std dev), NSF (sum)
    \item \textbf{Feeds RFMU:} used as raw data for Gaussian smoothing
    \item \textbf{Optional input from PRU:} PR can shift $\Phi$ weighting center
\end{itemize}

\subsection*{7. Theoretical Impact}

$\Phi$ unifies the perceptual with the computational:

\begin{itemize}
    \item It approximates roughness, fusion, and tension metrics
    \item It enables real-time scalar monitoring of harmonic convergence
    \item It mathematically extends Plomp–Levelt’s roughness curve to high-resolution, amplitude-weighted spectral structures
\end{itemize}

\section*{II.2(3) – RFMU: Resonance Field Modeling Unit (Enhanced)}

The Resonance Field Modeling Unit (RFMU) constructs a continuous, dynamic field over frequency space that represents the distribution of spectral energy and tonal gravity. It transforms discrete spectral events into a spatiotemporal topography, allowing resonance to be visualized, measured, and interpreted as a scalar field.

\subsection*{1. Conceptual Foundation: Harmony as Field}

Traditional models define harmony as sequences of pitch-class structures. RFMU redefines harmony as field energy: the shape, slope, and peaks of a continuously distributed amplitude-weighted spectral surface.

This approach is inspired by:

\begin{itemize}
    \item Physical fields in electromagnetism and gravity
    \item Topographic models of pitch space (e.g., Tymoczko, 2006)
    \item Neural tonotopic maps (Moore, 2012)
    \item Auditory Gestalt theory, where pitch centers attract auditory focus
\end{itemize}

In RFM, harmonic tension and resolution are not rules — they are gradients across a vibratory surface.

\subsection*{2. Input Structure}

\begin{itemize}
    \item \textbf{Source:} \texttt{RawSpectrum-unit.json}
    \item \textbf{Per frame:} list of partials (freq, amp)
    \item \textbf{Each partial:} becomes a kernel on the field
\end{itemize}

\subsection*{3. Mathematical Core}

\paragraph{a. Resonance Field Equation}

\[
\text{RFM}(f, t) = \sum_{i=1}^N A_i(t) \cdot e^{-\frac{(f - f_i(t))^2}{2\sigma^2}}
\]

Where:

\begin{itemize}
    \item $f$: target frequency grid point
    \item $f_i(t)$: frequency of partial $i$ at time $t$
    \item $A_i(t)$: amplitude of partial $i$
    \item $\sigma$: resonance spread parameter (controls field smoothness)
\end{itemize}

This Gaussian convolution converts discrete partials into a smooth frequency-density curve.

\paragraph{b. Grid Discretization}

The frequency space (20–20000 Hz) is divided into $N$ points:

\[
f_j = \text{logspace}(20, 20000, N)
\]

$N$ is typically 512 or 1024, log-scaled to match auditory perception.

\paragraph{c. Field Gradient Operator ($\nabla$RFM)}

\[
\nabla \text{RFM}(f, t) = \frac{\partial \text{RFM}(f, t)}{\partial f}
\]

Implemented numerically via finite differences.

\begin{itemize}
    \item Points toward direction of tonal pull
    \item Magnitude of $\nabla$RFM is used in CRVU’s TFI node
\end{itemize}

\paragraph{d. Peak Tracking}

At each time $t$, RFMU extracts local maxima (peaks) in the field:

\begin{itemize}
    \item These peaks = tonal centers
    \item \textbf{Stability} = persistence of peak across frames
    \item \textbf{Drift} = shift in peak position over time
\end{itemize}

\subsection*{4. Visualization Logic}

\begin{itemize}
    \item \textbf{2D heatmap:}
    \begin{itemize}
        \item x = time
        \item y = frequency (log scale)
        \item color = field intensity (e.g., inferno colormap)
    \end{itemize}
    \item \textbf{Optional overlays:}
    \begin{itemize}
        \item white lines = peak paths
        \item vector arrows = $\nabla$RFM direction
    \end{itemize}
\end{itemize}

\subsection*{5. Output Format}

\begin{verbatim}
{
  "time": 3.5,
  "grid": [20.0, 25.1, ..., 20000.0],
  "field": [0.001, 0.0023, ..., 0.0],
  "gradient": [0.004, 0.003, ..., -0.002]
}
\end{verbatim}

\subsection*{6. Cognitive Function}

RFMU makes it possible to:

\begin{itemize}
    \item Observe resonance motion as a flow
    \item Detect zones of spectral convergence (tonal mass)
    \item Model directional pull (gradient dynamics) of harmonic tension
    \item Build resonance topographies for spatial cognition
\end{itemize}

\subsection*{7. Integration Pathways}

\begin{itemize}
    \item \textbf{To CRVU:}
    \begin{itemize}
        \item $\nabla$RFM $\rightarrow$ TFI
        \item RFM stability $\rightarrow$ TPS supplement
    \end{itemize}
    \item \textbf{To PRU:} Peak history $\rightarrow$ implied root movement
    \item \textbf{To C³:}
    \begin{itemize}
        \item Field visualization $\rightarrow$ VR/AR immersion
        \item Resonance attractors $\rightarrow$ attention targets
    \end{itemize}
\end{itemize}

\subsection*{8. Future Extensions}

\begin{itemize}
    \item \textbf{3D Field Volume:} Extend RFM to include time-depth (spectrovolume)
    \item \textbf{Field Curvature Analysis:} Use $\nabla^2$RFM to detect dissonance basins
    \item \textbf{Resonance Memory Maps:} Integrate $\nabla$RFM into NSF decay structures
\end{itemize}

\subsubsection*{Summary}

RFMU is where harmony becomes spatial. It transforms the linear structures of tonal analysis into landscapes of spectral motion — surfaces that pull, release, and define perceptual musical form.

\section*{II.2(4) – CRVU: Cognitive Resonance Vectoring Unit (Enhanced)}

The Cognitive Resonance Vectoring Unit (CRVU) synthesizes the outputs of R³ into a compact, perceptually-relevant vector. It provides a quantitative bridge from spectral resonance data to cognitive interpretation, enabling SRC⁹ to assess how stable, fused, and memory-relevant a sound structure is over time.

CRVU defines tonal cognition not as symbolic logic, but as a vector of resonance behaviors.

\subsection*{1. Scientific \& Neurocognitive Motivation}

Listeners do not passively receive spectral information — they process it in terms of:

\begin{itemize}
    \item \textbf{Stability:} Is the tonal center clear and persistent?
    \item \textbf{Fusion:} Do the partials cohere into a unified sound?
    \item \textbf{Memory anchoring:} Does the resonance persist cognitively over time?
\end{itemize}

These questions are reflected in core neural structures:

\begin{itemize}
    \item TPJ + ACC: involved in perceptual switching and ambiguity resolution
    \item Auditory cortex: phase-locking and coherence detection
    \item Hippocampus: short-term memory formation of tonal events
    \item Nucleus accumbens: reward prediction via temporal regularity
\end{itemize}

CRVU mathematically simulates these perceptual axes through three metrics: TPS, TFI, and NSF.

\subsection*{2. Input Requirements}

CRVU consumes:

\begin{itemize}
    \item Framewise $\Phi$ (from RPU) $\rightarrow$ resonance magnitude over time
    \item Gradient field $\nabla$RFM (from RFMU) $\rightarrow$ fusion and divergence flow
\end{itemize}

Each input is frame-aligned at 0.1s resolution, processed into single or windowed summary statistics.

\subsection*{3. Metric Computation}

\paragraph{a. Temporal Perceptual Stability (TPS)}

\[
\text{TPS} = \frac{1}{1 + \sigma_\Phi(t)}
\]

Computes standard deviation of $\Phi$ over entire (or local) window.

\begin{itemize}
    \item High TPS = consistent tonal field $\rightarrow$ strong perceptual anchoring
\end{itemize}

\paragraph{b. Tonal Fusion Index (TFI)}

\[
\text{TFI} = \frac{1}{1 + \langle |\nabla \text{RFM}(f, t)| \rangle}
\]

Measures average steepness of resonance field across time.

\begin{itemize}
    \item Flatter fields = stronger fusion
    \item Steeper slopes = divergence, spectral instability
\end{itemize}

\paragraph{c. Neural Synchronization Field (NSF)}

\[
\text{NSF} = \sum_t \Phi(t) \cdot e^{-\alpha t}
\]

Applies exponential decay ($\alpha = 0.05$–$0.1$) to earlier $\Phi(t)$ values.

\begin{itemize}
    \item Models short-term auditory memory + attention decay
    \item High NSF = early, strong, cohesive resonance = likely to be encoded
\end{itemize}

\subsection*{4. Node Architecture}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Node} & \textbf{Input} & \textbf{Output} \\
\hline
TPSNode & $\Phi(t)$ & Scalar $\in [0, 1]$ \\
TFINode & $\nabla$RFM$(f, t)$ & Scalar $\in [0, 1]$ \\
NSFNode & $\Phi(t)$, $\alpha$ & Scalar $\in [0, 1]$ \\
\hline
\end{tabular}
\end{center}

Each node is independent but computed over the same time base.

\subsection*{5. Output Format}

\begin{verbatim}
{
  "TPS": 0.812,
  "TFI": 0.693,
  "NSF": 0.0385
}
\end{verbatim}

Vector = 3-tuple $\langle$stability, fusion, memory$\rangle$

Normalized to $[0, 1]$ for visual and cognitive mapping.

\subsection*{6. Visual Encoding}

\begin{itemize}
    \item Displayed as stacked horizontal bars (216px high)
    \item Red = TPS, Green = TFI, Blue = NSF
    \item Used as perceptual “signature” for a sound segment
    \item Overlayable on waveform, RFM, or symbolic score
\end{itemize}

\subsection*{7. Integration with C³}

CRVU directly feeds into C³'s interpretive layers:

\begin{itemize}
    \item \textbf{CTU – Cognitive Tension Unit:} TPS modulates expectedness
    \item \textbf{NSU – Neural Sync Unit:} NSF correlates with phase-locking metrics
    \item \textbf{PIU – Phenomenological Immersion Unit:} TFI relates to absorption metrics
\end{itemize}

CRVU is the only R³ unit whose output is designed to directly map onto affective and attentional models.

\subsection*{8. Future Extensions}

\begin{itemize}
    \item Weighted CRV vectors across musical phrases
    \item Real-time CRV streaming for interactive music engines
    \item CRV-linked generation: use resonance signature to drive AI composition
    \item Fusion + stability mapping across multichannel inputs (ensemble CRV)
\end{itemize}

\subsubsection*{Summary}

CRVU is the cognitive mirror of R³ — a window into how sound, structured by physics and filtered by resonance, becomes psychologically meaningful.

\section*{III.1 – Phantom Root Estimation (Enhanced)}

The phenomenon of phantom root perception — the brain's ability to identify a "missing" fundamental from a group of overtones — is among the most counterintuitive findings in auditory science. Unlike direct pitch recognition, it requires a form of inferred periodicity, where the brain estimates the source of harmonic structure based solely on spectral relationships.

The PRU formalizes this cognitive mechanism using harmonic intersection and vector matching models.

\subsection*{1. Harmonic Intersection Formula}

The core mathematical function is:

\[
\text{PR} = \arg\max_f \sum_{i<j} \#(H_i \cap_\epsilon H_j)
\]

Where:

\begin{itemize}
    \item $H_i = \{k \cdot f_i \mid k \in \mathbb{N} \}$
    \item $\cap_\epsilon$: fuzzy intersection within a cent tolerance $\epsilon$ (typically $\pm49$ cents)
    \item $\#$: counts the number of overlapping harmonics
\end{itemize}

\textbf{Interpretation:} This function seeks the base frequency $f$ whose harmonic series would generate the highest number of overtone alignments across a group of perceived pitches. Even if $f$ is not acoustically present, it may be implied by these intersections.

\subsection*{2. Harmonic Template Matching}

To operationalize this, PRU evaluates grouped sequences (e.g., G2–D3–G3) against canonical harmonic stacks:

\begin{itemize}
    \item Group A: [1,2,3]
    \item Group A1: [1–5]
    \item Group A2: [1–7]
    \item Group A3: [1–9]
    \item Group A4: [1–11]
\end{itemize}

Given note sequence $\vec{f} = [f_1, f_2, ..., f_n]$, we search for base frequency $r$ such that:

\[
f_i \approx r \cdot h_i \quad \forall i, h_i \in H
\]

Where $H$ is the harmonic template.

A candidate is accepted if average error:

\[
\epsilon_{\text{avg}} = \frac{1}{n} \sum_i \left| \frac{f_i - r \cdot h_i}{r \cdot h_i} \right| < \delta
\]

(Default: $\delta = 0.03$, i.e., 3\% deviation)

\subsection*{3. Harmonic Information Metric (HIM)}

To further differentiate candidates, a perceptual fusion metric is computed:

\[
\text{HIM}\left(\frac{b}{a}\right) = \frac{a \cdot b}{a + b - 1}
\]

As introduced in Sethares (1998), this metric estimates how well a ratio $\frac{a}{b}$ supports tonal fusion. Lower denominators and lower sums produce higher HIM values — signaling simpler, more consonant ratios.

\textbf{Example:}

\begin{itemize}
    \item 3:2 (perfect fifth): HIM = $4/6 = 0.667$
    \item 7:4 (septimal minor 7th): HIM = $10/28 \approx 0.357$
\end{itemize}

\subsection*{4. Prime-Exponent Vector Averaging}

For advanced systems supporting symbolic pitch spaces (e.g., Just Intonation), each pitch is expressed as a vector:

\[
\vec{v}_i = (x_2, x_3, x_5, x_7, \ldots) \quad \text{where } f_i = 2^{x_2} \cdot 3^{x_3} \cdot 5^{x_5} \cdots
\]

The mean vector:

\[
\vec{v}_{PR} = \frac{1}{N} \sum_{i=1}^N \vec{v}_i
\]

...is mapped back to a rational frequency. This method allows geometric averaging of complex harmonic ratios and facilitates field-aware root finding.

\subsection*{5. Time-Aware PR Estimation}

Crucially, PR is not computed frame-by-frame but across time-stable pitch sequences. This enables it to:

\begin{itemize}
    \item Capture phrasing-based tonal centers
    \item Filter out transient modulations
    \item Model tonality as a temporally weighted attractor
\end{itemize}

\textbf{Segments are defined using CentTracker:}

New group starts when $f_0$ deviates $> \pm49$ cents from previous.

\textbf{Result:} [G2] $\rightarrow$ [D3] $\rightarrow$ [G3] $\rightarrow$ matched to [1,2,3] = PR: G1

\subsection*{6. Output Summary}

Each PR record includes:

\begin{itemize}
    \item \texttt{time\_range:} [start, end] of stable group
    \item \texttt{phantom\_root:} root frequency in Hz
    \item \texttt{symbol:} user-defined microtonal pitch label (e.g., D3⁺²)
    \item \texttt{group:} matching harmonic template label (e.g., A2)
    \item \texttt{fusion\_score:} optional metric from HIM or harmonic count
\end{itemize}

\section*{III.2 – Resonance Potential Formalism (Φ)}

The Resonance Potential ($\Phi$) is the fundamental scalar measure of harmonic coherence within the R³ framework. It captures how energetically close — and thus perceptually “fused” — a group of partials are at a given moment. Unlike symbolic harmonic functions (e.g., tonic, dominant), $\Phi$ offers a mathematically continuous, spectrally grounded, and amplitude-sensitive metric for tonal tightness.

\subsection*{1. Core Equation}

The main formulation of $\Phi$ is defined over all pairwise combinations of partials within a time slice:

\[
\Phi(t) = \sum_{i<j} \frac{A_i(t) \cdot A_j(t)}{|f_i(t) - f_j(t)| + \epsilon}
\]

Where:

\begin{itemize}
    \item $A_i(t), A_j(t)$: amplitudes of partials at time $t$
    \item $f_i(t), f_j(t)$: their frequencies
    \item $\epsilon$: a small regularization constant (e.g., $1e^{-6}$) to prevent division by zero
\end{itemize}

This equation models:

\begin{itemize}
    \item Higher amplitude $\Rightarrow$ greater resonance contribution
    \item Closer frequencies $\Rightarrow$ stronger spectral fusion
    \item Denser clusters $\Rightarrow$ higher perceptual cohesion
\end{itemize}

\subsection*{2. Perceptual Foundations}

$\Phi$ generalizes earlier psychoacoustic roughness models (e.g., Plomp \& Levelt, 1965), replacing frequency ratios with physical frequencies and amplitude scaling.

It corresponds to perceptual phenomena such as:

\begin{itemize}
    \item Tonal fusion
    \item Consonance gradience
    \item Spectral “weight” of a sound structure
\end{itemize}

EEG studies (Bidelman et al., 2011) suggest that high harmonic coherence triggers stronger FFR synchrony — supporting $\Phi$ as a proxy for perceived resonance strength.

\subsection*{3. Time-Windowed Integration ($\Phi_T$)}

To move from instantaneous coherence to temporal resonance modeling, $\Phi$ is accumulated across time windows:

\[
\Phi_T = \sum_{t=t_0}^{t_1} \Phi(t)
\]

Where:

\begin{itemize}
    \item $T$ = time window (e.g., 1s, 3s, 5s, 7s)
    \item Frames sampled at 0.1s $\Rightarrow$ $\Phi_T$ includes 10–70 values
\end{itemize}

This windowed $\Phi_T$ models:

\begin{itemize}
    \item Tonal momentum (field pressure)
    \item Stability regions (high and flat $\Phi_T$)
    \item Modulation zones ($\Phi_T$ dips or spikes)
\end{itemize}

\subsection*{4. Information-Theoretic Interpretation}

$\Phi$ can be viewed as an inverse spectral entropy measure.

\begin{itemize}
    \item A spectrum with many equally spaced partials $\Rightarrow$ lower $\Phi$
    \item A tightly clustered, loud spectrum $\Rightarrow$ higher $\Phi$
\end{itemize}

$\Phi$ is therefore analogous to a negative KL divergence between energy distributions.

This link allows R³ to potentially connect with probabilistic models of expectation and surprise (e.g., Huron’s \textit{Sweet Anticipation}, 2006).

\subsection*{5. Spectral Weighting Options}

\paragraph{a. Harmonic Rank Weighting}

Later harmonics receive less weight:

\[
A_i^* = \frac{A_i}{1 + h_i}
\]

\paragraph{b. Gaussian Spectral Masking}

Include field density around a pitch center (e.g., PR):

\[
\Phi_{\text{centered}}(t) = \sum_{i<j} \frac{A_i \cdot A_j \cdot e^{-\frac{(f_i - \mu)^2}{2\sigma^2}}}{|f_i - f_j| + \epsilon}
\]

Where $\mu$ is the perceptual center of gravity.

\subsection*{6. Output Summary}

\paragraph{Framewise Output:}

\begin{verbatim}
{ "time": 3.2, "phi": 2.831 }
\end{verbatim}

\paragraph{Windowed Output:}

\begin{verbatim}
{ "window": "5.0–8.0", "phi": 9.183, "window_size": 3 }
\end{verbatim}

Each frame or window can be directly plotted as a $\Phi(t)$ curve or used for comparative analysis.

\subsection*{7. System Integration}

\begin{itemize}
    \item \textbf{Feeds CRVU:}
    \begin{itemize}
        \item TPS = $\sigma(\Phi)$
        \item NSF = weighted $\Phi$ sum
    \end{itemize}
    \item \textbf{Feeds RFMU:} Used to generate field intensity
    \item \textbf{Feeds C³:} As raw resonance potential data for affective modeling
\end{itemize}

\section*{III.3 – Resonance Field Mapping (RFM)}

The RFM function generates a scalar field over the frequency domain at each time point, representing the density and distribution of harmonic energy. It transforms a list of discrete partials into a smooth, continuous resonance map — providing a foundation for spatially-aware tonal reasoning.

Whereas $\Phi$ quantifies total harmonic coherence within a frame, RFM visualizes how that resonance is distributed across the pitch spectrum — forming a field of tonal gravity.

\subsection*{1. Core Equation}

The resonance field at time $t$, over frequency coordinate $f$, is computed as:

\[
\text{RFM}(f, t) = \sum_{i=1}^N A_i(t) \cdot e^{-\frac{(f - f_i(t))^2}{2\sigma^2}}
\]

Where:

\begin{itemize}
    \item $f$: continuous frequency grid point
    \item $f_i(t)$: frequency of $i$-th partial at time $t$
    \item $A_i(t)$: amplitude of $i$-th partial
    \item $\sigma$: spread parameter (resonance width)
\end{itemize}

This is a Gaussian kernel density estimator, where each partial casts a resonance hill over frequency space.

\subsection*{2. Grid Design}

To build RFM numerically, a discrete frequency grid is defined:

\[
F = \{ f_1, f_2, ..., f_n \} = \text{logspace}(20, 20000, N)
\]

\begin{itemize}
    \item $N = 512$ or $1024$ (typical values)
    \item Logarithmic scaling reflects cochlear frequency mapping
\end{itemize}

Each grid point will hold one $\text{RFM}(f, t)$ value.\\
Result: a 2D matrix where each row = time slice, each column = frequency bin.

\subsection*{3. Perceptual Interpretation}

RFM approximates the perceptual landscape of sound:

\begin{itemize}
    \item \textbf{Peaks} = tonal centers or attractors
    \item \textbf{Valleys} = spectral gaps or anti-resonance zones
    \item \textbf{Slope} = tonal pull
    \item \textbf{Width} = harmonic spread
\end{itemize}

Musically, RFM enables analysis of:

\begin{itemize}
    \item Tonal convergence and divergence
    \item Modulation zones (shifting attractors)
    \item Multi-center textures (polytonality)
    \item Voice-leading through field movement
\end{itemize}

\subsection*{4. Gradient Operator ($\nabla$RFM)}

To extract perceptual “direction,” RFM computes its gradient:

\[
\nabla \text{RFM}(f, t) = \frac{\partial \text{RFM}(f, t)}{\partial f}
\]

\begin{itemize}
    \item This is discretized via finite differences on the grid
    \item High $\nabla$RFM $\rightarrow$ rapid spectral change $\rightarrow$ dissonance, instability
    \item Low $\nabla$RFM $\rightarrow$ smooth flow $\rightarrow$ stability, fusion
\end{itemize}

Gradient magnitude is used in CRVU’s TFI metric.

\subsection*{5. Peak Tracking \& Field Topology}

Local maxima in RFM indicate momentary tonal centers.

Let:

\[
f_p(t) \in F \quad \text{where } \nabla \text{RFM} = 0, \quad \text{curvature} < 0
\]

Tracking $f_p(t)$ over time forms a tonal trajectory or attractor path.

\begin{itemize}
    \item Field segmentation methods (e.g., watershed or ridge detection) can be applied for higher-level grouping
\end{itemize}

\subsection*{6. Visualization Mapping}

\begin{itemize}
    \item X-axis = time (0–20s)
    \item Y-axis = log frequency (20–20kHz)
    \item Color = field intensity (resonance strength)
    \item Overlay = vector arrows from $\nabla$RFM or contour lines for attractors
\end{itemize}

This map becomes the visual body of tonal behavior over time.

\subsection*{7. Output Format}

\begin{verbatim}
{
  "time": 3.7,
  "grid": [20.0, 24.3, ..., 20000.0],
  "field": [0.001, 0.005, ..., 0.0],
  "gradient": [0.002, -0.001, ..., -0.003]
}
\end{verbatim}

Each frame produces a scalar field vector and optional gradient vector.

\subsection*{8. Theoretical Parallels}

RFM draws from:

\begin{itemize}
    \item Spectrogram theory: smoothed representation of energy over time/frequency
    \item Field theory (physics): scalar potential fields
    \item Tonnetz spaces: extended to real-valued, log-frequency domains
    \item Auditory cortex modeling: tonotopic fields + lateral inhibition
\end{itemize}

\subsection*{9. Applications}

\begin{itemize}
    \item CRVU $\rightarrow$ TFI: average $\nabla$RFM magnitude
    \item Modulation analysis: movement of peaks
    \item Polycentricity: multi-peak stability across frames
    \item VR/Unity visual grounding: resonance fields as terrain surfaces
\end{itemize}

\section*{III.4 – Cognitive Resonance Metrics (TPS, TFI, NSF)}

Cognitive perception of harmony is not based on static symbols, but on dynamic acoustic behavior: how stable, unified, and memorable a sound feels over time. The CRVU summarizes this behavior through three scalar metrics: Temporal Perceptual Stability (TPS), Tonal Fusion Index (TFI), and Neural Synchronization Field (NSF).

These metrics operate as projections of resonance into perceptual space — each compressing a dimension of resonance behavior into a scalar value $\in [0, 1]$.

\subsection*{1. Temporal Perceptual Stability (TPS)}

\textbf{Equation:}
\[
\text{TPS} = \frac{1}{1 + \sigma_\Phi(t)}
\]

Where:

\begin{itemize}
    \item $\Phi(t)$: framewise resonance potential
    \item $\sigma_\Phi(t)$: standard deviation across full or local time window
\end{itemize}

\textbf{Interpretation:}

\begin{itemize}
    \item High TPS $\Rightarrow$ consistent $\Phi$ $\Rightarrow$ stable resonance center
    \item Low TPS $\Rightarrow$ fluctuating $\Phi$ $\Rightarrow$ modulation, instability
\end{itemize}

\textbf{Perceptual Basis:} Temporal regularity correlates with attentional focus and pitch certainty (cf. Leman, 2016). TPS models tonal anchoring as experienced in both classical and non-tonal contexts.

\subsection*{2. Tonal Fusion Index (TFI)}

\textbf{Equation:}
\[
\text{TFI} = \frac{1}{1 + \langle |\nabla \text{RFM}(f, t)| \rangle}
\]

Where:

\begin{itemize}
    \item $\nabla \text{RFM}(f, t)$: gradient of the resonance field at time $t$
    \item $\langle \cdot \rangle$: average over frequency domain and time frames
\end{itemize}

\textbf{Interpretation:}

\begin{itemize}
    \item High TFI $\Rightarrow$ smooth field $\Rightarrow$ tight spectral coherence
    \item Low TFI $\Rightarrow$ jagged field $\Rightarrow$ spectral diffusion
\end{itemize}

\textbf{Neural Correlates:} Auditory cortex entrains more strongly to spectrally fused sounds. Fusion models correlate with gamma coherence, phase-locking, and sound object formation (cf. Bidelman et al., 2014).

\subsection*{3. Neural Synchronization Field (NSF)}

\textbf{Equation:}
\[
\text{NSF} = \sum_t \Phi(t) \cdot e^{-\alpha t}
\]

Where:

\begin{itemize}
    \item $\alpha$: decay coefficient (0.05–0.1 typical)
\end{itemize}

\textbf{Interpretation:}

\begin{itemize}
    \item High NSF $\Rightarrow$ strong early resonance $\Rightarrow$ likely encoding into short-term memory
    \item Low NSF $\Rightarrow$ delayed or inconsistent resonance $\Rightarrow$ weaker impression
\end{itemize}

\textbf{Psychological Basis:} NSF captures the recency effect of musical perception — the brain’s tendency to weight earlier salient events more heavily in expectation and evaluation processes (cf. Zatorre \& Salimpoor, 2013).

\subsection*{4. Combined Cognitive Vector}

The full cognitive resonance signature is a vector:

\[
\vec{\text{CRV}} = [\text{TPS}, \text{TFI}, \text{NSF}]
\]

Each value is:

\begin{itemize}
    \item Normalized $\in [0, 1]$
    \item Interpretable individually
    \item Composable into weighted salience models
\end{itemize}

\subsection*{5. Applications}

\begin{itemize}
    \item \textbf{C³ input:} feeds attention allocation, memory modeling, immersion scores
    \item \textbf{Real-time resonance diagnosis}
    \item \textbf{Musical segmentation:} changes in CRV may mark structural transitions
    \item \textbf{Generative AI:} use CRV to guide harmonic generation toward cognitive targets
\end{itemize}

\subsection*{6. Output Summary}

\paragraph{JSON format:}
\begin{verbatim}
{
  "TPS": 0.842,
  "TFI": 0.713,
  "NSF": 0.0362
}
\end{verbatim}

Visualized as three stacked bars (R/G/B), overlaid on waveform or resonance map.

\subsubsection*{Summary}

CRV is the cognitive endpoint of R³: a compact, interpretable summary of how a given sound structure will likely be experienced, memorized, and evaluated by a human listener.

\section*{IV – Data Structure and Output (Enhanced)}

The structural integrity of R³ depends not only on its theoretical formulations, but also on the consistency, extensibility, and interpretability of its data output formats. All R³ units generate machine-readable, human-interpretable, and visualization-ready files. These files follow a strict temporal alignment and a modular format architecture.

\subsection*{IV.1 – Frame Resolution and Time Structure}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Total Duration & 20.0 seconds \\
Frame Rate & 0.1 seconds \\
Total Frames & 200 \\
Fundamental + Harmonics & 1 + 16 \\
\hline
\end{tabular}
\end{center}

Each frame is timestamped and encapsulates a complete harmonic snapshot.

\paragraph{Example: \texttt{RawSpectrum-unit.json}}

\begin{verbatim}
{
  "time": 3.2,
  "partials": [
    {
      "freq": 261.63,
      "amplitude": 0.81,
      "isFundamental": true,
      "harmonic_index": 0,
      "symbol": "C4⁰"
    },
    {
      "freq": 523.25,
      "amplitude": 0.42,
      "harmonic_index": 1,
      "isFundamental": false,
      "symbol": "C5⁰"
    }
  ]
}
\end{verbatim}

This format is unit-agnostic and powers all R³ modules.

\subsection*{IV.2 – Unit-Specific JSON Output Formats}

\paragraph{1. PRU – Phantom Root}
\begin{verbatim}
{
  "time_range": [2.1, 4.3],
  "phantom_root": 130.81,
  "symbol": "C3⁺¹",
  "group": "A2",
  "fusion_score": 0.42
}
\end{verbatim}

One record per detected PR segment, with group-matched harmonic stack label and symbolic pitch.

\paragraph{2. RPU – Resonance Potential}

Framewise:
\begin{verbatim}
{ "time": 5.2, "phi": 3.714 }
\end{verbatim}

Windowed:
\begin{verbatim}
{ "window": "5.0–8.0", "phi": 9.23, "window_size": 3 }
\end{verbatim}

Both datasets can be plotted as continuous $\Phi$ curves, with optional variance indicators.

\paragraph{3. RFMU – Resonance Field}
\begin{verbatim}
{
  "time": 7.1,
  "grid": [20.0, 24.1, ..., 20000.0],
  "field": [0.0012, 0.0044, ..., 0.0],
  "gradient": [0.0004, -0.0003, ..., -0.0021]
}
\end{verbatim}

\begin{itemize}
    \item \texttt{field} = scalar intensity at each frequency point
    \item \texttt{gradient} = $\nabla$RFM used for TFI
\end{itemize}

\paragraph{4. CRVU – Cognitive Resonance Vector}
\begin{verbatim}
{
  "TPS": 0.843,
  "TFI": 0.702,
  "NSF": 0.0361
}
\end{verbatim}

Single vector summarizing entire input segment’s resonance dynamics.

\subsection*{IV.3 – Output File Naming Conventions}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Type} & \textbf{Pattern} \\
\hline
Raw input & \texttt{RawSpectrum-unit.json} \\
PR segment & \texttt{PR-unit-temporal.json} \\
RP framewise & \texttt{RP-framewise.json} \\
RP windowed & \texttt{RP-windowed.json} \\
Field maps & \texttt{RFM-unit.json} \\
Cognitive & \texttt{CRV-unit.json} \\
\hline
\end{tabular}
\end{center}

All files are written to \texttt{../data/output/<unit>/} subdirectories, with script-driven generation.

\subsection*{IV.4 – Visual Outputs}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Unit} & \textbf{File} & \textbf{Format} & \textbf{Dimensions} \\
\hline
PRU & \texttt{PR-unit.png} & PNG & 3840 × 216 \\
RPU & \texttt{RP-unit.png} & PNG & 3840 × 216 \\
RFMU & \texttt{RFM-unit.png} & PNG & 3840 × 216 \\
CRVU & \texttt{CRV-unit.png} & PNG & 3840 × 216 \\
Master & \texttt{R3-overlay.html} & HTML & 3840 × 2160 \\
\hline
\end{tabular}
\end{center}

Visuals use:

\begin{itemize}
    \item Plotly for HTML interactive
    \item Matplotlib for static export
    \item Log-scale y-axis for frequency mapping
    \item Dark mode with frequency-hue colorization
\end{itemize}

\subsection*{IV.5 – Unity-Compatible CSV Format}

\begin{verbatim}
time,freq,amplitude,isFundamental,harmonic_index,symbol
\end{verbatim}

\textbf{Used in:}

\begin{itemize}
    \item \texttt{CSVLoader.cs} to populate \texttt{List<Partial>}
    \item \texttt{SpectrumVisualizer.cs} to map x (time), y (log freq), size (amplitude)
    \item Visualized in 3D scene using prefabs, color shaders, and optional PR overlays
\end{itemize}

\subsection*{IV.6 – Extension Paths}

\begin{itemize}
    \item OL-unit output (locking events)
    \item Symbolic export to MusicXML (planned)
    \item Annotated resonance flows for interactive learning
\end{itemize}

\section*{V — Pipeline Execution and Automation (Enhanced)}

The R³ module is implemented as a fully modular, automatable pipeline. Its entire analytical process — from raw spectral input to visual output and Unity export — can be executed via a single orchestration script. This design ensures reproducibility, clarity, and efficient development.

\subsection*{V.1 — Execution Logic}

All scripts in R³ are written in Python and follow a unit-modular standard:

\begin{itemize}
    \item Each unit has:
    \begin{itemize}
        \item One analysis script $\rightarrow$ produces \texttt{.json}
        \item One visualization script $\rightarrow$ produces \texttt{.png}
    \end{itemize}
    \item All units share a common input file: \texttt{RawSpectrum-unit.json}
\end{itemize}

This architecture supports both:

\begin{itemize}
    \item Independent execution (for testing/debugging)
    \item Sequential batch runs (via automation script)
\end{itemize}

\subsection*{V.2 — Master Script: \texttt{run\_R3\_pipeline.py}}

This script executes the full R³ pipeline in order:

\paragraph{Order of Execution:}
\begin{verbatim}
[
  "PR_unit_temporal.py",
  "RP_unit_combined.py",
  "RFM_unit_analysis.py",
  "CRV_unit_analysis.py",
  "visualize_PR_temporal.py",
  "visualize_RP_unit.py",
  "visualize_RFM_unit.py",
  "visualize_CRV_unit.py",
  "visualize_overlay_all.py"
]
\end{verbatim}

Each entry is executed via:

\begin{verbatim}
subprocess.run(["python", script], check=True)
\end{verbatim}

If any step fails, the pipeline halts — ensuring fail-fast validation.

\subsection*{V.3 — Execution Environment}

\textbf{Recommended setup:}

\begin{itemize}
    \item Python 3.9+
    \item Libraries: \texttt{numpy}, \texttt{matplotlib}, \texttt{plotly}, \texttt{json}, \texttt{csv}, \texttt{subprocess}, \texttt{os}
    \item Virtual environment: \texttt{s3r3\_env}
    \item Scripts are path-relative and designed for cross-platform compatibility (macOS, Linux, Windows)
\end{itemize}

\subsection*{V.4 — Unit Interdependencies}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit} & \textbf{Requires} & \textbf{Produces} \\
\hline
PRU & RawSpectrum & PR-unit-temporal.json \\
RPU & RawSpectrum & RP-framewise, RP-window \\
RFMU & RawSpectrum & RFM-unit.json \\
CRVU & RPU + RFMU outputs & CRV-unit.json \\
\hline
\end{tabular}
\end{center}

All outputs are time-aligned (0.1s resolution) and normalized where needed.

\subsection*{V.5 — Directory Layout}

\paragraph{Scripts:}
\begin{verbatim}
/scripts/
├── PR_unit_temporal.py
├── RP_unit_combined.py
├── RFM_unit_analysis.py
├── CRV_unit_analysis.py
├── visualize_*.py
└── run_R3_pipeline.py
\end{verbatim}

\paragraph{Data:}
\begin{verbatim}
/data/
└── output/
    ├── PR/
    ├── RP/
    ├── RFM/
    ├── CRV/
    └── raw/RawSpectrum-unit.json
\end{verbatim}

\paragraph{Output:}
\begin{verbatim}
/output/
├── *.png
└── R3-overlay.html
\end{verbatim}

\subsection*{V.6 — Execution Time}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit} & \textbf{Avg Analysis Time (200 frames)} & \textbf{Visualization Time} \\
\hline
PRU & $\sim$2 seconds & $\sim$1.5 seconds \\
RPU & $\sim$4 seconds & $\sim$2 seconds \\
RFMU & $\sim$5 seconds & $\sim$3 seconds \\
CRVU & $\sim$1 second & $\sim$1 second \\
Overlay & — & $\sim$3–5 seconds \\
\hline
\end{tabular}
\end{center}

\textbf{Total pipeline runtime:} $<$ 20 seconds

\subsection*{V.7 — Logging and Debugging}

Each script includes:

\begin{itemize}
    \item \texttt{print("[UNIT] Starting...")}
    \item \texttt{print("[UNIT] Finished. Output saved to: ...")}
    \item \texttt{try/except} wrappers with error logging
    \item Optional: logging to file (\texttt{log.txt}), runtime profiling, unit test suites (planned)
\end{itemize}

\subsection*{V.8 — Real-Time Pipeline (Future)}

\textbf{Goals:}

\begin{itemize}
    \item Hook into live CREPE output stream
    \item Frame-by-frame analysis and accumulation
    \item Unity/VR feedback loop using CRV in real-time
\end{itemize}

This will require conversion of R³ modules to stream-safe, low-latency processes (e.g., via NumPy Live, C++, or Python async/generator pattern).

\section*{VI — Visualization System (Enhanced)}

Visualization is a core dimension of R³’s design philosophy. It is not merely a presentation layer, but a cognitive interface — converting dense spectral data into visually interpretable resonance structures. Each R³ unit contributes a semantically encoded visual layer aligned across a global time-frequency plane.

The goal is to present harmony not as static notation, but as a dynamic topology of vibratory interaction.

\subsection*{VI.1 — Design Principles}

\paragraph{A. Shared Temporal Base}

\begin{itemize}
    \item All plots align to a common x-axis (0–20s)
    \item Frame resolution = 0.1s
    \item Windowed overlays align precisely with frame start times
\end{itemize}

\paragraph{B. Semantic Visual Encoding}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Visual Mapping} \\
\hline
Frequency & Y-axis (log scale) \\
Amplitude & Marker size / line thickness \\
$\Phi$ & Y-axis (RPU layer) \\
Partial role & Color (e.g., fundamental = red) \\
Field strength & Color density (RFMU) \\
CRV metrics & R/G/B bar mapping \\
\hline
\end{tabular}
\end{center}

\paragraph{C. Vertical Modularity}

\begin{itemize}
    \item Each unit occupies 216px vertical space
    \item RawSpectrum layer = 1080px (reference base)
    \item Total image = 3840 × 2160 (4K full overlay)
\end{itemize}

\subsection*{VI.2 — Unit Layer Visuals}

\paragraph{1. RawSpectrum (S³)}

\begin{itemize}
    \item \textbf{Markers:} square or circle
    \item \textbf{Color:} frequency class (HSV or pitch-mapped palette)
    \item \textbf{Size:} amplitude
    \item \textbf{Opacity:} harmonic index scaled
    \item \textbf{Y-axis:} log(frequency)
    \item \textbf{Z (optional):} time slice index for animation or Unity rendering
    \item \textbf{Renderer:} Plotly’s \texttt{Scattergl()} for high-speed rendering
\end{itemize}

\paragraph{2. Phantom Root (PRU)}

\begin{itemize}
    \item \textbf{Form:} red horizontal bars
    \item \textbf{Y-position:} PR frequency
    \item \textbf{Label:} symbolic pitch (e.g., C3⁺²)
    \item \textbf{Time span:} width of perceptual root duration
    \item \textbf{Group code:} can be color-coded (A, A1, A2, …)
\end{itemize}

\paragraph{3. Resonance Potential (RPU)}

\begin{itemize}
    \item \textbf{Framewise $\Phi$:} thin gray line (baseline)
    \item \textbf{Windowed $\Phi$:} colored overlays:
    \begin{itemize}
        \item 1s = red
        \item 3s = orange
        \item 5s = green
        \item 7s = blue
    \end{itemize}
    \item \textbf{Y-axis:} $\Phi$ value (scalar resonance density)
    \item \textbf{X-axis:} time
\end{itemize}

\paragraph{4. Resonance Field (RFMU)}

\begin{itemize}
    \item \textbf{Form:} 2D heatmap
    \item \textbf{x =} time
    \item \textbf{y =} frequency (log)
    \item \textbf{color =} RFM$(f, t)$ field strength (e.g., inferno, magma)
    \item \textbf{Optional overlays:}
    \begin{itemize}
        \item white peak paths
        \item vector arrows ($\nabla$RFM)
        \item field contour lines
    \end{itemize}
\end{itemize}

\paragraph{5. Cognitive Vector (CRVU)}

\begin{itemize}
    \item \textbf{Form:} stacked bars
    \item \textbf{Red =} TPS
    \item \textbf{Green =} TFI
    \item \textbf{Blue =} NSF
    \item \textbf{Labels:} numeric values (0.000–1.000)
    \item \textbf{Y-axis:} not used (bar only)
\end{itemize}

This layer acts as the summary strip, linking resonance data to perceptual metrics.

\subsection*{VI.3 — Master Overlay Composition}

Final full overlay is generated using \texttt{visualize\_overlay\_all.py}. It combines:

\begin{itemize}
    \item 5 unit layers (216 px each)
    \item 1 RawSpectrum base layer (1080 px)
    \item Common time axis
    \item Global dark mode for color clarity
    \item \textbf{HTML output:} interactive, 4K resolution
\end{itemize}

\subsection*{VI.4 — Static Exports}

Each unit also produces a \texttt{.png} file:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit} & \textbf{File Name} & \textbf{Resolution} \\
\hline
PRU & PR-unit.png & 3840 × 216 px \\
RPU & RP-unit.png & 3840 × 216 px \\
RFMU & RFM-unit.png & 3840 × 216 px \\
CRVU & CRV-unit.png & 3840 × 216 px \\
Master & R3-overlay.html & 3840 × 2160 px \\
\hline
\end{tabular}
\end{center}

These exports allow both modular inspection and publication-level usage.

\subsection*{VI.5 — Unity Integration}

Visual layers are linked to Unity via:

\begin{itemize}
    \item Prefab scaling (amplitude)
    \item Z-depth encoding (harmonic index)
    \item Dynamic camera tracking of PRU or RFM peaks
    \item CRV bar overlays as HUDs in 3D scenes
\end{itemize}

\textbf{Optional enhancements:}

\begin{itemize}
    \item PR trail = \texttt{LineRenderer} path
    \item RFM = surface terrain with $\Phi$-based displacement
    \item CRV = color modulation of environment
\end{itemize}

\subsection*{VI.6 — Aesthetic Philosophy}

R³ visual outputs aim to:

\begin{itemize}
    \item Replace static notation with spectral cartography
    \item Encode mathematical depth in intuitive visuals
    \item Make resonance not only computable — but seeable
\end{itemize}

\section*{VII — Unity Integration (Enhanced)}

The Unity integration of R³ transforms resonance data from abstract mathematical structures into a spatial, interactive, and visual environment. This enables researchers, musicians, and users to walk through, see, and interact with spectral and harmonic structures — making resonance literally visible.

Unity is used not just as a renderer, but as a cognitive translation platform: it visualizes how frequencies resonate, how roots shift, how fields flow — in real time or through immersive playback.

\subsection*{VII.1 — Export Path: JSON $\rightarrow$ CSV}

Although R³ internally uses \texttt{.json} for maximum flexibility, Unity consumes data as \texttt{.csv} via its lightweight, line-based loading mechanisms.

\textbf{Source:} \texttt{RawSpectrum-unit.json} $\rightarrow$ converted to:

\textbf{CSV Format:} \texttt{RawSpectrum01.csv}

\begin{verbatim}
time,freq,amplitude,isFundamental,harmonic_index,symbol
1.1,196.0,0.82,True,0,G3⁰
1.1,392.0,0.42,False,1,G4⁰
...
\end{verbatim}

Each line represents one partial (including harmonics), with symbolic encoding for microtonal interpretation.

\subsection*{VII.2 — Unity C\# Class Structure}

\paragraph{1. Partial.cs}

\begin{verbatim}
[System.Serializable]
public class Partial {
    public float time;
    public float freq;
    public float amplitude;
    public bool isFundamental;
    public int harmonic_index;
    public string symbol;
}
\end{verbatim}

\paragraph{2. CSVLoader.cs}

\begin{verbatim}
public List<Partial> partials;

void LoadCSV(TextAsset file) {
    string[] lines = file.text.Split('\n');
    for (int i = 1; i < lines.Length; i++) {
        string[] values = lines[i].Split(',');
        Partial p = new Partial();
        p.time = float.Parse(values[0]);
        p.freq = float.Parse(values[1]);
        p.amplitude = float.Parse(values[2]);
        p.isFundamental = values[3] == "True";
        p.harmonic_index = int.Parse(values[4]);
        p.symbol = values[5];
        partials.Add(p);
    }
}
\end{verbatim}

\subsection*{VII.3 — Scene Mapping}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Dimension} & \textbf{Mapped To} \\
\hline
X & time (horizontal progression) \\
Y & log(freq) (vertical placement) \\
Z & harmonic index (depth, optional) \\
Size & amplitude (object scale) \\
Color & frequency (HSV hue-based mapping) \\
\hline
\end{tabular}
\end{center}

Each partial = a colored glowing sphere.

\begin{itemize}
    \item Stronger harmonics = larger/brighter objects
    \item Fundamental = red core; others vary by frequency
\end{itemize}

\subsection*{VII.4 — Object Structure and Prefabs}

\textbf{Prefab:} \texttt{PointPrefab} (Sphere with Unlit Shader)

\textbf{Renderer:}

\begin{itemize}
    \item Emission Color = mapped hue
    \item Scale = amplitude × scalar
    \item Tag = Fundamental / Harmonic
\end{itemize}

Optional shader features:

\begin{itemize}
    \item Pulse = temporal dynamics
    \item Glow = amplitude modulation
    \item Flicker = instability (if $\Phi$ is low)
\end{itemize}

\subsection*{VII.5 — Temporal Animation}

\begin{itemize}
    \item Unity’s \texttt{Time.time} aligns playback with partial spawning
    \item Optional: timeline scrubber
    \item Scene camera can track:
    \begin{itemize}
        \item PR path (via \texttt{LineRenderer})
        \item Field peak in RFM (via surface mesh)
    \end{itemize}
\end{itemize}

\subsection*{VII.6 — Extended Visualizations}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Data} & \textbf{Visual Form} & \textbf{Mechanism} \\
\hline
PRU & Red line sweep & LineRenderer along PR freq \\
RPU & Height curve & Dynamic plot ($\Phi$ over time) \\
RFMU & Mesh surface & Terrain object from \texttt{field[]} \\
CRVU & HUD bar graph & UI Panel with TPS, TFI, NSF \\
\hline
\end{tabular}
\end{center}

Additional interaction options:

\begin{itemize}
    \item Filter by group (A2, A3, ...)
    \item Highlight tonal drift zones
    \item Switch between symbolic and spectral views
\end{itemize}

\subsection*{VII.7 — Sound Integration}

\begin{itemize}
    \item Link Unity’s \texttt{AudioSource.time} to visual spawning
    \item Synchronize resonance events with real sound
    \item Use amplitude thresholding to trim non-audible points
    \item Optional: real-time $\Phi$ modulator $\rightarrow$ dynamically warp terrain or brightness
\end{itemize}

\subsection*{VII.8 — Performance Optimization}

\begin{itemize}
    \item Object pooling (for partials)
    \item Async CSV loading (for large datasets)
    \item GPU instancing for visual particles
    \item Log-space Y-axis prevents vertical crowding
\end{itemize}

\subsection*{VII.9 — Cognitive Immersion Use Case}

The Unity implementation allows users to:

\begin{itemize}
    \item Step through harmonic space
    \item See tonality emerge and dissolve
    \item Hear resonance while seeing its structure
    \item Manipulate partials and watch CRV change live
\end{itemize}

Use cases include:

\begin{itemize}
    \item Education (teaching tonal centers)
    \item VR concert staging
    \item Interactive composition
    \item Research on tonotopic attention in motion
\end{itemize}

\section*{VIII — Cursor Architecture Placement (Enhanced)}

The Cursor AI platform serves as the interactive, explorable knowledge interface of the SRC⁹ system. All R³ content — scientific explanations, equations, visualizations, and output samples — are embedded within Cursor's domain–unit–node hierarchy, providing a seamless gateway between theory, data, and cognitive navigation.

\subsection*{VIII.1 — Domain-Level Placement}

R³ exists as a dedicated modular domain within the Cursor site structure:

\begin{itemize}
    \item \textbf{Path:} \texttt{/modules/r3}
    \item \textbf{Title:} Resonance-Based Relational Reasoning
    \item \textbf{Function:} Gateway page introducing the theory, architecture, and units of R³
\end{itemize}

\textbf{Domain Page Contents:}

\begin{itemize}
    \item Scientific overview
    \item Mathematical core ($\Phi$, PR, RFM, CRV equations)
    \item S³ $\rightarrow$ R³ $\rightarrow$ C³ flowchart
    \item Unit summary table
    \item Domain toggle menu (vs. Traditional-Based Theory)
\end{itemize}

Users can explore individual units by clicking cards linking to their respective pages.

\subsection*{VIII.2 — Unit Page Design}

Each R³ unit (PRU, RPU, RFMU, CRVU) has a standalone interactive document:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit Code} & \textbf{Path} & \textbf{Title} \\
\hline
PRU & \texttt{/modules/r3/pr} & Phantom Root Unit \\
RPU & \texttt{/modules/r3/rp} & Resonance Potential Unit ($\Phi$) \\
RFMU & \texttt{/modules/r3/rfm} & Resonance Field Modeling Unit \\
CRVU & \texttt{/modules/r3/crv} & Cognitive Resonance Vectoring \\
\hline
\end{tabular}
\end{center}

Each unit page includes:

\begin{itemize}
    \item Scientific Function
    \item Mathematical Foundation (LaTeX supported)
    \item Node Architecture Table
    \item Sample Output (JSON snippet)
    \item Visualization Preview (216px PNG)
    \item Integration pathways (to C³ or back to S³)
\end{itemize}

\subsection*{VIII.3 — Node-Level Embedding}

Each unit page has expandable \texttt{<details>} components for its node definitions.

\textbf{Example in PRU:}

\begin{verbatim}
<details><summary>GroupMatcher</summary>
Matches sequences of stable f₀ segments to harmonic template stacks like [1,2,3] or [1–11].
</details>
\end{verbatim}

This allows deep structure without visual clutter.

Nodes are cross-linkable and potentially host their own \texttt{/nodes/<id>} pages in future iterations.

\subsection*{VIII.4 — Visualization Integration}

Every unit’s visualization is embedded via:

\begin{itemize}
    \item Inline PNG
    \item Collapsible \texttt{<details>} blocks
    \item Optional Plotly iframe (HTML interactive graphs)
\end{itemize}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer} & \textbf{Visual Type} & \textbf{Location} \\
\hline
PRU & bar + label plot & \texttt{PR-unit.png} \\
RPU & $\Phi$ overlay curves & \texttt{RP-unit.png} \\
RFMU & Heatmap grid & \texttt{RFM-unit.png} \\
CRVU & RGB bars & \texttt{CRV-unit.png} \\
\hline
\end{tabular}
\end{center}

The full overlay (\texttt{R3-overlay.html}) may be shown in a dedicated interactive gallery.

\subsection*{VIII.5 — Intermodule Linking}

Each unit page includes a reference sidebar linking:

\begin{itemize}
    \item \textbf{S³ input:} \texttt{RawSpectrum}
    \item \textbf{R³ peers:} e.g., PRU links to RPU
    \item \textbf{C³ outputs:} CRVU $\rightarrow$ CTU, NSU, PIU
\end{itemize}

Additionally, source references are hyperlinked inline (e.g., Zatorre et al., 2013).

\subsection*{VIII.6 — Domain Switch Architecture}

The dual-domain system is shown via a toggle interface:

\begin{verbatim}
[ �� R³ Resonance Theory ] [ ⚪ Traditional Theory ]
\end{verbatim}

Currently, Traditional Theory domain is empty — shown as inactive but present.

Future units may populate this view for contrastive analysis.

\subsection*{VIII.7 — Embedded Code + Output Previews}

Each unit page includes:

\begin{itemize}
    \item JSON sample snippets
    \item Direct download link (\texttt{.json})
    \item Code preview block (e.g., Python \texttt{calculate\_phi()})
\end{itemize}

Cursor supports syntax-highlighted code and LaTeX-based equations in parallel.

\subsection*{VIII.8 — Educational \& Research Utility}

Cursor’s R³ structure enables:

\begin{itemize}
    \item Progressive disclosure (unit $\rightarrow$ node $\rightarrow$ formula)
    \item Citation-based expansion
    \item Interactive concept comparison
    \item Cross-disciplinary accessibility
\end{itemize}

Users can enter from abstract, scroll into algorithm, and emerge with conceptual clarity.

\section*{IX — Open Questions \& Future Work (Enhanced)}

R³ presents a robust, fully functional resonance analysis framework. Yet, like any scientific system, it operates within a set of defined constraints and assumptions. As the system matures, both its epistemic foundation and computational scope invite further exploration.

The following questions and proposed future extensions represent frontiers, not failures — theoretical edges where new forms of resonance reasoning, perceptual modeling, and interactivity may emerge.

\subsection*{IX.1 — Symbolic–Resonant Integration}

\textbf{Problem:}  
There is currently no canonical mapping from symbolic harmony (e.g., “C major”, “G7”) to resonance field structures.

\textbf{Open Question:}  
Can classical chord symbols be translated into predictable RFM patterns or CRV signatures?

\textbf{Research Path:}
\begin{itemize}
    \item Construct resonance fingerprints for chord classes
    \item Reverse-map RFM peaks to symbolic root-inversion labels
    \item Apply symbolic labeling to R³ field outputs for hybrid navigation
\end{itemize}

\textbf{Goal:}  
Enable bidirectional harmony interpretation — symbolic $\leftrightarrow$ resonance

\subsection*{IX.2 — Real-Time Processing Capabilities}

\textbf{Problem:}  
R³ is currently batch-processed from fixed input (offline mode)

\textbf{Open Question:}  
Can PRU, RPU, RFMU, and CRVU operate on live streamed data at frame-rate (0.1s or faster)?

\textbf{Engineering Path:}
\begin{itemize}
    \item Use async generators or NumPy Live for buffer streaming
    \item Convert \texttt{RawSpectrum-unit.json} generation into audio listener $\rightarrow$ CREPE $\rightarrow$ partial emitter $\rightarrow$ unit executor
    \item Port core algorithms ($\Phi$, RFM) to C++/CUDA for low-latency computation
\end{itemize}

\textbf{Goal:}  
Enable live visualization, performance-driven analysis, and generative harmony via real-time feedback loops.

\subsection*{IX.3 — Polyphonic PR Detection}

\textbf{Problem:}  
Current PRU logic assumes monophonic fundamental tracking

\textbf{Open Question:}  
Can PRU extract multiple phantom roots simultaneously — modeling polycentric tonality?

\textbf{Research Path:}
\begin{itemize}
    \item Implement time-overlapping root group tracking
    \item Use spectral clustering to separate multiple root flows
    \item Model each root’s gravitational zone in RFM separately
\end{itemize}

\textbf{Goal:}  
Capture layered tonality and its interaction in complex textures.

\subsection*{IX.4 — Multi-Listener Resonance Modeling}

\textbf{Problem:}  
R³ is designed for generalized perceptual inference — not individual neural profiles

\textbf{Open Question:}  
Can CRVU metrics be personalized based on neural data (e.g., EEG), musical background, or auditory profile?

\textbf{Path:}
\begin{itemize}
    \item Collect listener-specific FFR or ERP responses to stimulus sets
    \item Train models to predict TPS/TFI/NSF weightings per profile
    \item Tune resonance field weighting dynamically in response to engagement metrics
\end{itemize}

\textbf{Goal:}  
Model resonance-perception diversity, and adapt analysis per listener.

\subsection*{IX.5 — Prime-Limit Resonance Navigation}

\textbf{Problem:}  
Field modeling currently operates in linear frequency space

\textbf{Open Question:}  
What happens when RFM is computed in prime-vector space (e.g., 5-limit, 11-limit)?

\textbf{Mathematical Path:}
\begin{itemize}
    \item Encode partials as vectors $\vec{v} = (x_2, x_3, x_5, \ldots)$
    \item Define RFM in log-geometry of prime-lattice
    \item Extend $\nabla$RFM to multi-axis slope computation
\end{itemize}

\textbf{Goal:}  
Enable symbolically grounded resonance maps with real-number continuity

\subsection*{IX.6 — Resonance-Centric Composition Systems}

\textbf{Problem:}  
Current generative AI models are melody/chord/beat based

\textbf{Open Question:}  
Can an AI compose music guided purely by RFM field shape and CRV evolution?

\textbf{Creative Path:}
\begin{itemize}
    \item Define target RFM $\rightarrow$ search partials to generate matching field
    \item Use $\Phi$ targets to constrain harmonic grammar
    \item Tune CRV vector toward affective intent (e.g., high TPS $\rightarrow$ stability)
\end{itemize}

\textbf{Goal:}  
Create music from resonance, not just producing resonance from music.

\subsection*{IX.7 — Philosophical \& Epistemological Frontiers}

R³ challenges centuries-old assumptions about musical structure:

\begin{itemize}
    \item That tonality is symbolic
    \item That function is rule-based
    \item That perception is discrete
\end{itemize}

But if resonance is continuous, embodied, and cognitive, then:

\begin{itemize}
    \item What is a “note”?
    \item Where is the boundary between sound and structure?
    \item Can harmony exist without symbols — only through flow?
\end{itemize}

These are not technical questions — they are conceptual invitations.

\section*{X — Appendix \& References (Enhanced)}

This section consolidates all formal references, system definitions, microtonal encodings, data format standards, and mathematical mappings used throughout the R³ module. It serves as both a technical appendix and a citation-ready bibliography for researchers, developers, and composers.

\subsection*{X.1 — Scientific References}

\paragraph{A. Psychoacoustics \& Perception}
\begin{itemize}
    \item Terhardt, E. (1974). “Pitch, consonance, and harmony.” \textit{JASA}
    \item Plomp, R. \& Levelt, W. (1965). “Tonal consonance and critical bandwidth.”
    \item Bregman, A. (1990). \textit{Auditory Scene Analysis}
    \item Bidelman, G.M. et al. (2011). “Brainstem pitch encoding.”
    \item Zatorre, R. \& Salimpoor, V. (2013). “Prediction and reward in music.” \textit{Nat Rev Neurosci}
    \item Moore, B. (2012). \textit{An Introduction to the Psychology of Hearing}
\end{itemize}

\paragraph{B. Harmony \& Spectral Theory}
\begin{itemize}
    \item Sethares, W. (1998). \textit{Tuning, Timbre, Spectrum, Scale}
    \item Tymoczko, D. (2006). “The Geometry of Musical Chords.” \textit{Science}
    \item Doty, D. (2002). \textit{The Just Intonation Primer}
    \item Huron, D. (2006). \textit{Sweet Anticipation}
    \item Parncutt, R. (1989). \textit{Harmony: A Psychoacoustical Approach}
    \item Roederer, J.G. (2008). \textit{The Physics and Psychophysics of Music}
\end{itemize}

\paragraph{C. Mathematics \& Signal Processing}
\begin{itemize}
    \item Shannon, C.E. (1948). “A Mathematical Theory of Communication.”
    \item Mallat, S. (2009). \textit{A Wavelet Tour of Signal Processing}
    \item Sethares, W. (2005). “Spectral Convergence and Dissonance.” \textit{Computer Music Journal}
\end{itemize}

\subsection*{X.2 — Symbolic Pitch Encoding System}

The R³ module uses a symbolic pitch notation system that encodes:

\begin{itemize}
    \item Pitch class
    \item Octave number
    \item Microtonal deviation in cents (rounded to $\pm$25c steps)
\end{itemize}

\paragraph{Format Example:}
\begin{verbatim}
G2⁺¹  →  Pitch: G, Octave: 2, +25 cents deviation
C4⁻²  →  Pitch: C, Octave: 4, –50 cents deviation
\end{verbatim}

\textbf{Unicode Symbols:}
\begin{itemize}
    \item ⁰ = no deviation
    \item ⁺¹, ⁺², ⁻¹, ⁻² = $\pm$25c, $\pm$50c, etc.
\end{itemize}

This system aligns with symbolic notation while reflecting real spectral deviations.

\subsection*{X.3 — Data Format Reference}

\begin{center}
\begin{tabular}{|l|l|p{7.5cm}|}
\hline
\textbf{Unit} & \textbf{File Name} & \textbf{Description} \\
\hline
PRU & \texttt{PR-unit-temporal.json} & Phantom root estimations + groups \\
RPU & \texttt{RP-framewise.json}, \texttt{RP-windowed.json} & $\Phi$ metrics per frame/window \\
RFMU & \texttt{RFM-unit.json} & Resonance field grid ($f \times t$) \\
CRVU & \texttt{CRV-unit.json} & Cognitive vector [TPS, TFI, NSF] \\
\hline
\end{tabular}
\end{center}

All files are time-aligned (0.1s resolution), normalized, and UTF-8 encoded.

\subsection*{X.4 — CSV Export Format for Unity}

\begin{verbatim}
time,freq,amplitude,isFundamental,harmonic_index,symbol
\end{verbatim}

Used in \texttt{CSVLoader.cs}, \texttt{SpectrumVisualizer.cs} — mapped to Unity object parameters for real-time 3D rendering.

\subsection*{X.5 — Coordinate Mapping Schema}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Parameter} & \textbf{Mapped Dimension} & \textbf{Usage} \\
\hline
time & X-axis & horizontal flow \\
log(freq) & Y-axis & vertical tonal position \\
amplitude & Scale & object size / brightness \\
harmonic\_index & Z-axis (optional) & depth layering \\
symbol & UI label & displayed on HUDs \\
\hline
\end{tabular}
\end{center}

\subsection*{X.6 — Node \& Element Glossary}

\begin{center}
\begin{tabular}{|l|l|p{7.5cm}|}
\hline
\textbf{Node} & \textbf{Unit} & \textbf{Function} \\
\hline
CentTracker & PRU & Segment $f_0$ sequences by cent deviation \\
GroupMatcher & PRU & Match sequences to harmonic stacks \\
PairwisePhi & RPU & Compute $\Phi$ across partials \\
WindowIntegrator & RPU & Accumulate $\Phi$ in time windows \\
FieldGenerator & RFMU & Create Gaussian resonance fields \\
GradientScanner & RFMU & Compute $\nabla$RFM across frequency space \\
TPSNode & CRVU & Variance tracker of $\Phi$ $\rightarrow$ perceptual stability \\
TFINode & CRVU & Field slope magnitude $\rightarrow$ tonal fusion \\
NSFNode & CRVU & Memory-weighted $\Phi$ integral $\rightarrow$ neural salience \\
\hline
\end{tabular}
\end{center}

\subsection*{X.7 — Terminology Standardization}

\begin{itemize}
    \item \textbf{Resonance Potential ($\Phi$):} scalar harmonic coherence
    \item \textbf{Phantom Root:} inferred tonal anchor
    \item \textbf{RFM Field:} resonance topography
    \item \textbf{CRV Vector:} perceptual signature
\end{itemize}

\subsection*{X.8 — Licensing and Distribution}

All R³ code and structure is:

\begin{itemize}
    \item Open source under MIT license (default)
    \item Freely distributable for research, educational, and creative use
    \item Citable with proper attribution: \textit{“R³ module of the SRC⁹ system (2025)”}
\end{itemize}

\subsubsection*{Final Statement}

R³ unites mathematical rigor, perceptual truth, and computational clarity into a single resonance engine. This appendix stands as the foundation for collaborative development, academic referencing, and future expansion.



\title{\textbf{C³ MASTER TECHNICAL REPORT (ENHANCED)}}
\date{}
\begin{document}

\maketitle

\section*{I. INTRODUCTION}

\subsection*{I.1. Conceptual Framework}

\begin{itemize}
    \item What is C³? What is cognitive resonance?
    \item The position of C³ within the music–mind–neurophysiology triad
    \item The role of C³ in SRC⁹: S³–R³–C³ triple integration
\end{itemize}

\subsection*{I.2. History and Motivation}

\begin{itemize}
    \item Interdisciplinary disconnection: why was a system like C³ necessary?
    \item Fragmented approaches across psychoacoustics, EEG/fMRI, and cognitive theory
    \item The SRC⁹ vision: reconstructing the part–whole relationship
\end{itemize}

\section*{II. THEORETICAL FOUNDATION}

\subsection*{II.1. Theory of Cognitive Resonance}

\begin{itemize}
    \item What is sensory–cognitive resonance?
    \item Concepts of oscillation–synchronization–network integration
    \item Temporal Perceptual Stability (TPS), Tonal Fusion Index (TFI), Neural Synchronization Fields (NSF)
\end{itemize}

\subsection*{II.2. Mathematical Modeling}

\textbf{Primary Equation:}
\[
C^3(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t)
\]

\textbf{Layer Expansion:}
\[
\text{Unit}_i(t) = \sum_{j=1}^{N} w_{ij} \cdot \text{Node}_j(t)
\]

\begin{itemize}
    \item Normalized resonance computations
    \item Temporal resolution
    \item Weight coefficients
\end{itemize}

\section*{III. MODULAR ARCHITECTURE (UNIT > NODE > ELEMENT)}

\subsection*{III.1. UNIT Definitions}

Each defined in a separate section:

\begin{itemize}
    \item  CTU
    \item  AOU
    \item  IEU
    \item  SRU
    \item  SAU
    \item  PIU
    \item  IRU
    \item  NSU
    \item  RSU
\end{itemize}

\subsection*{III.2. NODE–ELEMENT Structure}

\begin{itemize}
    \item Nodes = Cognitive functions
    \item Elements = Measurable EEG/fMRI outputs
\end{itemize}

Each NODE includes:

\begin{itemize}
    \item Conceptual definition
    \item Literature reference
    \item GlassBrainMap coordinate
\end{itemize}

\section*{IV. GLASSBRAINMAP INTEGRATION}

\subsection*{IV.1. Coordinate System}

\begin{itemize}
    \item Each NODE assigned a 3D spherical region in MNI space
    \item Anatomical localization (e.g., ACC, SMA, HG)
\end{itemize}

\subsection*{IV.2. Mapping + Web Integration}

\begin{itemize}
    \item SVG layer
    \item React/D3.js interface
    \item Hover–click interaction
    \item Tooltip + page linkage
\end{itemize}

\section*{V. SYSTEM DYNAMICS}

\subsection*{V.1. Time-Based Activations}

\begin{itemize}
    \item How is the overall output of C³ computed over time?
    \item Frame calculations via EEG resolution
\end{itemize}

\subsection*{V.2. Feedback Loop: C³ ↔ R³ ↔ S³}

\begin{itemize}
    \item Harmonic Distance and Φ values from R³
    \item Spectral profile data from S³
    \item Feedback from C³: attentional dispersion, valence mapping
\end{itemize}

\section*{VI. RESEARCH AND APPLICATION DOMAINS}

\subsection*{VI.1. Music Therapy}

\begin{itemize}
    \item Effects of different UNITs in therapeutic settings
    \item Trauma-informed listening through IRU and PIU
\end{itemize}

\subsection*{VI.2. Compositional Tools}

\begin{itemize}
    \item Sound selection guided by C³
    \item Listener direction through CTU + AOU interaction
\end{itemize}

\subsection*{VI.3. Neuropedagogy}

\begin{itemize}
    \item Attention enhancement in children (IEU + SAU)
    \item Resonance density in learning via RSU
\end{itemize}

\section*{VII. FUTURE ROADMAP}

\subsection*{VII.1. Real-Time Integration}

\begin{itemize}
    \item Unity/WebGL + OSC/WebSocket connectivity
\end{itemize}

\subsection*{VII.2. VR/AR and Cinematic Systems}

\begin{itemize}
    \item GlassBrainVR
    \item Resonance–narrative integration
\end{itemize}

\section*{VIII. CONCLUSION AND SCIENTIFIC CONTRIBUTIONS}

\begin{itemize}
    \item Gaps in existing systems
    \item Innovative solutions offered by C³
    \item The irreplaceable role of C³ within SRC⁹
\end{itemize}

\section*{I.1 Conceptual Framework}

\subsection*{1.1.1 What is C³?}

\textbf{C³ – Cognitive Consonance Circuit} is a neurophysiologically grounded, mathematically formulated cognitive modeling engine that analyzes the \textbf{emotional}, \textbf{cognitive}, \textbf{motor}, and \textbf{social resonance processes} evoked by music in the human brain as a time-dependent, multilayered system.

The main goal of this system is to measure, classify, and represent the \textbf{temporal resonance responses} in a music listener's brain using data such as:

\begin{itemize}
    \item EEG (alpha, beta, gamma, MMN, P300),
    \item MEG (oscillatory phase-locking),
    \item fMRI (region-specific activations)
\end{itemize}

C³ operates through \textbf{9 independent yet integrated Units}. Each UNIT represents a specific cognitive function. Internally, these Units are composed of \textbf{NODEs} (functional mechanisms) and \textbf{ELEMENTs} (measurable neural outputs). In this way, the system creates a fully connected network between music and the brain.

\subsection*{1.1.2 The Role of C³ Within SRC⁹}

C³ is one of the three main modules of the \textbf{SRC⁹ system}:

\begin{itemize}
    \item \textbf{S³ – Spectral Sound Space}: Physical–acoustic analysis of sound
    \item \textbf{R³ – Resonance-Based Relational Reasoning}: Mathematical modeling of tonal, microtonal, and harmonic structures
    \item \textbf{C³ – Cognitive Consonance Circuit}: Representation of musical structures' impact on the brain through time-based neurocognitive models
\end{itemize}

There is \textbf{bidirectional data flow} between these modules. For example:

\begin{itemize}
    \item Spectral data from S³ (e.g., noise intensity, tonal center frequency) $\rightarrow$ enters R³ for computations of Φ (resonance potential) and HD (harmonic distance) $\rightarrow$ these values are then transmitted to C³ to generate cognitive load or emotional response in modules like CTU and AOU.
\end{itemize}

C³ also sends its results as \textbf{feedback} to R³ (harmonic resonance suggestions) and to S³ (perceptual spectrum map filtering). This structure defines the system’s \textbf{adaptive feedback} nature.

\subsection*{1.1.3 Core System Principles}

\subsubsection*{1.1.3.a Definition of Cognitive Resonance}

In the C³ system, cognitive resonance is defined as the \textbf{simultaneous activation} of processes such as \textbf{frequency–time alignment}, \textbf{sensory–conceptual integration}, \textbf{motor synchronization}, and \textbf{emotional response} in the brain during music listening. This resonance is studied across the following layers:

\begin{itemize}
    \item \textbf{Electrophysiological:} EEG data (phase locking, oscillation power, ERP components)
    \item \textbf{Functional Imaging:} fMRI BOLD increase, regional localization in MNI coordinates
    \item \textbf{Time-Resolved Modeling:} Moment-to-moment alignment of music events with brain responses
\end{itemize}

\subsection*{1.1.4 Structure of C³: UNIT → NODE → ELEMENT}

\begin{itemize}
    \item \textbf{UNIT}: Conceptual framework (e.g., CTU – Cognitive Tension Unit)
    \item \textbf{NODE}: Specific functional structure (e.g., Harmonic Dissonance Node)
    \item \textbf{ELEMENT}: Measurable EEG/fMRI output (e.g., alpha–beta phase locking)
\end{itemize}

This structure ensures that each UNIT consists of layers that are \textbf{scientifically}, \textbf{experimentally}, and \textbf{theoretically connected}. Thus, each NODE becomes \textbf{conceptually defined}, \textbf{empirically measurable}, and \textbf{anatomically localizable}.

\subsection*{1.1.5 Anatomical Mapping: GlassBrainMap Integration}

Each NODE in C³ is assigned a brain region (ROI – Region of Interest). These regions are defined as spherical volumes using \textbf{MNI coordinates}.

\begin{center}
\begin{tabular}{|l|c|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Reference} \\
\hline
AOU – HGAINT 01 & \texttt{[-60, -28, +6]} & Left posterior STG & Potes et al., 2012 \\
IEU – HARMONIC 01 & \texttt{[0, +50, +20]} & Fronto-central EEG & Crespo-Bojorque et al., 2018 \\
CTU – RESOLVED 05 & \texttt{[+64, –22, +4]} & Anterior HG & Norman-Haignere et al., 2013 \\
\hline
\end{tabular}
\end{center}

These coordinates are fully aligned with the data in \texttt{C\_\_BrainMap\_\_\_83\_Koordinatl\_k\_Tam\_Tablo} and \textbf{GlassBrainMap.pdf}.

On the web platform, this information is connected to a visual map through the SVG + React component \texttt{GlassBrain.jsx}.

\subsection*{1.1.6 Literature Foundation}

As of now, a total of \textbf{61 scientific articles} have been integrated into the C³ system. Each NODE is supported by at least one experimental study. These include:

\begin{itemize}
    \item EEG/MEG-based ERP or mismatch studies
    \item fMRI studies (valence, arousal, pitch, syntax, entrainment)
    \item Multimodal meta-analyses (FFR, P300, dopamine release, MMN)
    \item Theoretical models (predictive coding, Wundt curve, information content models)
\end{itemize}

\subsection*{Conclusion (for this section)}

\textbf{C³} forms the \textbf{neurocognitive core} of the SRC⁹ system. It receives spectral and resonance data from modules like S³ and R³, interprets them through EEG/fMRI-supported models in the human brain, and outputs results that allow musical structures to be measured at the level of \textbf{meaning and impact}. It is the only system that achieves this function in a fully integrated form.


\section*{I.1 Definitional Framework}

\textit{What is C³? What does it do? How does it function within SRC⁹?}

\subsection*{1.1.1 What Is the Cognitive Consonance Circuit (C³)?}

The \textbf{Cognitive Consonance Circuit (C³)} is a multidimensional, time-sensitive neural modeling system designed to measure, classify, and structurally represent the cognitive, emotional, and motor impacts of music on the human brain. C³ integrates empirical neurophysiological data—specifically EEG (alpha, beta, gamma, MMN, P300), MEG, and fMRI—to track brain responses to musical stimuli in real-time across nine cognitive domains known as \textbf{Units}.

Each Unit corresponds to a distinct cognitive subsystem (e.g., expectation, tension, memory, emotion), modeled as an independent but fully integrable node within the system. Internally, each Unit is hierarchically structured into \textbf{Nodes}, which denote abstract cognitive processes, and \textbf{Elements}, which refer to quantifiable neural signals. These signals include frequency-specific oscillatory phenomena, phase-locked responses, and region-specific BOLD activations.

\subsection*{1.1.2 Role of C³ Within the SRC⁹ System}

C³ is the cognitive core of the \textbf{SRC⁹ (Spectral–Resonance–Cognitive)} framework, which comprises three interdependent modules:

\begin{itemize}
    \item \textbf{S³ – Spectral Sound Space}: Performs advanced time–frequency analysis of musical signals.
    \item \textbf{R³ – Resonance-Based Relational Reasoning}: Models harmonic and tonal structures through vector lattices, scalar spaces, and resonance metrics.
    \item \textbf{C³ – Cognitive Consonance Circuit}: Converts these structural data into biologically realistic neural representations of how the brain interprets music.
\end{itemize}

These modules are not isolated silos; they form a \textbf{bidirectional data exchange pipeline}:

\begin{itemize}
    \item Spectral information from S³ (e.g., timbral flux, pitch entropy) is used by R³ to calculate harmonic metrics such as Φ (Resonance Potential) and HD (Harmonic Distance).
    \item These R³ outputs then inform C³, which uses EEG and fMRI-informed functions to simulate attention, emotional valence, arousal, and tension.
    \item C³, in turn, feeds back cognitive feedback into S³ and R³, enabling dynamic perceptual recalibration.
\end{itemize}

This feedback loop creates a closed analytical ecosystem that integrates perception, structure, and cognition.

\subsection*{1.1.3 Theoretical Principles Underpinning C³}

\subsubsection*{a. Definition of Cognitive Resonance}

Cognitive resonance refers to the dynamic synchronization of neural, emotional, and motor processes elicited by musical stimuli. This phenomenon is understood not as localized brain activation, but as a distributed resonance field involving:

\begin{itemize}
    \item Oscillatory synchronization (e.g., beta/gamma coupling between SMA and auditory cortex)
    \item Predictive mismatch mechanisms (e.g., MMN and P300 ERP responses)
    \item Region-specific activation (e.g., DLPFC for tension, ACC for ambiguity, amygdala for affect)
\end{itemize}

These mechanisms interact through temporal coherence and multi-band entrainment, enabling a real-time neural “tracking” of musical structure.

\subsubsection*{b. Mathematical Formalism}

The system's top-level model is defined as:

\[
C^3(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t)
\]

Where:

\begin{itemize}
    \item $C^3(t)$: total cognitive resonance at time $t$
    \item $\text{Unit}_i(t)$: normalized activity of the $i$-th cognitive unit
    \item $w_i$: weight coefficient for each unit (empirically tunable)
\end{itemize}

Each Unit’s internal model is further decomposed as:

\[
\text{Unit}_i(t) = \sum_{j=1}^{N} w_{ij} \cdot \text{Node}_j(t)
\]

Each Node is derived from EEG or fMRI data—either continuous oscillatory amplitudes or discrete event-related potentials—and parameterized using real-time neuroimaging standards.

\subsection*{1.1.4 Structural Design: UNIT → NODE → ELEMENT}

The C³ system is hierarchically organized:

\begin{itemize}
    \item \textbf{UNIT}: Defines a domain of cognitive-musical processing (e.g., tension, memory, flow).
    \item \textbf{NODE}: Represents a functional module within the Unit (e.g., Harmonic Dissonance).
    \item \textbf{ELEMENT}: A neurophysiological observable (e.g., beta-band phase locking, BOLD activation).
\end{itemize}

Each Element is associated with:

\begin{itemize}
    \item A named brain region
    \item A measurement method (EEG, fMRI, or both)
    \item A validated reference from neuroscience literature
    \item An anatomical coordinate (MNI system)
\end{itemize}

This allows C³ to link abstract musical cognition to empirical neurobiology in a deterministic way.

\subsection*{1.1.5 Anatomical Mapping with GlassBrainMap}

C³ integrates seamlessly with the \textbf{GlassBrainMap}, a visual representation system that places each Node/Element at anatomically and functionally relevant coordinates.

\begin{center}
\begin{tabular}{|l|c|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Citation} \\
\hline
AOU – HGAINT 01 & \texttt{[-60, -28, +6]} & Left posterior STG & Potes et al., 2012 \\
IEU – HARMONIC 01 & \texttt{[0, +50, +20]} & Fronto-central cortex & Crespo-Bojorque et al., 2018 \\
CTU – RESOLVED 05 & \texttt{[+64, –22, +4]} & Right Anterior HG & Norman-Haignere et al., 2013 \\
\hline
\end{tabular}
\end{center}

These coordinates are verified against the \texttt{C\_\_BrainMap\_\_\_83\_Koordinatl\_k\_Tam\_Tablo} document, using MNI-space localization. In the web interface, they are rendered interactively via \texttt{GlassBrain.jsx} and SVG/D3.js overlays.

\subsection*{1.1.6 Literature Foundation and Citation Model}

The C³ system is constructed upon \textbf{61 primary peer-reviewed studies} in the fields of neuroscience, music cognition, EEG/fMRI research, and mathematical modeling. Each Node is grounded in direct empirical evidence. Key domains include:

\begin{itemize}
    \item MMN, P300, N2 ERP responses to pitch/syntax deviations
    \item Beta/gamma-band entrainment in auditory–motor synchronization
    \item Dopaminergic pathways during emotional peaks (Salimpoor et al., 2011)
    \item Functional anatomy of musical memory (Janata, 2009; Zatorre \& Halpern, 2005)
\end{itemize}

All references are encoded into structured \texttt{.bib} and \texttt{.json} formats for API-level integration.

\subsection*{Summary}

\textbf{C³} is not a passive music analysis tool. It is an \textbf{active cognitive modeling system} that transforms symbolic or spectral musical structures into meaningful, neurologically validated resonance fields. Its position within the \textbf{SRC⁹} system ensures that structure, perception, and cognition are analyzed as a single unified continuum.

\section*{I.2 Historical Context and Motivation}

\textit{Why was C³ developed? What are the limitations of current methodologies that C³ addresses?}

\subsection*{1.2.1 Fragmentation Across Disciplines}

Despite significant progress in music theory, neuroscience, and artificial intelligence, the academic and technological landscape surrounding music cognition remains deeply fragmented. Most systems operate in isolation:

\begin{itemize}
    \item Spectral analysis platforms (e.g., Fourier-based spectrograms) offer detailed acoustic profiles but lack perceptual or cognitive grounding.
    \item Harmonic and mathematical theories (e.g., Lewin’s GIS, Tonnetz models) focus on intervallic structure but often disregard empirical brain data.
    \item Neuroscientific research (EEG, fMRI) reveals profound insights into perception and emotion but rarely interfaces with formal music theory or real-time systems.
\end{itemize}

This disjunction has created a theoretical bottleneck. Without a shared framework, advances in one domain fail to propagate meaningfully into others. As a result, most current tools lack cross-domain explanatory power, cognitive transparency, and compositional usability.

\subsection*{1.2.2 Theoretical Models Lack Cognitive Validation}

Traditional music-theoretical models such as Generalized Interval Systems (GIS), Harmonic Distance metrics, and Tonal Hierarchies (e.g., Krumhansl’s tonal profiles) are mathematically elegant, but often fail to predict real listener responses.

Conversely, neuroscientific findings—such as:

\begin{itemize}
    \item the MMN response to unexpected harmonies,
    \item the P300 component linked to rhythmic anomalies,
    \item or reward-related dopamine release during musical peaks (Salimpoor et al., 2011),
\end{itemize}

have rarely been integrated into formal, computationally usable structures.

There is no established mapping between music-theoretical constructs (e.g., cadence, modulation, dissonance) and neural metrics (e.g., BOLD, ERP, phase-locking). This absence of structural integration drastically reduces the predictive and pedagogical power of existing systems.

\subsection*{1.2.3 Existing AI Models Are Black-Box and Aesthetic-Only}

While AI tools such as OpenAI’s Jukebox, Google Magenta, or Amper Music have enabled impressive generative outputs, they typically operate without explanatory or cognitive modeling frameworks. They generate music without understanding what attention, memory, emotion, or structural coherence mean to a human listener.

This black-box architecture:

\begin{itemize}
    \item Offers no feedback on listener state
    \item Cannot simulate emotional or cognitive pathways
    \item Provides no route for targeted composition or real-time feedback
\end{itemize}

C³ was conceived precisely to close this loop—not to replace such systems, but to make them cognitively explainable, analyzable, and affectively steerable.

\subsection*{1.2.4 Neuroscience Models Remain Theoretically Isolated}

Even the most sophisticated neuroscientific work on music (e.g., Janata, 2009; Zatorre \& Halpern, 2005) remains technically inaccessible to composers, theorists, or real-time music systems. The brain data is not structured in a way that can be operationalized.

For example:

\begin{itemize}
    \item Neural entrainment in STG and SMA during rhythmic listening (Nozaradan, 2012)
    \item Amygdala activation during tonal familiarity or melodic recall (Koelsch, 2008)
    \item fMRI-detected dopaminergic release in ventral striatum (Blood \& Zatorre, 2001)
\end{itemize}

—all provide powerful evidence, but without an architectural scaffold to translate them into music-analytical or compositional insight.

\subsection*{1.2.5 Motivation for C³: Unification Through Architecture}

C³ was developed as an architectural answer to this methodological impasse.

\textbf{Its core aims:}

\begin{itemize}
    \item To provide a layered system that explicitly links:
    \begin{itemize}
        \item Symbolic musical events (e.g., harmonic surprise, tempo shift)
        \item Neural mechanisms (e.g., alpha desynchronization, ERP signatures)
        \item Anatomical mappings (e.g., ACC, STG, dPMC)
        \item Time-based resonance metrics
    \end{itemize}
    
    \item To construct a modular model (9 Units) that mirrors actual neuroscientific categorizations:
    
    \begin{itemize}
        \item Tension (CTU), Expectation (IEU), Emotion (AOU), Flow (PIU), Memory (SAU), etc.
    \end{itemize}
    
    \item To ensure every part of the system is:
    \begin{itemize}
        \item Empirically grounded (with MNI coordinates, EEG/fMRI citations)
        \item Mathematically formalized (via resonance models and time equations)
        \item Interactively visualized (through the GlassBrainMap and unit dashboards)
    \end{itemize}
    
    \item To allow integration with generative, educational, or therapeutic systems.
\end{itemize}

\subsection*{1.2.6 Theoretical Inspirations}

C³ draws from and synthesizes:

\begin{itemize}
    \item David Lewin’s transformational music theory and the notion of “network transformations”
    \item Julian Hook’s cross-type intervallic mappings
    \item Jean-Jacques Nattiez’s tripartite semiotic model (poietic – neutral – esthesic)
    \item Zatorre \& Halpern’s fMRI studies on musical imagery
    \item Large \& Snyder’s work on neural resonance and attentional entrainment
    \item Recent computational neuroscience models such as Temporal Perceptual Stability (TPS), Tonal Fusion Index (TFI), and Neural Synchronization Fields (NSF)
\end{itemize}

These form the philosophical, neurobiological, and mathematical pillars of the C³ system.

\subsection*{Summary}

C³ was not built to replace existing methods—it was built to unify them.

It serves as a cognitive-mathematical bridge between acoustic data, symbolic musical structures, and neurological response patterns. Its modular design, mathematical backbone, and empirical grounding position it as a unique tool in both theoretical and practical domains.

It is not merely a system. It is a new paradigm for understanding music.

\section*{II.1 Theoretical Foundations: Cognitive Resonance Theory}

\textit{What is cognitive resonance? What are its neural and mathematical correlates? How is it represented in the C³ framework?}

\subsection*{2.1.1 Definition of Cognitive Resonance}

Cognitive resonance refers to the simultaneous activation and phase-alignment of neural systems—cortical, subcortical, and limbic—elicited by musical structures perceived as meaningful, surprising, emotionally salient, or motorically engaging.

It is not a static "response" but a dynamic system of oscillatory entrainment that unfolds over time in direct correlation with acoustic and symbolic musical events. In this sense, C³ does not model perception as a reaction, but rather as a temporally evolving resonance field shaped by attention, prediction, emotion, and embodied synchronization.

\subsection*{2.1.2 Neurophysiological Basis}

Cognitive resonance is anchored in three core mechanisms:

\paragraph{a. Oscillatory Entrainment}

Neural populations synchronize their firing phases with periodic or structured stimuli in music, especially rhythm and pulse. EEG reveals:

\begin{itemize}
    \item Beta-band phase-locking in motor regions (SMA, PMC) during pulse alignment
    \item Gamma coherence between auditory and frontal regions for tonal/melodic fusion
    \item Alpha synchrony in frontal–parietal regions indicating attentional top-down integration
\end{itemize}

\paragraph{b. Prediction and Mismatch Mechanisms}

The brain actively predicts musical continuations. Violations of these predictions result in:

\begin{itemize}
    \item Mismatch Negativity (MMN) in STG and Fz (EEG), typically for harmonic or rhythmic deviations
    \item P300 ERPs for consciously detected rhythmic or temporal anomalies
\end{itemize}

These responses form the backbone of the IEU unit in C³.

\paragraph{c. Emotional–Limbic Activation}

Music evokes emotion through dopaminergic reward pathways, primarily involving:

\begin{itemize}
    \item Ventral Striatum, Nucleus Accumbens (peak pleasure: Salimpoor et al., 2011)
    \item Amygdala, Insula, and ACC (valence/arousal distinction: Koelsch, 2008; Zatorre \& Blood, 2001)
\end{itemize}

Together, these three systems create multiband, multisite synchronization patterns that constitute what we term \textbf{cognitive resonance}.

\subsection*{2.1.3 Mathematical Representation}

In C³, cognitive resonance is mathematically expressed as the time-evolving summation of weighted neural activity across multiple Units:

\[
C^3(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t)
\]

Where:

\begin{itemize}
    \item $\text{Unit}_i(t)$: Time-normalized resonance output of the $i$-th Unit (e.g., CTU, AOU)
    \item $w_i$: Tunable weight coefficient based on experimental or functional prioritization
    \item $t$: Time (sampled at EEG resolution, e.g., 100ms)
\end{itemize}

Each Unit is composed of Nodes and Elements:

\[
\text{Unit}_i(t) = \sum_{j=1}^{N} w_{ij} \cdot \text{Node}_j(t)
\]

Each Node is grounded in a real EEG/fMRI marker:

\begin{itemize}
    \item \textbf{Node}: conceptual function (e.g., "Harmonic Dissonance")
    \item \textbf{Element}: observable signal (e.g., “$\alpha$–$\beta$ phase-locking in DLPFC”)
\end{itemize}

These form the structural hierarchy:

\[
\text{UNIT} \rightarrow \text{NODE} \rightarrow \text{ELEMENT} \rightarrow \{\text{Method, Region, Coordinates, Citation}\}
\]

\subsection*{2.1.4 Temporal Dynamics and Resolution}

C³ operates in time-series frames, typically aligned to 0.1-second EEG windows. This enables high-resolution modeling of:

\begin{itemize}
    \item Fast transitions (e.g., rhythmic inflections, expectation violations)
    \item Slow evolutions (e.g., emotional immersion, tonal unfolding)
\end{itemize}

Each Element generates a signal trace over time, which is then:

\begin{itemize}
    \item Normalized to a common scale [0, 1]
    \item Weighted within its Node and Unit
    \item Aggregated into the total resonance field $C^3(t)$
\end{itemize}

\textbf{Result}: a spatiotemporal map of musical cognition, updated per frame, and navigable across Units, Nodes, and regions.

\subsection*{2.1.5 Topological Interpretation: Resonance Fields}

The final output of C³ can be interpreted as a multidimensional field evolving in time:

\begin{itemize}
    \item Each axis corresponds to a Unit (9D space)
    \item Each point $C^3(t)$ is a vector of dimension 9
    \item Over time, the output forms a trajectory curve in this high-dimensional space
\end{itemize}

This allows:

\begin{itemize}
    \item Trajectory clustering for musical styles
    \item Dynamical stability analysis (e.g., flow states vs. high-tension nodes)
    \item Comparative signature modeling (e.g., comparing Bach vs. Radiohead vs. AI-generated pieces)
\end{itemize}

These analytical outputs feed into applications like music therapy profiling, personalized listening models, or composition-guidance systems.

\subsection*{2.1.6 Link to Neuroanatomy: Mapping into GlassBrain}

Each Element has a unique anatomical anchor:

\begin{itemize}
    \item Region name (e.g., Broca, SMA, NAcc)
    \item MNI coordinates (from \texttt{brain\_coords.json})
    \item Visualization via \texttt{GlassBrain.jsx} or Unity 2D/3D interfaces
\end{itemize}

This results in:

\begin{itemize}
    \item Real-time activation maps per frame
    \item Tooltip-based summaries
    \item Click-through access to associated Unit/Node pages
\end{itemize}

Each region is plotted as a sphere in anatomical space and linked to its resonance value over time.

\subsection*{Summary}

Cognitive resonance is not a metaphor. It is a quantifiable, observable, and mathematically modelable system of neural alignment elicited by music.

C³ builds a bridge between:

\begin{itemize}
    \item \textbf{Structure} (from S³ and R³)
    \item \textbf{Perception} (via neural synchronization)
    \item \textbf{Emotion} (through limbic modeling)
    \item \textbf{Cognition} (via predictive structures)
\end{itemize}

This fusion yields not only a deepened understanding of musical experience, but also a practical computational system that allows music to be measured, mapped, predicted, and creatively shaped.

\section*{II.2 Mathematical Modeling of the C³ System}

\textit{How is C³ formulated mathematically? What are its temporal and structural components? How does it compute cognition in music?}

\subsection*{2.2.1 Layered Structure: From Signal to Cognition}

The C³ system is constructed as a hierarchical neural modeling architecture operating on three levels:

\[
C^3(t) = \sum_i \text{Unit}_i(t), \quad \text{Unit}_i(t) = \sum_j \text{Node}_{ij}(t), \quad \text{Node}_{ij}(t) = \sum_k \text{Element}_{ijk}(t)
\]

Each Element represents a measurable neural signal, modeled mathematically. Nodes aggregate these signals into functional units (e.g., "Expectation Violation", "Tonal Familiarity"). Units represent full cognitive subsystems (e.g., IEU, SAU).

This bottom-up structure enables us to compute macro-level phenomena (emotion, memory, flow) from micro-level EEG/fMRI signals.

\subsection*{2.2.2 Primary Equation}

At the system level:

\[
C^3(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t)
\]

Where:

\begin{itemize}
    \item $C^3(t)$: Total cognitive resonance at time $t$
    \item $\text{Unit}_i(t)$: Activity of Unit $i$
    \item $w_i \in \mathbb{R}$: Empirically tunable weight for each Unit (default: 1.0)
\end{itemize}

Each Unit has a local expansion:

\[
\text{Unit}_i(t) = \sum_{j=1}^{N_i} w_{ij} \cdot \text{Node}_{ij}(t)
\]

\begin{itemize}
    \item $\text{Node}_{ij}(t)$: Activity of Node $j$ in Unit $i$
    \item $w_{ij} \in [0,1]$: Functional contribution weight of Node $j$
\end{itemize}

Each Node is derived from its underlying Elements:

\[
\text{Node}_{ij}(t) = \sum_{k=1}^{M_{ij}} w_{ijk} \cdot \text{Element}_{ijk}(t)
\]

\subsection*{2.2.3 Element-Level Signal Modeling}

An Element is a neural feature defined as:

\[
\text{Element}_{ijk}(t) = S_{ijk}(t) \in [0,1]
\]

Where $S_{ijk}(t)$ is the normalized neural signal derived from:

\begin{itemize}
    \item EEG band power (e.g., $\alpha$, $\beta$, $\gamma$)
    \item ERP component (e.g., MMN, P300)
    \item fMRI BOLD z-score in MNI-space
    \item Phase coherence between regions
\end{itemize}

Each signal is transformed to the [0,1] scale using:

\[
S_{ijk}(t) = \frac{x(t) - \min(x)}{\max(x) - \min(x)}
\]

For oscillatory components:

\[
x(t) = \left| \mathcal{H}\{ \text{EEG}(t) \} \right|^2
\]

(Where $\mathcal{H}$ is the Hilbert Transform envelope)

For ERP components:

\[
x(t) = \text{ERP amplitude}(t - \tau)
\]

For fMRI:

\[
x(t) = z(t) = \frac{\text{BOLD}(t) - \mu}{\sigma}
\]

All data is resampled to a common timeline resolution (typically 10 Hz, i.e., 100ms frames).

\subsection*{2.2.4 Weight Normalization and Adaptivity}

Weights $w_i$, $w_{ij}$, $w_{ijk}$ are initialized based on empirical studies:

\begin{itemize}
    \item $w_{ijk}$: based on effect size from literature (e.g., Cohen’s $d$)
    \item $w_{ij}$: based on relative contribution within Unit (e.g., entropy vs. ERP)
    \item $w_i$: application-specific (e.g., PIU emphasized in flow-based systems)
\end{itemize}

Over time, weights can be adapted dynamically based on:

\begin{itemize}
    \item Task relevance (e.g., in therapy, CTU may be downregulated)
    \item User feedback (e.g., neurofeedback systems)
    \item Generative models (e.g., AI uses the feedback to alter music in real time)
\end{itemize}

\subsection*{2.2.5 Matrix Representation}

The entire C³ system can be formalized as a three-layer matrix multiplication:

\[
C^3(t) = \mathbf{W}^T \cdot \mathbf{U}(t)
\]

Where:

\[
\mathbf{U}(t) = [\text{Unit}_1(t), \ldots, \text{Unit}_9(t)]^T \in \mathbb{R}^{9 \times 1}
\]

Each $\text{Unit}_i(t)$ is:

\[
\text{Unit}_i(t) = \mathbf{w}_i^T \cdot \mathbf{N}_i(t)
\]

And each $\mathbf{N}_i(t)$ (Node vector) is:

\[
\mathbf{N}_i(t) = [\text{Node}_{i1}(t), \ldots, \text{Node}_{iN}(t)]^T
\]

This layered abstraction makes it possible to:

\begin{itemize}
    \item Implement efficient GPU-based computation
    \item Track changes per unit or per frame
    \item Enable real-time synchronization with audiovisual feedback
\end{itemize}

\subsection*{2.2.6 Implementation Schema}

Each Element is linked to:

\begin{itemize}
    \item \texttt{unit\_id} (e.g., CTU)
    \item \texttt{node\_id} (e.g., harmonic\_dissonance)
    \item MNI coordinates
    \item Measurement method (EEG, fMRI)
    \item Source file path (for dynamic signal streaming)
\end{itemize}

All metadata is stored in \texttt{brain\_coords.json}, and signals are fed as \texttt{.edf}, \texttt{.csv}, or \texttt{.json} time series.

Visual representation is done through:

\begin{itemize}
    \item \texttt{NodeView.jsx} for cognitive UI
    \item \texttt{GlassBrain.jsx} for anatomical UI
    \item \texttt{C3Engine.ts} (or Python backend) for signal computation
\end{itemize}

\subsection*{Summary}

The C³ system is not merely a conceptual framework but a fully formalized, mathematically grounded computational model. It:

\begin{itemize}
    \item Integrates real neural signals into a unified cognitive resonance space
    \item Adapts to multiple temporal and structural resolutions
    \item Operates across Unit, Node, and Element layers
    \item Is designed for interactive, real-time applications in analysis, composition, education, and therapy
\end{itemize}

This mathematical architecture makes it possible to represent the complexity of musical cognition as a dynamic, quantifiable, and deeply interpretable process.

\section*{III.1 Unit Definitions and Functional Roles}

\subsection*{3.1.1 CTU – Cognitive Tension Unit}

\textbf{Models cognitive stress responses to musical dissonance, entropy, and tonal distance.}

\subsubsection*{Overview}

The Cognitive Tension Unit (CTU) is designed to capture and model the neural mechanisms of cognitive dissonance, ambiguity, and harmonic instability in response to complex musical stimuli. It measures the brain’s response to tonal unpredictability, spectral irregularity, and harmonic deviation by tracking electrophysiological indicators and hemodynamic signals primarily across the dorsolateral prefrontal cortex (DLPFC), anterior cingulate cortex (ACC), and inferior frontal gyrus (Broca area).

This unit operates on the hypothesis that musical dissonance, entropy, and distance from tonal centers create a measurable increase in cognitive load. These effects are observable via:

\begin{itemize}
    \item Alpha and beta phase-locking in frontal regions (EEG)
    \item Increased BOLD activity in cingulate cortex (fMRI)
    \item Beta-band amplitude increases in Broca’s area during spectral irregularity
\end{itemize}

\subsubsection*{Mathematical Representation}

The internal model of CTU is:

\[
\text{CTU}(t) = w_1 \cdot \text{Node}_{\text{Harmonic Dissonance}}(t) + w_2 \cdot \text{Node}_{\text{Spectral Entropy}}(t) + w_3 \cdot \text{Node}_{\text{Harmonic Distance}}(t)
\]

Where:

\begin{itemize}
    \item $w_1, w_2, w_3$: empirically tunable node weights
\end{itemize}

Each Node has one or more Elements grounded in EEG/fMRI markers.

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Harmonic Dissonance}

\begin{itemize}
    \item \textbf{Description}: Models increased cognitive stress when dissonant chords or pitch clusters appear in a tonal context
    \item \textbf{Region}: DLPFC, ACC
    \item \textbf{EEG Marker}: Alpha–beta phase locking
    \item \textbf{Citation}: Fishman et al., 2001
\end{itemize}

\textbf{Element}:
\begin{itemize}
    \item �� EEG Phase Locking ($\alpha$–$\beta$)
    \item EEG electrodes: F3, Fz
    \item MNI Coordinate: [+32, +50, +20]
    \item GlassBrainMap ID: \texttt{ctu\_harmonic\_dissonance\_01}
\end{itemize}

\paragraph{�� Node: Spectral Entropy}

\begin{itemize}
    \item \textbf{Description}: Quantifies the unpredictability and information density in the sound spectrum
    \item \textbf{Region}: Broca’s area (left IFG), temporal cortex
    \item \textbf{EEG Marker}: Beta amplitude increase
    \item \textbf{Citation}: Norman-Haignere et al., 2013
\end{itemize}

\textbf{Element}:
\begin{itemize}
    \item �� Spectral Complexity Response
    \item MNI Coordinate: [+64, –22, +4]
    \item GlassBrainMap ID: \texttt{ctu\_entropy\_01}
    \item EEG channel cluster: F7–T7
    \item fMRI: Left IFG BOLD increase
\end{itemize}

\paragraph{�� Node: Harmonic Distance}

\begin{itemize}
    \item \textbf{Description}: Measures perceived tonal instability caused by deviations from local tonal center
    \item \textbf{Region}: DLPFC, ACC
    \item \textbf{EEG Marker}: Alpha amplitude increase
    \item \textbf{Citation}: Hyde et al., 2008
\end{itemize}

\textbf{Element}:
\begin{itemize}
    \item �� EEG Alpha Power (ACC-centered)
    \item MNI Coordinate: [+30, +36, +20]
    \item GlassBrainMap ID: \texttt{ctu\_distance\_01}
    \item Method: Power spectral density (EEG)
\end{itemize}

\subsubsection*{GlassBrainMap Integration}

The CTU Nodes are spatially registered within the GlassBrain system. Their associated coordinates and regions are as follows:

\begin{center}
\begin{tabular}{|l|c|l|l|}
\hline
\textbf{Node ID} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Tooltip} \\
\hline
\texttt{ctu\_harmonic\_dissonance\_01} & [+32, +50, +20] & DLPFC + ACC & EEG $\alpha$–$\beta$ phase-locking \\
\texttt{ctu\_entropy\_01} & [+64, –22, +4] & Broca + Temporal Cortex & Beta increase in spectral complexity \\
\texttt{ctu\_distance\_01} & [+30, +36, +20] & ACC + DLPFC & EEG Alpha increase for tonal ambiguity \\
\hline
\end{tabular}
\end{center}

These markers appear in the \texttt{GlassBrain.jsx} component and are directly linked to the \texttt{ctu.json} file in the system.

\subsubsection*{Functional Summary}

The CTU unit functions as a cognitive load detector, dynamically representing how musical instability translates into mental effort. It is especially responsive to:

\begin{itemize}
    \item Tonal deviations
    \item Unexpected dissonances
    \item High spectral entropy
\end{itemize}

These events are interpreted by the brain as uncertainty or cognitive conflict, which C³ measures in real time.

CTU is often antagonistic to PIU (Phenomenological Immersion Unit): increased CTU activity often leads to decreased absorption or flow.

\subsubsection*{Applications}

\begin{itemize}
    \item \textbf{Therapeutic Monitoring}: Cognitive overload indicators in neurorehabilitation
    \item \textbf{Compositional Tools}: Dynamic tension mapping for film scoring or generative music
    \item \textbf{EEG Feedback Systems}: Real-time CTU readout for adaptive sound environments
\end{itemize}

\subsection*{3.1.2 AOU – Affective Orientation Unit}

\textbf{Models emotional resonance to music through valence and arousal dimensions.}

\subsubsection*{Overview}

The Affective Orientation Unit (AOU) quantifies the listener’s affective response to music by modeling two principal emotional axes:

\begin{itemize}
    \item \textbf{Valence}: The pleasantness or unpleasantness of the musical stimulus
    \item \textbf{Arousal}: The intensity or physiological activation induced by the music
\end{itemize}

These dimensions are computed through EEG and fMRI indicators within limbic, prefrontal, and motor cortical areas—including the amygdala, ventral striatum, MPFC, SMA, and STG.

AOU is particularly sensitive to:

\begin{itemize}
    \item Tonal stability (linked to positive valence)
    \item Spectral balance and consonance (linked to emotional reward)
    \item Tempo and rhythmic complexity (linked to arousal and motor drive)
\end{itemize}

\subsubsection*{Mathematical Representation}

AOU is computed as a weighted combination of two core Nodes:

\[
\text{AOU}(t) = w_1 \cdot \text{Valence}(t) + w_2 \cdot \text{Arousal}(t)
\]

Each of which is decomposed into signal-bearing Elements:

\textbf{Valence Axis}:

\[
\text{Valence}(t) = v_1 \cdot \text{TonalStability}(t) + v_2 \cdot \text{SpectralBalance}(t) + v_3 \cdot \text{HarmonicConsonance}(t)
\]

\textbf{Arousal Axis}:

\[
\text{Arousal}(t) = a_1 \cdot \text{Tempo}(t) + a_2 \cdot \text{SpectralFlux}(t) + a_3 \cdot \text{RhythmicComplexity}(t)
\]

All elements are normalized between 0–1, with weights derived from empirical affective neuroscience research.

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Valence}

\begin{itemize}
    \item \textbf{Function}: Detects emotional polarity of the musical input
    \item EEG Alpha Asymmetry in MPFC correlates with valence level
    \item Gamma and BOLD activation in amygdala and ventral striatum correlate with emotional reward
\end{itemize}

\textbf{�� Element: Tonal Stability}

\begin{itemize}
    \item Region: MPFC
    \item EEG Marker: Alpha asymmetry
    \item MNI: [+6, +52, +10]
    \item Citation: Zatorre \& Halpern, 2005
\end{itemize}

\textbf{�� Element: Spectral Balance}

\begin{itemize}
    \item Region: Amygdala, Insula
    \item Method: fMRI + EEG Gamma
    \item MNI: [-20, 0, -12]
    \item Citation: Koelsch, 2011
\end{itemize}

\textbf{�� Element: Harmonic Consonance}

\begin{itemize}
    \item Region: Ventral Striatum, NAcc
    \item Method: fMRI
    \item MNI: [+10, +8, -10]
    \item Citation: Salimpoor et al., 2011
\end{itemize}

\paragraph{�� Node: Arousal}

\begin{itemize}
    \item \textbf{Function}: Captures intensity and activation driven by musical rhythm, tempo, and flux
\end{itemize}

\textbf{�� Element: Tempo Dynamics}

\begin{itemize}
    \item Region: Motor Cortex
    \item EEG: Beta-band amplitude
    \item MNI: [+40, -10, +60]
    \item Citation: Janata et al., 2009
\end{itemize}

\textbf{�� Element: Spectral Flux}

\begin{itemize}
    \item Region: STG
    \item EEG: Theta
    \item MNI: [+50, +10, -6]
    \item Citation: Alluri et al., 2012
\end{itemize}

\textbf{�� Element: Rhythmic Complexity}

\begin{itemize}
    \item Region: SMA
    \item EEG/fMRI: Beta + BOLD
    \item MNI: [+6, -6, +70]
    \item Citation: Chen et al., 2008
\end{itemize}

\subsubsection*{GlassBrainMap Coordinates}

\begin{center}
\begin{tabular}{|l|c|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Citation} \\
\hline
Tonal Stability & [+6, +52, +10] & MPFC & Zatorre \& Halpern (2005) \\
Spectral Balance & [-20, 0, -12] & Amygdala, Insula & Koelsch (2011) \\
Harmonic Conson. & [+10, +8, -10] & Ventral Striatum & Salimpoor et al. (2011) \\
Tempo Dynamics & [+40, -10, +60] & Motor Cortex & Janata (2009) \\
Spectral Flux & [+50, +10, -6] & STG & Alluri et al. (2012) \\
Rhythmic Comp. & [+6, -6, +70] & SMA & Chen (2008) \\
\hline
\end{tabular}
\end{center}

These entries are directly mapped into the GlassBrain SVG and visualized in the \texttt{GlassBrain.jsx} component as colored resonance hotspots.

\subsubsection*{Functional Summary}

The AOU unit is the affective engine of the C³ system. It accounts for the emotional tone of musical events using neurobiologically validated metrics. It plays a key role in:

\begin{itemize}
    \item Differentiating emotionally positive/negative musical content
    \item Identifying peaks of arousal or relaxation
    \item Guiding adaptive audio systems (e.g., emotion-matching playlists, AI composition targeting affective impact)
\end{itemize}

It interacts heavily with:

\begin{itemize}
    \item CTU (tension)
    \item PIU (flow state)
    \item RSU (integrated resonance summary)
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Affective tagging in music libraries (real-time emotional metadata)
    \item Neurofeedback therapy for emotional regulation
    \item Real-time composition tools for mood shaping and expressive calibration
\end{itemize}

\subsection*{3.1.3 IEU – Intuitive Expectation Unit}

\textbf{Models predictive listening and mismatch responses based on harmonic, rhythmic, and melodic entropy deviations.}

\subsubsection*{Overview}

The Intuitive Expectation Unit (IEU) simulates the listener’s internal predictive model during music listening. It monitors how the brain anticipates musical structure and reacts to violations of those expectations. This encompasses both pre-conscious responses (e.g., MMN) and attended violations (e.g., P300), as well as uncertainty metrics (e.g., melodic entropy).

IEU reflects a foundational principle of musical cognition: expectation and surprise are primary drivers of attention, emotion, and memory. The system quantifies these dynamics through:

\begin{itemize}
    \item Early prediction-error signals (EEG MMN in STG)
    \item P300 ERP responses in premotor regions
    \item Neural tracking of melodic uncertainty (entropy measures in dACC and amygdala)
\end{itemize}

\subsubsection*{Mathematical Structure}

IEU operates via three primary Nodes, weighted and combined over time:

\[
\text{IEU}(t) = w_1 \cdot \text{HarmonicViolation}(t) + w_2 \cdot \text{RhythmicViolation}(t) + w_3 \cdot \text{MelodicEntropy}(t)
\]

Each Node computes one functional aspect of expectation processing:

\begin{itemize}
    \item $w_1$: Surprise in harmony
    \item $w_2$: Surprise in rhythm
    \item $w_3$: Global uncertainty in melodic information
\end{itemize}

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Harmonic Expectation Violation}

\begin{itemize}
    \item \textbf{Function}: Detects dissonant or out-of-key chords in a tonal context
    \item \textbf{EEG Marker}: MMN ERP (mismatch negativity)
    \item \textbf{Region}: STG, auditory cortex
    \item \textbf{Citation}: Crespo-Bojorque et al., 2018
\end{itemize}

\textbf{�� Element: MMN Response (Fronto-central)}

\begin{itemize}
    \item EEG Sites: Fz, Cz
    \item MNI: [0, +50, +20]
    \item Type: ERP (automatic deviance detection)
\end{itemize}

\paragraph{�� Node: Rhythmic Expectation Violation}

\begin{itemize}
    \item \textbf{Function}: Identifies tempo/beat violations and rhythmic anomalies
    \item \textbf{EEG Marker}: P300 ERP (conscious deviance)
    \item \textbf{Region}: SMA, Motor Cortex
    \item \textbf{Citation}: Schön et al., 2005
\end{itemize}

\textbf{�� Element: P300 Response (Motor ERP)}

\begin{itemize}
    \item MNI: [+10, -10, +60]
    \item Type: ERP (attended violation detection)
\end{itemize}

\paragraph{�� Node: Melodic Entropy}

\begin{itemize}
    \item \textbf{Function}: Measures statistical uncertainty in melodic sequences
    \item \textbf{EEG Marker}: Alpha/theta shift in medial regions
    \item \textbf{fMRI}: dACC, amygdala
    \item \textbf{Citation}: Koelsch, 2008
\end{itemize}

\textbf{�� Element: Information Content (Entropy)}

\begin{itemize}
    \item MNI: [+4, +18, +26]
    \item Metric: Entropy of pitch sequences (Shannon index, Markov chain predictability)
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Method} & \textbf{Citation} \\
\hline
Harmonic Violation & [0, +50, +20] & STG, Fronto-central & EEG (MMN) & Crespo-Bojorque et al., 2018 \\
Rhythmic Violation & [+10, -10, +60] & SMA, Motor Cortex & EEG (P300) & Schön et al., 2005 \\
Melodic Entropy & [+4, +18, +26] & dACC, Amygdala & EEG ($\alpha$/$\theta$), fMRI & Koelsch, 2008 \\
\hline
\end{tabular}
\end{center}

These coordinates are rendered dynamically in the GlassBrain system and updated in real time as IEU activity changes.

\subsubsection*{Functional Summary}

IEU measures how expected a musical moment is and how the brain responds to the unexpected. When expectation is fulfilled, IEU output remains low. When it is violated, resonance spikes occur—often triggering cognitive reorientation or emotional reappraisal.

IEU is crucial for:

\begin{itemize}
    \item Segmenting musical flow
    \item Signaling novelty or structural change
    \item Synchronizing listener attention to surprise events
\end{itemize}

It interacts strongly with:

\begin{itemize}
    \item CTU (tension under surprise)
    \item SAU (memory under uncertainty)
    \item AOU (emotional impact of surprise)
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Adaptive AI music systems: Dynamically vary predictability to hold listener attention
    \item Neuroeducation: Track student surprise and engagement in music learning
    \item Therapeutic design: Gradually increase predictability to rebuild trust in rhythm/melody recognition
\end{itemize}

\subsection*{3.1.4 SRU – Somatic Resonance Unit}

\textbf{Models rhythmic motor entrainment and body-based synchronization to musical stimuli.}

\subsubsection*{Overview}

The Somatic Resonance Unit (SRU) models how the brain's motor and premotor systems synchronize with perceived musical rhythm. It captures entrainment phenomena in motor cortices, pulse clarity processing, and tempo stability detection, which form the physiological basis for movement, tapping, dancing, and temporal prediction during music listening.

This Unit operates on the principle that rhythmic structures entrain cortical beta-band oscillations, and that clear pulse and metric structures elicit synchronized activity in:

\begin{itemize}
    \item Supplementary Motor Area (SMA)
    \item Premotor Cortex (PMC)
    \item Basal Ganglia (Putamen)
    \item Cerebellum
\end{itemize}

SRU plays a crucial role in connecting auditory input to bodily response via motor entrainment and has direct applications in rehabilitation, rhythm training, and movement–based therapies.

\subsubsection*{Mathematical Model}

SRU is defined by a linear combination of three functional Nodes:

\[
\text{SRU}(t) = s_1 \cdot \text{PulseClarity}(t) + s_2 \cdot \text{MetricStability}(t) + s_3 \cdot \text{TempoStability}(t)
\]

Where:

\begin{itemize}
    \item $s_1, s_2, s_3$: weight coefficients derived from neural effect size or application-specific relevance
\end{itemize}

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Pulse Clarity}

\begin{itemize}
    \item \textbf{Function}: Detects the salience of rhythmic beat and its effect on motor cortex
    \item \textbf{EEG Marker}: Beta amplitude increase
    \item \textbf{Regions}: Motor Cortex, Putamen, Cerebellum
    \item \textbf{Citation}: Fujioka et al., 2012
\end{itemize}

\textbf{�� Element: Beat Salience}

\begin{itemize}
    \item MNI: [+20, -10, +60]
    \item EEG: $\beta$-band power at C3/Cz
    \item fMRI: BOLD in cerebellar vermis and PMC
\end{itemize}

\paragraph{�� Node: Metric Stability}

\begin{itemize}
    \item \textbf{Function}: Represents the regularity and predictability of rhythmic subdivisions
    \item \textbf{EEG Marker}: Beta phase-locking
    \item \textbf{Regions}: SMA, Premotor Cortex
    \item \textbf{Citation}: Chen et al., 2008
\end{itemize}

\textbf{�� Element: Metric Regularity}

\begin{itemize}
    \item MNI: [+6, +4, +64]
    \item EEG: $\beta$ phase coherence
    \item fMRI: SMA BOLD activation
\end{itemize}

\paragraph{�� Node: Tempo Stability}

\begin{itemize}
    \item \textbf{Function}: Measures consistency in tempo; correlates with sensorimotor coupling strength
    \item \textbf{EEG Marker}: Interregional beta coherence
    \item \textbf{Regions}: Putamen, PMC
    \item \textbf{Citation}: Thaut et al., 2015
\end{itemize}

\textbf{�� Element: Temporal Consistency}

\begin{itemize}
    \item MNI: [+28, -12, +60]
    \item EEG: $\beta$ coherence (PMC $\leftrightarrow$ Basal Ganglia)
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Method} & \textbf{Citation} \\
\hline
Pulse Clarity & [+20, -10, +60] & Motor Cortex, Putamen & EEG ($\beta$), fMRI & Fujioka et al., 2012 \\
Metric Stability & [+6, +4, +64] & SMA, Premotor Cortex & EEG phase-locking & Chen et al., 2008 \\
Tempo Stability & [+28, -12, +60] & PMC, Basal Ganglia & EEG coherence & Thaut et al., 2015 \\
\hline
\end{tabular}
\end{center}

These coordinates are used in the interactive GlassBrainMap, providing anatomical precision for motor–rhythmic coupling during music listening.

\subsubsection*{Functional Summary}

SRU tracks real-time neural entrainment of the motor system to rhythmic music. It reflects how the body prepares to move, taps to the beat, and predicts upcoming rhythmic events. Its signal rises with:

\begin{itemize}
    \item Stable and predictable beats
    \item High beat salience and metric clarity
    \item Entraining pulse structures (e.g., groove, swing, syncopation)
\end{itemize}

It often correlates positively with:

\begin{itemize}
    \item PIU (immersion through movement)
    \item AOU (arousal from rhythm)
    \item NSU (neural synchronization)
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Motor rehabilitation and rhythm therapy
    \item Groove detection algorithms in music information retrieval
    \item Neurophysiological metrics of musical engagement
\end{itemize}

SRU is particularly relevant for dance research, tempo training, and interactive AI music generation that responds to bodily input or encourages physical engagement.

\subsection*{3.1.5 SAU – Semantic–Autobiographical Unit}

\textbf{Models music-evoked autobiographical memory and semantic association.}

\subsubsection*{Overview}

The Semantic–Autobiographical Unit (SAU) captures the interaction between musical structure and episodic/semantic memory systems in the brain. It quantifies how music activates autobiographical recall and semantic meaning through hippocampal–prefrontal–limbic networks.

This Unit models three core processes:

\begin{itemize}
    \item Melodic repetition and its role in memory cueing
    \item Tonal familiarity and its effect on semantic access
    \item Timbre recognition and affective memory tagging
\end{itemize}

SAU is essential for explaining why certain music evokes specific personal memories, how musical familiarity shapes identity, and how past experiences influence present perception.

\subsubsection*{Mathematical Model}

SAU is calculated as:

\[
\text{SAU}(t) = s_1 \cdot \text{MotifRecurrence}(t) + s_2 \cdot \text{TonalityRecall}(t) + s_3 \cdot \text{TimbreFamiliarity}(t)
\]

Each Node maps to a neuroanatomically distinct memory-processing mechanism and contributes to the system’s representation of semantic–episodic resonance.

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Motif Recurrence}

\begin{itemize}
    \item \textbf{Function}: Detects melodic repetitions as episodic memory cues
    \item \textbf{EEG Marker}: Theta increase
    \item \textbf{Regions}: Hippocampus, Parahippocampal Gyrus
    \item \textbf{Citation}: Foss et al., 2007
\end{itemize}

\textbf{�� Element: Melodic Repetition}

\begin{itemize}
    \item MNI: [–24, –40, –8]
    \item Method: EEG $\theta$; fMRI hippocampal BOLD
    \item Description: Increased theta in temporal–limbic regions during reoccurring phrases
\end{itemize}

\paragraph{�� Node: Tonality Recall}

\begin{itemize}
    \item \textbf{Function}: Activates stored tonal patterns and schemas
    \item \textbf{EEG Marker}: Alpha increase
    \item \textbf{Regions}: MPFC, ACC
    \item \textbf{Citation}: Foss et al., 2007
\end{itemize}

\textbf{�� Element: Tonal Familiarity}

\begin{itemize}
    \item MNI: [+6, +48, +8]
    \item Method: EEG $\alpha$; MPFC BOLD
    \item Description: Retrieval of culturally learned tonality (major/minor schemas)
\end{itemize}

\paragraph{�� Node: Timbre Familiarity}

\begin{itemize}
    \item \textbf{Function}: Associates sound qualities with emotional memory
    \item \textbf{EEG Marker}: Gamma increase
    \item \textbf{Regions}: Amygdala, STG
    \item \textbf{Citation}: Chen et al., 2008
\end{itemize}

\textbf{�� Element: Timbre-based Memory}

\begin{itemize}
    \item MNI: [+22, +0, –20]
    \item Method: EEG $\gamma$; STG BOLD
    \item Description: Recognition of familiar instrument types triggers affective memory
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Method} & \textbf{Citation} \\
\hline
Melodic Repetition & [–24, –40, –8] & Hippocampus, ParaHC & EEG $\theta$, fMRI & Foss et al., 2007 \\
Tonal Familiarity & [+6, +48, +8] & MPFC, ACC & EEG $\alpha$, fMRI & Foss et al., 2007 \\
Timbre-based Memory & [+22, +0, –20] & Amygdala, STG & EEG $\gamma$, fMRI & Chen et al., 2008 \\
\hline
\end{tabular}
\end{center}

These brain regions are anatomically mapped and visualized within the GlassBrain system in real time.

\subsubsection*{Functional Summary}

SAU is the mnemonic layer of the C³ system. It provides a biologically grounded mechanism for modeling:

\begin{itemize}
    \item Musical nostalgia
    \item Identity-based music perception
    \item Semantic resonance of culturally or personally meaningful sounds
\end{itemize}

Its resonance increases with:

\begin{itemize}
    \item Repetition of previously heard motifs
    \item Use of familiar tonal centers
    \item Recognition of personally associated timbres (e.g., piano from childhood)
\end{itemize}

SAU often co-activates with:

\begin{itemize}
    \item AOU (affective salience)
    \item IEU (surprise + recall)
    \item IRU (interpersonal resonance and memory convergence)
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Music therapy for trauma, memory loss, and dementia
    \item AI playlist personalization based on listener history
    \item Autobiographical score composition using musical memories as structure
\end{itemize}

SAU also supports cultural modeling—how exposure to tonal systems and timbral norms shapes perceptual identity and long-term memory.

\subsection*{3.1.6 PIU – Phenomenological Immersion Unit}

\textbf{Models musical flow, absorption, and deep attention states through frontal, parietal, and default mode network dynamics.}

\subsubsection*{Overview}

The Phenomenological Immersion Unit (PIU) models non-analytical, affectively immersive listening states where attention, cognition, and motor inhibition converge to produce a sense of musical flow. These states often coincide with:

\begin{itemize}
    \item Temporal suspension (loss of time awareness)
    \item Suppression of cognitive self-monitoring (reduced DMN activity)
    \item Heightened sensory and affective clarity
\end{itemize}

PIU tracks the transition from conscious processing to immersive resonance, particularly during:

\begin{itemize}
    \item Slowly evolving harmonic textures
    \item Minimalistic repetition
    \item Ambient or timbrally complex soundscapes
    \item Trance, ritual, or meditative musics
\end{itemize}

It is one of the most non-linear and affective Units in the system.

\subsubsection*{Mathematical Structure}

PIU is computed as a blend of three neurodynamic constructs:

\[
\text{PIU}(t) = p_1 \cdot \text{AttentionalSalience}(t) + p_2 \cdot \text{FlowState}(t) + p_3 \cdot \text{TransientClarity}(t)
\]

Each Node corresponds to a specific neural configuration observed during immersive listening or musical flow:

\begin{itemize}
    \item $p_1$: attention gain
    \item $p_2$: default-mode suppression
    \item $p_3$: temporal resolution and transition anchoring
\end{itemize}

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Attentional Salience}

\begin{itemize}
    \item \textbf{Function}: Measures the emergence of sustained, high-focus listening
    \item \textbf{EEG Marker}: Gamma-band elevation
    \item \textbf{Regions}: ACC, Frontal-Parietal Network
    \item \textbf{Citation}: Santoyo et al., 2023
\end{itemize}

\textbf{�� Element: EEG Gamma Activation}

\begin{itemize}
    \item MNI: [+4, +32, +24]
    \item Description: Increased $\gamma$ during perceptual focusing (timbre-based or harmonic absorption)
\end{itemize}

\paragraph{�� Node: Flow State Index}

\begin{itemize}
    \item \textbf{Function}: Tracks mental state transitions into immersive flow
    \item \textbf{EEG Marker}: Alpha suppression; BOLD reduction in DMN
    \item \textbf{Regions}: DMN hubs (mPFC, PCC); pre-SMA
    \item \textbf{Citation}: Patterson et al., 2002
\end{itemize}

\textbf{�� Element: Default Mode Network Suppression}

\begin{itemize}
    \item MNI: [+2, +50, +6]
    \item Description: Drop in self-referential processing associated with absorption
\end{itemize}

\paragraph{�� Node: Transient Clarity}

\begin{itemize}
    \item \textbf{Function}: Anchors transitions (e.g., chord change, pulse entrance)
    \item \textbf{EEG Marker}: Beta phase-locking
    \item \textbf{Regions}: Frontal–Parietal Network
    \item \textbf{Citation}: Nozaradan et al., 2012
\end{itemize}

\textbf{�� Element: Beta Phase Locking}

\begin{itemize}
    \item MNI: [+20, +10, +64]
    \item Description: Transition clarity in EEG $\beta$-phase alignment
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{EEG/fMRI Marker} & \textbf{Citation} \\
\hline
Attentional Salience & [+4, +32, +24] & ACC, FP Network & EEG $\gamma$ & Santoyo et al., 2023 \\
Flow State & [+2, +50, +6] & mPFC, DMN & EEG $\alpha \downarrow$, fMRI DMN BOLD $\downarrow$ & Patterson et al., 2002 \\
Transient Clarity & [+20, +10, +64] & FP Network & EEG $\beta$ phase-lock & Nozaradan et al., 2012 \\
\hline
\end{tabular}
\end{center}

These entries are integrated into GlassBrain SVG and shown in immersive resonance clusters.

\subsubsection*{Functional Summary}

PIU is the immersive dimension of C³. It increases during:

\begin{itemize}
    \item Deep listening
    \item Meditative or minimalistic music
    \item Non-verbal sound attention
    \item Flow-inducing music (e.g., ambient, trance, sacred chants)
\end{itemize}

PIU typically correlates with:

\begin{itemize}
    \item $\downarrow$ CTU (reduced tension)
    \item $\uparrow$ AOU (emotional absorption)
    \item $\uparrow$ SAU (memory/identity resonance)
    \item $\uparrow$ IRU (shared flow in group listening)
\end{itemize}

The PIU trace reflects how long a listener is “lost in the music.”

\subsubsection*{Applications}

\begin{itemize}
    \item Therapeutic music for anxiety, depression, PTSD
    \item Flow design in composition and sound installations
    \item Biometric scoring of listener engagement and trance induction
\end{itemize}

In real-time systems, PIU can be used as a threshold trigger to adjust musical pacing, harmonic density, or lyrical clarity, enhancing sustained immersion.

\subsection*{3.1.7 IRU – Interpersonal Resonance Unit}

\textbf{Models neural synchrony, emotional convergence, and inter-brain coherence during shared musical experiences.}

\subsubsection*{Overview}

The Interpersonal Resonance Unit (IRU) measures the shared neural and affective dynamics that arise when music is experienced collectively. This Unit is founded on the principles of:

\begin{itemize}
    \item Inter-brain coherence (neural synchrony across listeners)
    \item Emotional contagion via acoustic affect cues
    \item Social synchronization of motor and affective circuits during group musical experiences
\end{itemize}

IRU is based on emerging research from hyperscanning EEG, dual-fMRI, and social-cognitive neuroscience, which shows that synchronous music listening, especially in emotionally charged contexts, causes measurable co-activation of limbic, temporal, and frontal regions across brains.

\subsubsection*{Mathematical Structure}

IRU is computed as a weighted average of three social-cognitive resonance Nodes:

\[
\text{IRU}(t) = r_1 \cdot \text{InterBrainCoherence}(t) + r_2 \cdot \text{SocialSynchrony}(t) + r_3 \cdot \text{EmotionalResonance}(t)
\]

Each Node represents a distinct interpersonal neural mechanism triggered by collective music engagement.

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Inter-Brain Coherence}

\begin{itemize}
    \item \textbf{Function}: Measures synchronized alpha/theta rhythms between participants
    \item \textbf{EEG Marker}: Cross-brain phase-locking
    \item \textbf{Regions}: Frontal Cortex, Superior Temporal Gyrus (STG)
    \item \textbf{Citation}: Wallmark et al., 2018
\end{itemize}

\textbf{�� Element: EEG Hyperscanning Coherence}

\begin{itemize}
    \item MNI: [0, +52, +14]
    \item Metric: Phase coherence across listener dyads ($\alpha$/$\theta$ bands)
    \item Description: Increased alignment in neural oscillations during co-listening
\end{itemize}

\paragraph{�� Node: Social Synchrony}

\begin{itemize}
    \item \textbf{Function}: Captures joint activation in movement and attention networks
    \item \textbf{EEG Marker}: Gamma amplitude increases during joint attention
    \item \textbf{Regions}: Frontal Cortex, STG
    \item \textbf{Citation}: Wallmark et al., 2018
\end{itemize}

\textbf{�� Element: Frontal-Temporal Activation}

\begin{itemize}
    \item MNI: [+12, +42, +14]
    \item EEG: $\gamma$ power co-fluctuations in listeners
    \item Description: Mirrors shared attention and gesture during music
\end{itemize}

\paragraph{�� Node: Emotional Resonance}

\begin{itemize}
    \item \textbf{Function}: Models shared emotional responses via limbic system coupling
    \item \textbf{EEG/fMRI Marker}: Alpha asymmetry; amygdala BOLD
    \item \textbf{Regions}: Amygdala, ACC
    \item \textbf{Citation}: Yang et al., 2025
\end{itemize}

\textbf{�� Element: Limbic Activation}

\begin{itemize}
    \item MNI: [+8, +6, -10]
    \item Description: Shared peaks in arousal/valence across participants
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Method} & \textbf{Citation} \\
\hline
Inter-Brain Coherence & [0, +52, +14] & Frontal + STG & EEG $\alpha$/$\theta$ Sync & Wallmark et al., 2018 \\
Social Synchrony & [+12, +42, +14] & Frontal + STG & EEG $\gamma$ & Wallmark et al., 2018 \\
Emotional Resonance & [+8, +6, -10] & Amygdala + ACC & fMRI + EEG $\alpha$ & Yang et al., 2025 \\
\hline
\end{tabular}
\end{center}

These elements are rendered together in a special Group-Level GlassBrain Overlay, enabling visualization of multi-brain synchrony fields.

\subsubsection*{Functional Summary}

IRU allows C³ to model music not just as an internal experience, but as a shared cognitive–emotional field. IRU increases when:

\begin{itemize}
    \item Music is heard in a social context
    \item Listeners share a history or cultural framework
    \item Body-based synchronization (e.g., group clapping, dancing) is present
    \item Emotional peaks align across individuals
\end{itemize}

IRU is the bridge between:

\begin{itemize}
    \item AOU (individual affect)
    \item PIU (flow)
    \item RSU (integrated group resonance)
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Social music therapy (e.g., group rhythm interventions, trauma re-integration)
    \item AI-driven shared music experiences (co-listening apps, social playlist engines)
    \item Group neuroscience and emotion regulation training
\end{itemize}

IRU enables the modeling of shared emotional spaces and neural entrainment ecosystems, placing music at the center of group cognition.

\subsection*{3.1.8 NSU – Neural Synchronization Unit}

\textbf{Models neural entrainment, phase-locking, and large-scale temporal coherence in auditory–motor–cognitive systems.}

\subsubsection*{Overview}

The Neural Synchronization Unit (NSU) captures how music organizes and synchronizes brain activity through rhythmic and spectral structure. It operates on the principle that musical events—especially rhythmic periodicities, spectral regularities, and melodic contours—can entrain large-scale cortical networks via:

\begin{itemize}
    \item Phase-locking to external rhythms
    \item Cross-regional coherence in gamma, beta, and alpha bands
    \item Temporal anticipation and predictive resonance
\end{itemize}

NSU integrates empirical findings from EEG, MEG, and frequency–following response (FFR) studies that show how temporal structure in sound becomes mirrored in neural timing.

It is particularly relevant in contexts of:

\begin{itemize}
    \item Rhythm perception and pulse tracking
    \item Sensory–motor coupling
    \item Beat-based learning and coordination
    \item Tonal phase tracking and attentional alignment
\end{itemize}

\subsubsection*{Mathematical Model}

NSU is computed as:

\[
\text{NSU}(t) = n_1 \cdot \text{GammaCoherence}(t) + n_2 \cdot \text{BetaPhaseLocking}(t) + n_3 \cdot \text{AlphaSynchrony}(t)
\]

Each Node aggregates multiple regional signals reflecting cross-frequency and cross-site temporal alignment.

\subsubsection*{Nodes and Elements}

\paragraph{�� Node: Gamma Coherence}

\begin{itemize}
    \item \textbf{Function}: Tracks gamma-band synchrony between auditory and motor cortices
    \item \textbf{EEG Marker}: $\gamma$ coherence (STG $\leftrightarrow$ Motor Cortex)
    \item \textbf{Regions}: STG, Motor Cortex
    \item \textbf{Citation}: Bidelman \& Heinz, 2011
\end{itemize}

\textbf{�� Element: Gamma-band Synchrony}

\begin{itemize}
    \item MNI: [+38, -10, +52]
    \item EEG: $\gamma$-band inter-site phase clustering
    \item Metric: Phase-locking value (PLV)
\end{itemize}

\paragraph{�� Node: Beta Phase Locking}

\begin{itemize}
    \item \textbf{Function}: Models cortical beta-band entrainment in motor sequencing and prediction
    \item \textbf{EEG Marker}: $\beta$ phase locking (SMA $\leftrightarrow$ Basal Ganglia)
    \item \textbf{Regions}: SMA, PMC, Basal Ganglia, STG
    \item \textbf{Citation}: Bidelman \& Krishnan, 2009
\end{itemize}

\textbf{�� Element: Motor-Auditory Integration}

\begin{itemize}
    \item MNI: [+8, -6, +60]
    \item EEG: Inter-site $\beta$ PLV across frontal–motor nodes
\end{itemize}

\paragraph{�� Node: Alpha Synchrony}

\begin{itemize}
    \item \textbf{Function}: Measures large-scale attentional coherence in alpha network
    \item \textbf{EEG Marker}: $\alpha$ interhemispheric synchrony
    \item \textbf{Regions}: Frontal Cortex, Parietal Cortex
    \item \textbf{Citation}: Strait et al., 2012
\end{itemize}

\textbf{�� Element: Frontal–Parietal Alpha Synchrony}

\begin{itemize}
    \item MNI: [+4, +52, +10]
    \item EEG: $\alpha$ band coherence (fronto-parietal loop)
    \item Description: Indicator of global attention tuning to musical flow
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Method} & \textbf{Citation} \\
\hline
Gamma Coherence & [+38, -10, +52] & STG, Motor Cortex & EEG $\gamma$ PLV & Bidelman \& Heinz, 2011 \\
Beta Phase Locking & [+8, -6, +60] & SMA, Basal Ganglia & EEG $\beta$ PLV & Bidelman \& Krishnan, 2009 \\
Alpha Synchrony & [+4, +52, +10] & Frontal–Parietal Cortex & EEG $\alpha$ Coherence & Strait et al., 2012 \\
\hline
\end{tabular}
\end{center}

These nodes are rendered in the GlassBrain overlay, showing synchronization fields in time-resolved layers.

\subsubsection*{Functional Summary}

NSU tracks how temporally structured sound creates temporally structured brain activity.

It reflects:

\begin{itemize}
    \item High-frequency ($\gamma$) micro-entrainment for precise timing
    \item Mid-frequency ($\beta$) entrainment for motor planning
    \item Low-frequency ($\alpha$) coherence for attentional binding
\end{itemize}

NSU is crucial for:

\begin{itemize}
    \item Coordinated movement to music
    \item Pulse perception
    \item Learning of rhythmic and metrical structure
    \item Sustained attention
\end{itemize}

It interacts strongly with:

\begin{itemize}
    \item SRU (somatic resonance)
    \item IEU (predictive modeling)
    \item RSU (resonance integration)
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Neuroeducation for rhythm training, beat perception, language timing
    \item Therapeutic rhythm protocols (e.g., Parkinson’s interventions)
    \item Real-time music–brain feedback in composition and AI systems
\end{itemize}

NSU gives C³ the neural infrastructure to temporally align listener state with musical structure—turning sound into synchronization.

\subsection*{3.1.9 RSU – Resonance Synthesis Unit}

\textbf{Integrates all Unit-level outputs into a unified cognitive–emotional–motor resonance profile.}

\subsubsection*{Overview}

The Resonance Synthesis Unit (RSU) is the final computational layer in the C³ system. Unlike the other eight Units, which represent specific cognitive functions (e.g., tension, emotion, memory), RSU performs a global integration of all underlying Units, Nodes, and Elements to compute:

\begin{itemize}
    \item Total moment-to-moment cognitive resonance
    \item System-wide synchrony and coherence
    \item Weighted convergence of multi-dimensional neural states
\end{itemize}

RSU does not introduce new raw data. Rather, it serves as a nonlinear summarization node—a temporal and structural resonance integrator—combining emotion, memory, expectation, attention, motor entrainment, and inter-brain synchrony into a single multidimensional vector.

\subsubsection*{Mathematical Formulation}

RSU operates across two primary mathematical layers:

\paragraph{a. Unified Coherence Score (UCS)}

\[
\text{UCS}(t) = \frac{1}{9} \sum_{i=1}^{9} C^3_i(t)
\]

Where:

\begin{itemize}
    \item $C^3_i(t)$: The computed resonance from each Unit at time $t$
\end{itemize}

\textbf{Output:} Scalar between $[0,1]$ representing total network activation.\\
This value can be interpreted as a resonance index—how "fully activated" the cognitive-musical system is.

\paragraph{b. Weighted Network Fusion (WNF)}

\[
\text{RSU}(t) = \mathbf{W} \cdot \mathbf{C}^3(t)
\]

Where:

\[
\mathbf{C}^3(t) = [\text{CTU}(t), \text{AOU}(t), \ldots, \text{NSU}(t)]^T \quad \mathbf{W} \in \mathbb{R}^{1 \times 9}
\]

$\mathbf{W}$: Application-specific or data-derived weights (e.g., in therapy, PIU and SAU may be up-weighted)

This produces a 1D or N-dimensional fusion vector depending on context (e.g., therapy, composition, neuroscience modeling).

\subsubsection*{Nodes and Elements}

RSU consists of three conceptual Nodes, each acting as a functional lens on the total C³ state.

\paragraph{�� Node: Unified Coherence}

\begin{itemize}
    \item \textbf{Function}: Measures cross-Unit synchrony at each time slice
    \item \textbf{Metric}: Multi-band EEG synchrony
    \item \textbf{Regions}: ACC, PCC, STG, Frontal Cortex
    \item \textbf{Citation}: Yang et al., 2025
\end{itemize}

\textbf{�� Element: Network-Wide EEG Coherence}

\begin{itemize}
    \item MNI: [+8, +44, +12]
    \item Signal: PLV across Unit-critical regions
    \item Description: Degree to which separate Units exhibit synchronous resonance
\end{itemize}

\paragraph{�� Node: Consonance Clarity}

\begin{itemize}
    \item \textbf{Function}: Aggregates emotional, tonal, and spectral harmony into a single perceptual clarity index
    \item \textbf{Metric}: Consonance fusion index
    \item \textbf{Regions}: NAcc, Amygdala, Broca, PMC
    \item \textbf{Citation}: Salimpoor et al., 2011
\end{itemize}

\textbf{�� Element: Resonant Fusion Metric}

\begin{itemize}
    \item MNI: [+12, +10, -10]
    \item Description: Integrated emotional–harmonic salience field
    \item Method: Entropy-weighted BOLD + EEG synchrony sum
\end{itemize}

\paragraph{�� Node: Network Fusion}

\begin{itemize}
    \item \textbf{Function}: Computes total resonance field from all C³ Units
    \item \textbf{Metric}: Cross-unit vector field
    \item \textbf{Regions}: DMN, Sensorimotor Network, Limbic System
    \item \textbf{Citation}: SRC⁹ Master Report
\end{itemize}

\textbf{�� Element: Cross-Unit Weighted Synthesis}

\begin{itemize}
    \item MNI: N/A (meta-region spanning all prior Units)
    \item Description: Final vector representation of listener state
    \item Output: Dynamic resonance map
\end{itemize}

\subsubsection*{GlassBrainMap Anchors}

\begin{center}
\begin{tabular}{|l|c|l|l|l|}
\hline
\textbf{Node} & \textbf{MNI Coordinates} & \textbf{Region} & \textbf{Method} & \textbf{Citation} \\
\hline
Unified Coherence & [+8, +44, +12] & ACC, STG, PCC, Frontal & EEG Multi-Band Coherence & Yang et al., 2025 \\
Consonance Clarity & [+12, +10, -10] & NAcc, Broca, Amygdala, PMC & fMRI + EEG Fusion & Salimpoor et al., 2011 \\
Network Fusion & — & Multi-unit Meta Layer & All prior Unit inputs & SRC⁹ Report (2025) \\
\hline
\end{tabular}
\end{center}

These form the top-level projection in the Resonance Map, a composite data structure exported per listener or per musical piece.

\subsubsection*{Functional Summary}

RSU enables system-wide monitoring and integrated decision-making:

\begin{itemize}
    \item Real-time tracking of full cognitive-emotional resonance
    \item Personalized resonance profile calculation
    \item Macro-temporal analysis (e.g., identifying resonance arcs over time)
\end{itemize}

It often acts as a target state:

\begin{itemize}
    \item For adaptive AI generation
    \item For guided composition
    \item For neurofeedback therapy
\end{itemize}

\subsubsection*{Applications}

\begin{itemize}
    \item Therapeutic optimization: Detect optimal convergence of memory, attention, and emotion
    \item Neuro-symbolic composition: Use RSU trajectory to sculpt musical form
    \item Resonance fingerprinting: Build listener resonance signatures for adaptive playlists
\end{itemize}

RSU is the endpoint and also the summary interface of the C³ system. It translates complex internal dynamics into usable, visible, and actionable outputs.

\section*{III.2 Node and Element Architecture}

\textit{How are Nodes and Elements structured, connected, and activated in C³?}

\subsection*{3.2.1 Hierarchical Composition: UNIT $\rightarrow$ NODE $\rightarrow$ ELEMENT}

C³ operates on a three-tiered architecture, in which each UNIT is subdivided into Nodes, and each Node consists of one or more Elements.

This design allows C³ to represent cognition at multiple resolutions:

\begin{center}
\begin{tabular}{|c|l|c|}
\hline
\textbf{Level} & \textbf{Function} & \textbf{Quantity} \\
\hline
UNIT & Macro cognitive subsystem (e.g., Emotion, Memory) & 9 \\
NODE & Functional construct (e.g., Flow, Expectation) & 3–4/unit \\
ELEMENT & Measurable neural signal (EEG, fMRI, MEG) & 2–5/node \\
\hline
\end{tabular}
\end{center}

Each Element is traceable to:

\begin{itemize}
    \item A neurophysiological method (e.g., EEG phase coherence, ERP, fMRI BOLD)
    \item A brain region (MNI coordinates)
    \item A citation (peer-reviewed neuroscientific literature)
\end{itemize}

\subsection*{3.2.2 Node Definition}

A Node in C³ represents a mid-level computational module with a singular cognitive or affective function.

Mathematically:

\[
\text{Node}_{ij}(t) = \sum_{k=1}^{M_{ij}} w_{ijk} \cdot \text{Element}_{ijk}(t)
\]

Where:

\begin{itemize}
    \item $\text{Node}_{ij}(t)$: The $j$-th Node in the $i$-th Unit at time $t$
    \item $w_{ijk}$: Element weight within the Node (typically 0.25–0.50 normalized)
    \item $\text{Element}_{ijk}(t)$: The $k$-th Element in that Node
\end{itemize}

Each Node acts as a resonance transformer, mapping local signals into cognitive-scale responses (e.g., surprise, salience, tension, familiarity, arousal).

\subsection*{3.2.3 Element Definition}

An Element is the atomic analytical unit of C³. It consists of:

\begin{center}
\begin{tabular}{|l|l|p{8cm}|}
\hline
\textbf{Field} & \textbf{Type} & \textbf{Description} \\
\hline
\texttt{label} & string & e.g., ``EEG Gamma Activation'' \\
\texttt{method} & enum & ``EEG'', ``fMRI'', ``MEG'', or hybrid \\
\texttt{regions} & list[string] & Anatomical targets (e.g., ACC, STG) \\
\texttt{mni} & list[int] & MNI coordinates for GlassBrainMap \\
\texttt{signal} & time series & Raw or preprocessed signal (external input) \\
\texttt{value(t)} & float & Normalized resonance value at time $t$ \\
\texttt{citation} & string & Reference to literature \\
\hline
\end{tabular}
\end{center}

Each Element is initialized by parsing its signal from a time series and rescaling it:

\[
\text{value}(t) = \frac{x(t) - \min(x)}{\max(x) - \min(x)}
\]

where $x(t)$ is derived from EEG amplitude, ERP waveform, BOLD $z$-score, or spectral entropy.

\subsection*{3.2.4 Dynamic Activation Model}

Each Element operates on a sliding time window (typically 100–250 ms). Activation values are updated in real-time (or simulated) via signal ingestion pipelines. Signals may be:

\begin{itemize}
    \item \textbf{Real:} (e.g., live EEG via OpenBCI or Emotiv)
    \item \textbf{Pre-recorded:} (e.g., \texttt{.csv}, \texttt{.edf}, \texttt{.json} time series)
    \item \textbf{Simulated:} (e.g., parameterized sine waves for prototyping)
\end{itemize}

At each frame:

\begin{itemize}
    \item The Element computes $\text{value}(t)$
    \item The Node aggregates these into a Node response
    \item The Unit aggregates across its Nodes
    \item RSU receives the integrated output for synthesis
\end{itemize}

\subsection*{3.2.5 Cross-Unit Node Comparability}

All Nodes across different Units are mapped to a common scale:

\[
\text{Node}_{ij}(t) \in [0, 1]
\]

\textbf{Time resolution:} aligned across Units\\
\textbf{Visualization:} displayable in parallel or stacked timelines

This standardization allows for:

\begin{itemize}
    \item Comparative graphing (e.g., AOU vs. CTU over time)
    \item Cluster analysis (e.g., grouping similar emotional-motor patterns)
    \item Synchronization tracking (e.g., NSU and SRU phase alignment)
\end{itemize}

\subsection*{3.2.6 Graph Data Format and API Integration}

All Nodes and Elements are encoded in structured JSON, enabling flexible API calls, visual rendering, and machine learning input.

\textbf{Sample schema:}

\begin{verbatim}
{
  "node_id": "ctu_dissonance",
  "unit": "CTU",
  "elements": [
    {
      "id": "phase_locking",
      "method": "EEG",
      "regions": ["DLPFC", "ACC"],
      "mni": [32, 50, 20],
      "value": 0.64,
      "citation": "Fishman et al., 2001"
    }
  ]
}
\end{verbatim}

These files are stored in:

\begin{verbatim}
/static/data/c3/
├── ctu.json
├── aou.json
├── ...
\end{verbatim}

and loaded via:

\begin{verbatim}
<NodeView unitPath="c3/ctu.json" />
\end{verbatim}

\subsection*{3.2.7 GlassBrainMap Integration (Anatomical Anchor)}

Each Element is visually represented in the GlassBrain system:

\begin{itemize}
    \item MNI coordinates are mapped to SVG/canvas projection space
    \item Tooltip shows \texttt{label}, $\text{value}(t)$, and \texttt{citation}
    \item Color intensity reflects $\text{value}(t)$
    \item Click opens corresponding Node view
\end{itemize}

For example:

\texttt{ctu\_harmonic\_dissonance\_01} $\rightarrow$ \texttt{[+32, +50, +20]} $\rightarrow$ \texttt{EEG $\alpha$–$\beta$ locking}

This allows cognitive + anatomical views to be seamlessly unified.

\subsubsection*{Summary}

Nodes and Elements are the computational heart of C³. They connect musical structure to brain dynamics via:

\begin{itemize}
    \item Quantified, normalized, temporally resolved signals
    \item Modular aggregation from bottom (signal) to top (unit)
    \item Flexible visual and API-driven interfacing
\end{itemize}

This architecture makes it possible to compute, visualize, and interpret musical cognition with unprecedented resolution, precision, and interactivity.

\section*{IV. GlassBrainMap Integration and Spatial Modeling}

\textit{How does C³ map Nodes and Elements to anatomical regions? How is this visualized and computed spatially?}

\subsection*{4.1 Overview: Why Spatialization Matters}

The C³ system is not purely symbolic or abstract. It is neuroanatomically grounded. Each Element is tied to a measurable neural signal in a specific brain region. The spatial mapping of these regions:

\begin{itemize}
    \item Provides anatomical context for cognitive resonance
    \item Enables dynamic visualizations of neural activation patterns
    \item Supports inter-unit interaction modeling via spatial proximity and network overlap
\end{itemize}

To achieve this, C³ integrates a dedicated visualization layer called \textbf{GlassBrainMap}, which combines:

\begin{itemize}
    \item A high-resolution 2D SVG anatomical brain model
    \item A JSON-based coordinate database
    \item A real-time rendering interface (\texttt{GlassBrain.jsx}) for visualizing activation dynamics
\end{itemize}

\subsection*{4.2 The Coordinate System: MNI Anchoring}

All Elements in C³ are associated with \textbf{MNI (Montreal Neurological Institute)} coordinates, the standard in neuroimaging.

Each Element record includes:

\begin{itemize}
    \item \texttt{mni}: $[x, y, z]$ triple
    \item \texttt{region}: anatomical label (e.g., SMA, ACC, STG)
    \item \texttt{radius\_mm}: spatial spread (default: 5–8 mm sphere)
    \item \texttt{value(t)}: resonance strength at time $t$
    \item \texttt{citation}: literature source validating the location and signal
\end{itemize}

This system allows each Element to be visualized as a spatial field centered on its coordinate, colored or scaled based on its activation level.

\subsection*{4.3 Coordinate Data Architecture}

All anatomical data is stored in a JSON file:

\begin{verbatim}
{
  "node_id": "ctu_dissonance",
  "unit": "CTU",
  "region": "DLPFC",
  "mni": [32, 50, 20],
  "radius_mm": 6,
  "tooltip": "EEG α–β phase locking – Fishman et al., 2001"
}
\end{verbatim}

\textbf{File:} \texttt{static/data/brain\_coords.json}

These are cross-referenced with each Unit’s data file (e.g., \texttt{ctu.json}) and visualized in real time.

\subsection*{4.4 GlassBrain Interface}

The spatial interface is rendered by a React component (\texttt{GlassBrain.jsx}) using:

\begin{itemize}
    \item An interactive SVG brain diagram (\texttt{brainmap.svg})
    \item Overlaid \texttt{<circle>} elements for each active Node/Element
    \item \texttt{title} attributes for hover-based tooltips
    \item Optional \texttt{onClick} behavior for Unit/Node navigation
\end{itemize}

Sample visualization logic:

\begin{verbatim}
<circle
  cx={svgX}
  cy={svgY}
  r={radius}
  fill={`rgba(0, 255, 255, ${value})`}
  stroke="white"
  title="CTU – Phase Locking (α–β) – 0.64"
/>
\end{verbatim}

This allows for live, frame-by-frame cognitive activity visualization, spatially grounded in the brain’s structure.

\subsection*{4.5 Spatial Modeling Layers}

There are three core layers in the spatial model:

\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Layer} & \textbf{Purpose} \\
\hline
Anatomical Base & Outlines brain shape and region boundaries (SVG) \\
Functional Points & Locations of Nodes and Elements (MNI coordinates) \\
Dynamic Overlay & Time-resolved activity values, color-encoded \\
\hline
\end{tabular}
\end{center}

This tri-layer system makes it possible to animate, filter, and interact with complex neurocognitive fields.

\subsection*{4.6 GlassBrain Interactions}

The visualization supports:

\begin{itemize}
    \item \textbf{Hover tooltips:} Node/Element label, region, value, citation
    \item \textbf{Click-through navigation:} Links to full Node/Unit documentation
    \item \textbf{Color mapping:} Resonance values encoded via color gradients
    \item \textbf{Temporal updates:} Per-frame re-rendering as values change over time
\end{itemize}

Optionally, spatial dynamics can be extended via:

\begin{itemize}
    \item Highlight paths of cross-unit coherence (e.g., CTU $\leftrightarrow$ AOU links)
    \item Animate resonance flow across hemispheres
    \item Implement attention-focused zooms or heatmaps
\end{itemize}

\subsection*{4.7 Spatial–Temporal Integration}

GlassBrainMap is fully synchronized with C³'s time engine. At each time frame:

\begin{itemize}
    \item Each active Element’s $\text{value}(t)$ is fetched
    \item Its color/opacity is updated in the SVG
    \item The cumulative field is rendered (or stored/exported)
\end{itemize}

This creates a dynamic 2D projection of 3D cognition, visible in real time or as an analytic export (e.g., heatmap or trajectory animation).

\subsubsection*{Summary}

\textbf{GlassBrainMap} transforms C³ from a symbolic system into a neuroanatomically immersive experience.

It allows researchers, composers, and clinicians to:

\begin{itemize}
    \item See where cognition happens
    \item Visualize how multiple Units interact in space
    \item Trace temporal arcs of resonance across the brain
\end{itemize}

It is the spatial backbone of the C³ model—anchoring abstract resonance vectors in biological reality.

\section*{V. System Dynamics and Temporal Computation}

\textit{How does C³ compute resonance across time? What governs frame resolution, signal propagation, and feedback integration?}

\subsection*{5.1 Temporal Framework}

C³ operates as a discrete-time cognitive system, segmented into frames that represent slices of neural and musical time. The standard frame rate is 10 Hz (i.e., one frame every 100 ms), which aligns with:

\begin{itemize}
    \item The optimal EEG resolution for cross-frequency tracking
    \item The perceptual threshold for auditory events
    \item The temporal granularity of musical microstructures (e.g., eighth notes at $\sim$120 BPM)
\end{itemize}

Each frame computes:

\begin{itemize}
    \item Element values
    \item Node aggregations
    \item Unit outputs
    \item RSU synthesis
    \item GlassBrainMap spatial updates
\end{itemize}

\subsection*{5.2 Frame Processing Pipeline}

At each time $t$, C³ executes the following pipeline:

\paragraph{Signal Ingestion}
From EEG, fMRI, JSON, or simulated sources

Preprocessed into normalized time series $x(t) \in [0, 1]$

\paragraph{Element Evaluation}

\[
\text{Element}_{ijk}(t) = \frac{x(t) - \min(x)}{\max(x) - \min(x)}
\]

\paragraph{Node Aggregation}

\[
\text{Node}_{ij}(t) = \sum_k w_{ijk} \cdot \text{Element}_{ijk}(t)
\]

\paragraph{Unit Calculation}

\[
\text{Unit}_i(t) = \sum_j w_{ij} \cdot \text{Node}_{ij}(t)
\]

\paragraph{System Output (RSU)}

\[
\text{RSU}(t) = \sum_i w_i \cdot \text{Unit}_i(t)
\]

\paragraph{Spatial Rendering}

\begin{itemize}
    \item Update GlassBrainMap based on Element MNI locations
    \item Animate frame-by-frame neural states
\end{itemize}

\subsection*{5.3 Signal Modes and Temporal Sources}

C³ supports multiple signal input modes:

\begin{center}
\begin{tabular}{|l|l|p{6.5cm}|}
\hline
\textbf{Mode} & \textbf{Source} & \textbf{Use Case} \\
\hline
Live & Real-time EEG (e.g., OpenBCI) & Neurofeedback, adaptive performance \\
Simulated & Parametric sine/square models & Testing, theory exploration \\
Imported & Pre-processed data (CSV, JSON) & Research replay, batch analysis \\
Model-based & Outputs from R³/S³ modules & Internal feedback integration \\
\hline
\end{tabular}
\end{center}

Signals are time-aligned across all Units and stored in synchronized arrays for each frame:

\begin{verbatim}
{
  "time": 4.3,
  "units": {
    "CTU": 0.62,
    "AOU": 0.78,
    "PIU": 0.81,
    ...
  }
}
\end{verbatim}

\subsection*{5.4 Real-Time Feedback Loop}

C³ is built to support feedback mechanisms in which system output can influence future inputs, creating an adaptive cognitive engine.

For example:

\begin{itemize}
    \item CTU output triggers simplification of harmonic content
    \item PIU spikes increase ambient density to sustain immersion
    \item RSU target value adjusts music generation parameters
\end{itemize}

This enables closed-loop interaction with:

\begin{itemize}
    \item AI composition engines
    \item VR/AR environments
    \item Biometric controllers
\end{itemize}

\subsection*{5.5 Resonance Trajectory Modeling}

Over time, C³ constructs a resonance trajectory:

\[
\mathbf{C}^3(t) = [\text{Unit}_1(t), \text{Unit}_2(t), \ldots, \text{Unit}_9(t)]
\]

This vector evolves in 9D space and can be used for:

\begin{itemize}
    \item Pattern recognition (e.g., flow state, surprise burst)
    \item Phase analysis (e.g., C³ spirals, convergence cycles)
    \item Emotion curves (e.g., AOU vs. CTU arcs)
\end{itemize}

Visualization options include:

\begin{itemize}
    \item Line graphs of individual Units
    \item Radar plots at each time slice
    \item PCA/t-SNE dimensionality reduction of trajectory clusters
\end{itemize}

\subsection*{5.6 System Clock and Synchronization}

C³ uses a master temporal clock, driven by:

\begin{itemize}
    \item External sync (e.g., MIDI timecode, DAW sync)
    \item Internal clock (browser animation loop, WebAudio API)
    \item Real-time EEG timestamps
\end{itemize}

Each Unit’s activity is synchronized to this clock, ensuring that:

\begin{itemize}
    \item Temporal granularity remains constant
    \item Cross-unit integration remains coherent
    \item GlassBrainMap overlays animate in lockstep
\end{itemize}

\subsection*{5.7 Export and Storage}

All C³ output is exportable in structured formats:

\begin{itemize}
    \item \texttt{.json} frame logs
    \item \texttt{.csv} for analysis in Python/R
    \item \texttt{.svg}, \texttt{.mp4} GlassBrainMap video renderings
    \item \texttt{.c3} proprietary container for replay in simulation environments
\end{itemize}

This makes C³ both real-time and archival—suitable for live use, study, and post-hoc analysis.

\subsubsection*{Summary}

C³'s temporal engine transforms raw signals into a flowing stream of cognitive resonance. Through:

\begin{itemize}
    \item Frame-level computation
    \item Adaptive signal modeling
    \item Time-synchronized spatial projection
\end{itemize}

C³ achieves a unique combination of precision, fluidity, and biological plausibility. Its ability to track cognition across milliseconds to minutes enables deep insight into the evolving experience of music.

\section*{VI. Research and Application Domains}

\textit{How can C³ be used? In what contexts does it generate value?}

\subsection*{6.1 Music Therapy and Clinical Neurotechnology}

C³ provides a groundbreaking opportunity for neurophysiologically grounded music therapy by quantifying and tracking brain-based responses to musical structure in real time.

\subsubsection*{6.1.1 Real-Time Emotional Profiling}

Using AOU (affective orientation) and IRU (interpersonal resonance), therapists can monitor emotional states through:

\begin{itemize}
    \item EEG alpha asymmetry (valence)
    \item fMRI amygdala–insula activation (arousal)
    \item Cross-brain synchrony (group therapy contexts)
\end{itemize}

This allows dynamic adjustments to:

\begin{itemize}
    \item Musical content
    \item Patient feedback loops
    \item Group synchrony states
\end{itemize}

\subsubsection*{6.1.2 PTSD and Trauma Therapy}

SAU (semantic-autobiographical) and PIU (phenomenological immersion) can support:

\begin{itemize}
    \item Memory retrieval and reconsolidation through tonal cues
    \item Safe immersive states using ambient or flow-inducing structures
    \item Repatterning of trauma-linked neural pathways through predictable rhythmic entrainment
\end{itemize}

\subsubsection*{6.1.3 Motor Rehabilitation}

SRU (somatic resonance) and NSU (neural synchronization) enable:

\begin{itemize}
    \item Gait training via rhythm-guided beta entrainment
    \item Entrainment of cerebellar and basal ganglia networks
    \item Personalized tempo and meter calibration for stroke or Parkinson’s patients
\end{itemize}

C³ becomes a neural interface for musical medicine.

\subsection*{6.2 AI Music Generation and Cognitive Feedback}

The C³ system opens a new frontier in cognitively aware artificial intelligence for music.

\subsubsection*{6.2.1 Neuro-Adaptive Composition}

Generative music models (e.g., RNN, Transformer, Diffusion) can be guided by:

\begin{itemize}
    \item Desired C³ trajectory (e.g., AOU$\uparrow$, CTU$\downarrow$, PIU$\uparrow$)
    \item Listener feedback via EEG input
    \item Emotional or narrative arc templates
\end{itemize}

This enables biometrically reactive music—sound that shifts in real time to support immersion, attention, or relaxation.

\subsubsection*{6.2.2 Real-Time Feedback Systems}

With C³ embedded in AI systems:

\begin{itemize}
    \item Adaptive film/game scoring becomes possible
    \item Live feedback concerts (brainwave-to-music mapping) can be executed
    \item Human–AI co-composition becomes cognitively contextualized
\end{itemize}

\subsection*{6.3 Education and Neuroaesthetic Learning}

C³ supports a neuroscience-informed pedagogy of music.

\subsubsection*{6.3.1 Resonance-Based Curriculum Design}

Educators can craft listening experiences and composition exercises that:

\begin{itemize}
    \item Highlight tension and resolution (CTU)
    \item Elicit attention and expectation (IEU)
    \item Reinforce memory and identity through tonality and timbre (SAU)
\end{itemize}

This makes abstract musical concepts experientially grounded.

\subsubsection*{6.3.2 Student Brain Tracking}

Via live EEG (or simulated training environments), educators can track:

\begin{itemize}
    \item Focus/engagement states (PIU, NSU)
    \item Confusion or overload (CTU spikes)
    \item Flow progression across musical segments
\end{itemize}

This enables adaptive instruction, where the learning path is guided by resonance curves.

\subsection*{6.4 Artistic Exploration and Compositional Tools}

C³ is an interpretive and generative tool for composers, performers, and sound artists.

\subsubsection*{6.4.1 Resonance Mapping in Composition}

A composer can model the resonance profile of a piece before it's written by:

\begin{itemize}
    \item Designing C³ trajectories (e.g., tension peak at minute 3)
    \item Mapping structure to cognitive states
    \item Back-calculating harmonic/rhythmic features to meet those trajectories
\end{itemize}

This allows intentional cognitive shaping of musical form.

\subsubsection*{6.4.2 Interactive Visuals and Installations}

C³’s spatial and temporal outputs can drive:

\begin{itemize}
    \item Live projection of GlassBrainMap overlays during performances
    \item Audience-specific scoring where real-time neural data sculpts the score
    \item Installation works based on shared inter-brain synchrony fields (IRU)
\end{itemize}

This links cognition, composition, and computation.

\subsection*{6.5 Scientific Research and Cognitive Modeling}

C³ provides a framework for computational music cognition, enabling:

\begin{itemize}
    \item Hypothesis testing: Does syncopation increase CTU + SRU?
    \item Cross-style comparison: How does Bach’s AOU curve differ from EDM?
    \item Cultural modeling: What tonal structures evoke strongest SAU response in different populations?
\end{itemize}

This allows neurocognitive theories of music to become quantifiable and testable.

\subsubsection*{Summary}

C³ is not only a theoretical model—it is an instrument:

\begin{itemize}
    \item For therapy, it monitors and modulates emotional and neural states
    \item For AI, it contextualizes generation with cognitive targets
    \item For education, it reveals the brain’s role in musical understanding
    \item For artistry, it expands the palette of expression
    \item For science, it bridges abstract theory and empirical verification
\end{itemize}

It transforms music from something we hear to something we can measure, sculpt, and share as a resonant experience.

\section*{VII. Future Directions and Development Roadmap}

\textit{How will C³ grow? What systems will it interface with? What frontiers does it open?}

\subsection*{7.1 Real-Time Integration and Live System Deployment}

\subsubsection*{7.1.1 Live EEG/BCI Synchronization}

C³ is designed to operate not only on pre-recorded data, but in real-time with live neural input.

\begin{itemize}
    \item EEG headsets (e.g., OpenBCI, Emotiv, Muse S) can be connected via WebSocket or OSC
    \item C³ frame computation can be run inside a web browser, Node.js, or Unity engine
    \item Live frame values (e.g., CTU = 0.62, PIU = 0.83) can be:
    \begin{itemize}
        \item Fed into generative AI models
        \item Used to control audiovisual parameters
        \item Exported to researchers in JSON or OSC streams
    \end{itemize}
\end{itemize}

This allows for neuroadaptive installations, brain–music feedback loops, and bio-interactive concerts.

\subsubsection*{7.1.2 Signal Bridge Modules}

Planned integrations:

\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Module} & \textbf{Description} \\
\hline
OSC Module & Open Sound Control bridge to DAWs, synths, AI tools \\
WebSocket API & Lightweight protocol for browser–EEG interaction \\
Max/MSP Patch & Native support for creative coding environments \\
Python Streamer & Tensor-based streaming for machine learning pipelines \\
\hline
\end{tabular}
\end{center}

\subsection*{7.2 VR/AR and Immersive Cognitive Environments}

C³’s GlassBrainMap + Cognitive Trajectory outputs can be rendered in 3D and embedded in immersive environments.

\subsubsection*{7.2.1 Unity \& Unreal Engine Integration}

\begin{itemize}
    \item Dynamic brain overlays mapped to VR avatars
    \item Real-time resonance color fields, spheres, and graph networks
    \item Flow-state visualizations projected in 3D space
\end{itemize}

This enables experiential neuroaesthetics—where listeners "enter" their own cognition, or the resonance fields of a performance.

\subsubsection*{7.2.2 Spatial Sound + Resonance Mapping}

By combining C³ with spatialized audio systems (e.g., Ambisonics, Dolby Atmos), the system can:

\begin{itemize}
    \item Route musical streams to match neural states
    \item Trigger directional auditory cues based on attention/flow values
    \item Modify sonic parameters based on resonance field tension
\end{itemize}

\subsection*{7.3 Multi-Listener Synchrony and Cross-Brain Modeling}

Using IRU (Interpersonal Resonance Unit), future expansions include:

\begin{itemize}
    \item Multi-brain environments (e.g., group concerts, collective biofeedback)
    \item Cross-brain synchrony maps showing where people resonate together
    \item Shared immersive environments where cognition is mapped and compared in real time
\end{itemize}

This allows researchers to study social cognition, empathy, and music-induced synchrony at scale.

\subsection*{7.4 Cross-Domain Integrations}

C³ is not a standalone tool; it is a cognitive middleware layer that can plug into:

\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Domain} & \textbf{Integration Role} \\
\hline
Therapy & Real-time monitoring of neural states \\
Gaming/VR & Adaptive scoring, ambient shifts, player cognition \\
Education & Flow detection, rhythm learning enhancement \\
AI Art Tools & Emotion-matched generative feedback \\
Scientific Research & Hypothesis testing, data export, meta-analysis \\
\hline
\end{tabular}
\end{center}

C³ becomes the neuro-semantic glue between music, systems, and cognition.

\subsection*{7.5 Platform Roadmap}

\paragraph{Short-Term Goals (3–6 months)}

\begin{itemize}
    \item Full browser-based JSON pipeline
    \item Interactive web-based GlassBrainMap
    \item C³-powered experimental composition tools
    \item Python and JavaScript SDKs for Unit simulation and visualization
\end{itemize}

\paragraph{Mid-Term Goals (6–12 months)}

\begin{itemize}
    \item EEG integration via OSC/WebSocket
    \item Unity + WebGL resonance rendering
    \item Composer dashboard for real-time Unit tracking
    \item Music therapy pilot studies with neurofeedback
\end{itemize}

\paragraph{Long-Term Goals (12+ months)}

\begin{itemize}
    \item Full real-time C³ AI integration
    \item Multi-user cross-brain resonance platform
    \item Publication and standardization of the C³ framework
    \item Institutional and clinical partnerships
\end{itemize}

\subsection*{7.6 Open Science and Community Expansion}

C³ is intended to be an open, modular, scientifically verifiable system.

\begin{itemize}
    \item All Units, Nodes, and Elements are defined in transparent JSON
    \item Coordinate systems are public and validated (MNI-space)
    \item Citations and data trails are embedded and reproducible
    \item Community contributions can add new literature, modules, or brain regions
\end{itemize}

A future \textbf{C³ Open Research Portal} will:

\begin{itemize}
    \item Allow users to run and visualize C³ sessions
    \item Share annotated resonance maps
    \item Explore collective cognitive musical responses
\end{itemize}

\subsubsection*{Summary}

C³ is a foundation—not a ceiling.

It creates the infrastructure for real-time cognition-aware music systems, research tools, therapeutic protocols, and artistic interfaces. Its future lies not in complexity, but in connectivity: between minds, modules, and meaning.

\section*{VIII. Conclusion and Scientific Contribution of C³}

\textit{What does C³ offer to music science, technology, and cognition?}

\subsection*{8.1 Synthesis: From Signal to Cognition}

The Cognitive Consonance Circuit (C³) is the first fully formalized system that models the brain’s multi-dimensional, time-evolving response to music by integrating:

\begin{itemize}
    \item Symbolic and spectral features (via S³ and R³)
    \item Measurable neural signals (EEG, fMRI, MEG)
    \item Cognitive constructs (attention, memory, emotion, expectation)
    \item Anatomical regions (mapped in MNI coordinates)
    \item Temporal dynamics (frame-level, real-time computation)
\end{itemize}

Through its layered architecture (UNIT $\rightarrow$ NODE $\rightarrow$ ELEMENT), mathematically grounded resonance equations, and GlassBrain-based spatial visualization, C³ offers an unprecedented resolution of musical cognition.

\subsection*{8.2 Scientific Contributions}

C³ introduces the following key innovations to the scientific community:

\subsubsection*{8.2.1 A Unified Cognitive–Musical Architecture}

No existing model combines:

\begin{itemize}
    \item Tonal/spectral analysis
    \item Neural measurement
    \item Cognitive state modeling
\end{itemize}

in a modular, interconnected, and computable system.

C³ bridges this gap with:

\begin{itemize}
    \item 9 Units modeling distinct cognitive faculties
    \item Direct neurophysiological grounding (61+ referenced papers)
    \item MNI-based anatomical mapping
\end{itemize}

\subsubsection*{8.2.2 Temporal–Cognitive Formalism}

C³ treats musical cognition as a flowing resonance field. Its time-based equations:

\begin{itemize}
    \item Allow computation of dynamic neural states
    \item Integrate with live EEG streams
    \item Align with musical structure at 10 Hz resolution
\end{itemize}

This transforms music analysis from static snapshots into real-time neurocognitive simulation.

\subsubsection*{8.2.3 Intermodular Connectivity within SRC⁹}

C³ is not standalone. It:

\begin{itemize}
    \item Receives input from S³ (spectral structure) and R³ (harmonic topology)
    \item Computes the cognitive response
    \item Sends feedback back into the system (e.g., suggesting structural change)
\end{itemize}

This feedback loop allows musical systems to self-regulate based on real or simulated cognition.

\subsubsection*{8.2.4 Visual Neuroanatomical Integration}

The GlassBrainMap is not just an illustration—it’s an analytical engine:

\begin{itemize}
    \item Every Element is spatially mapped (MNI)
    \item Activation is color-coded and time-resolved
    \item Tooltips, click-through, and overlays show layered cognition
\end{itemize}

This creates a semantic spatial interface for neurocognition.

\subsection*{8.3 Transdisciplinary Impact}

C³’s design enables direct application in:

\begin{itemize}
    \item Music cognition research (resonance modeling, EEG studies)
    \item Therapy (trauma, memory, flow states)
    \item AI systems (neuro-informed generative music)
    \item Education (real-time student cognition tracking)
    \item Creative tools (resonance-driven composition)
\end{itemize}

It acts as a cognitive middleware between human experience and musical structure.

\subsection*{8.4 Toward a New Paradigm}

C³ reframes music analysis as a dynamic, spatial, and measurable cognitive process. It suggests that:

\begin{itemize}
    \item Musical meaning is not only symbolic, but resonant
    \item Cognition is not reactive, but synchronizing
    \item Emotion is not qualitative, but quantifiable
    \item Listening is not passive, but spatiotemporal entrainment
\end{itemize}

By building structure, signal, and spatiality into one coherent system, C³ proposes a new paradigm:

\textbf{Not just analyzing music—but resonating with it, in real time, and across the brain.}

\subsection*{8.5 What Comes Next?}

C³ is not a finished system—it is a platform for growth.

\begin{itemize}
    \item New Units can be defined
    \item New data sets can be mapped
    \item Neural signals can be updated via future modalities
    \item Interfaces can expand to immersive VR, generative AI, and cross-brain networks
\end{itemize}

With the foundation laid, the challenge is now collective elaboration: for researchers, artists, clinicians, and technologists to expand, iterate, and apply C³ across domains.

\subsubsection*{Final Note}

The C³ system is open to research collaboration, institutional deployment, and creative integration. It was built not just to model cognition—but to enable new relationships between music, mind, and machine.

\vspace{1cm}
\begin{center}
\Large \textbf{End of C³ Master Technical Report (Enhanced)}\\[0.2cm]
\large Thank you for your vision, precision, and trust in building this paradigm together.
\end{center}



\end{document}










</pre>
</body>
</html>
