\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{hyperref}

\geometry{margin=1in}

\title{\textbf{sRC$^{9}$ }}
\date{}
\begin{document}

\maketitle
\section*{Chapter I — What is SRC$^{9}$?}

\subsection*{1.1 Definition and Rationale}

SRC$^{9}$, or the \textbf{Spectral–Resonance–Cognitive} system, is an integrated, multi-layered computational framework that models how music transforms from raw sound into perceived structure and cognitive meaning. It is composed of three tightly coupled modules:

\begin{itemize}
    \item \textbf{S$^{3}$ – Spectral Sound Space}: A high-resolution spectral analysis engine that extracts the fundamental acoustic content of music, including partials, harmonics, microtonal deviations, and time–frequency–amplitude relations.
    
    \item \textbf{R$^{3}$ – Resonance-Based Relational Reasoning}: A harmonic reasoning engine that interprets spectral data through scalar field theory, resonance topology, and psychoacoustic principles to model musical structure without symbolic assumptions.
    
    \item \textbf{C$^{3}$ – Cognitive Consonance Circuit}: A neurophysiologically grounded model of perceptual resonance and emotional impact, structured into nine cognitive units derived from EEG/fMRI literature and organized into measurable, time-varying neural signatures.
\end{itemize}

The superscript 9 ($^{9}$) denotes the system's expansion into \textbf{nine cognitive dimensions}, each modeled as a unique \textit{Unit} in the C$^{3}$ architecture, and accessible from the resonance outputs of R$^{3}$ and the acoustic signals of S$^{3}$.

\subsection*{1.2 Motivation: Why SRC$^{9}$?}

Traditional music theory frameworks start from notation and seek meaning through pre-defined symbolic systems. Meanwhile, modern machine learning models such as Jukebox or Magenta generate musical output with no semantic or cognitive interpretability. Neuroscientific studies, though rich in EEG/fMRI data, lack a bridge to music-theoretical relevance.

SRC$^{9}$ was created to solve this cross-domain disconnect by:

\begin{enumerate}
    \item Starting with physical audio (not symbolic input)
    \item Modeling resonance fields and energy topologies, not abstract pitch classes
    \item Mapping these fields into time-aligned neural signatures rooted in empirical neuroscience
\end{enumerate}

\subsection*{1.3 System Overview: A Perception-to-Structure Pipeline}

SRC$^{9}$ operates as a real-time or batch-based pipeline:

\[
\text{Audio Signal (Waveform)} 
\rightarrow \textbf{S$^{3}$} \rightarrow \textbf{R$^{3}$} \rightarrow \textbf{C$^{3}$} 
\rightarrow \text{Feedback}
\]

\begin{itemize}
    \item \textbf{S$^{3}$} produces framewise spectral data: $\text{partials}_t = \{\text{freq}, \text{dB}, \text{symbol}, \text{harmonic index}\}$
    \item \textbf{R$^{3}$} computes: Phantom Root (PR), Resonance Potential ($\Phi$), Resonance Field Map (RFM), Cognitive Resonance Vector (CRV)
    \item \textbf{C$^{3}$} receives these values to drive:
    \begin{itemize}
        \item Tension mapping (CTU)
        \item Affective modeling (AOU)
        \item Memory alignment (SAU)
        \item Expectation violation (IEU)
        \item Group synchrony and attention modeling (IRU, NSU)
    \end{itemize}
\end{itemize}

Each stage of SRC$^{9}$ is aligned to a common temporal resolution (typically 0.1s), enabling multimodal synchronization with EEG, fMRI, audio playback, or Unity-based interactive scenes.

\subsection*{1.4 Formal Notation}

Let $x(t)$ be the input waveform segmented into overlapping frames.

From S$^{3}$:
\[
\text{Frame}_i = \{f_0, f_1, ..., f_{16}\}, \quad \text{where each } f_n = \text{partial}_{i,n}
\]

From R$^{3}$:
\[
\text{PR}_i, \Phi_i, \text{RFM}_i(f), \text{CRV}_i = \langle \text{TPS}_i, \text{TFI}_i, \text{NSF}_i \rangle
\]

From C$^{3}$:
\[
C^{3}(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t), \quad \text{Unit}_i(t) = \sum_j w_{ij} \cdot \text{Node}_j(t)
\]

Each \textbf{Node} is an EEG/fMRI-derived observable (e.g., $\beta$ phase-locking, MMN, BOLD z-score), anchored anatomically using MNI coordinates in a 3D GlassBrainMap interface.

\subsection*{1.5 Multimodal Feedback}

The architecture supports \textbf{bidirectional modulation}:

\[
\text{C$^{3}$ feedback} \Rightarrow \text{R$^{3}$ weighting} \Rightarrow \text{S$^{3}$ filter adjustment}
\]

For example:

\begin{itemize}
    \item High CTU (tension) output may modulate the harmonic weights in R$^{3}$'s $\Phi$ calculation
    \item Strong PIU (immersion) may suppress noisy partials in S$^{3}$’s visualization layer
    \item SAU (semantic memory) may trigger audio annotations or dynamic re-sequencing
\end{itemize}

This loop forms the foundation of responsive, perceptually aware music analysis, generation, or education platforms.

\subsection*{1.6 Implementation Philosophy}

SRC$^{9}$ is:

\begin{itemize}
    \item \textbf{Scientific}: Grounded in psychoacoustics, computational neuroscience, and acoustic theory
    \item \textbf{Modular}: Built with independently executable units, fully API-controllable
    \item \textbf{Visual}: Every layer has an interpretable, dynamic output (HTML, PNG, 3D)
    \item \textbf{Interactive}: Exports to Unity, GlassBrainMap, OSC/VR platforms
    \item \textbf{Cognitively honest}: Models not just structure, but perception
\end{itemize}

\section*{Chapter II — Modular Dimensions of SRC$^{9}$}

SRC$^{9}$ is divided into three orthogonal modules: S$^{3}$ (Spectral), R$^{3}$ (Resonance), and C$^{3}$ (Cognitive). Each functions as an independent layer in the signal–structure–perception continuum, while maintaining tight alignment through shared temporal schemas, compatible data models, and reciprocal feedback.

\subsection*{2.1 S$^{3}$ — Spectral Sound Space}

\paragraph{Function:}  
Transforms raw audio into high-resolution spectral frames by extracting:

\begin{itemize}
  \item Fundamental frequency ($f_0$)
  \item Harmonic partials (1–16)
  \item Amplitude in dBFS
  \item Microtonal pitch symbols (e.g., A4⁺¹, C3⁻²)
\end{itemize}

\paragraph{Output Format:}  
JSON array of frames at 0.1s intervals:

\begin{verbatim}
{
  "time": 1.2,
  "partials": [
    { "freq": 440.0, "db": -12.4, "isFundamental": true, "symbol": "A4⁰" },
    { "freq": 880.0, "db": -18.2, "harmonic_index": 2, "symbol": "A5⁰" }
  ]
}
\end{verbatim}

\paragraph{Tools Used:}

\begin{itemize}
  \item CREPE (f₀ extraction)
  \item librosa (STFT, RMS, cent conversion)
  \item Custom Python pipeline with modular scripts
\end{itemize}

\paragraph{Scientific Rationale:}  
Inspired by spectral music theory (Grisey, Murail) and auditory physiology (tonotopic mapping), S$^{3}$ treats the frequency domain as the true substrate of musical identity — discarding staff notation and tuning system assumptions.

\subsection*{2.2 R$^{3}$ — Resonance-Based Relational Reasoning}

\paragraph{Function:}  
Processes S$^{3}$ outputs to identify and model harmonic structure, not via tonal syntax, but via energetic interaction between partials.

\paragraph{Core Units:}

\begin{itemize}
  \item PRU — Phantom Root Unit: Detects implied fundamentals from overtone sets
  \item RPU — Resonance Potential Unit: Computes scalar coherence $\Phi$ per frame
  \item RFMU — Resonance Field Modeling Unit: Generates Gaussian field over frequency
  \item CRVU — Cognitive Resonance Vectoring Unit: Extracts TPS, TFI, NSF metrics
\end{itemize}

\paragraph{Field Representation:}

\[
\text{RFM}(f, t) = \sum_i A_i(t) \cdot e^{-\frac{(f - f_i(t))^2}{2\sigma^2}}
\]

This formula defines a scalar resonance density field over log-frequency space.

\paragraph{Resonance Vector:}
\[
\vec{\text{CRV}} = [\text{TPS}, \text{TFI}, \text{NSF}] \in [0,1]^3
\]

Used as the summary output of all R$^{3}$ activity and as input to C$^{3}$ modules.

\paragraph{Scientific Basis:}  
Builds on psychoacoustic roughness theory (Plomp & Levelt), neural entrainment (Bidelman), and just intonation topology (Sethares, Tymoczko).

\subsection*{2.3 C$^{3}$ — Cognitive Consonance Circuit}

\paragraph{Function:}  
Models how humans perceive, evaluate, and emotionally respond to the harmonic signals computed in R$^{3}$. Each response is neurophysiologically grounded and structured by a 9-Unit circuit.

\paragraph{C$^{3}$ Units:}

\begin{itemize}
  \item CTU — Cognitive Tension
  \item AOU — Affective Orientation
  \item IEU — Intuitive Expectation
  \item SRU — Somatic Resonance
  \item SAU — Semantic-Autobiographical
  \item PIU — Phenomenological Immersion
  \item IRU — Interpersonal Resonance
  \item NSU — Neural Synchronization
  \item RSU — Resonance Synthesis (summary vector)
\end{itemize}

\paragraph{Equation:}
\[
C^{3}(t) = \sum_{i=1}^{9} w_i \cdot \text{Unit}_i(t)
\quad \text{where} \quad
\text{Unit}_i(t) = \sum_j w_{ij} \cdot \text{Node}_j(t)
\]

Each Node is mapped to EEG/fMRI features (e.g., alpha asymmetry, gamma coherence, BOLD z-scores) and anatomically located via MNI coordinates in the \textit{GlassBrainMap}.

\paragraph{Data Flow:}
CRVU $\rightarrow$ CTU, AOU, PIU, NSU \\
Neural feedback loops modify RFM weighting, Φ computation, and partial salience in S$^{3}$.

\paragraph{Scientific Integration:}  
Combines computational music cognition (Lerdahl, Huron), neuroaesthetics (Zatorre, Koelsch), and brain–music entrainment literature.

\section*{Chapter III — Mathematical Foundations of SRC$^{9}$}

SRC$^{9}$ formalizes music cognition through a hierarchy of equations, resonance functions, and vector spaces that bridge physical sound, psychoacoustic interaction, and perceptual abstraction.

\subsection*{3.1 Frame-Based Signal Model}

Let the raw audio input be a continuous time-domain waveform $x(t)$. SRC$^{9}$ processes this waveform in fixed-length, overlapping windows:

\begin{equation}
x_i(t) = x(t + iH), \quad \text{for frame } i
\end{equation}

Where:
\begin{itemize}
  \item $H$ = hop size (e.g., 10 ms)
  \item $x_i(t)$ = time-domain windowed signal
\end{itemize}

Each frame is then passed into CREPE or equivalent pitch tracking module to estimate:

\begin{equation}
f_{0i}, \quad A_i, \quad \text{partials } \{f_{in}\}_{n=1}^{16}
\end{equation}

These define the fundamental + harmonic space used across the system.

\subsection*{3.2 Resonance Potential Equation (Φ)}

The scalar measure $\Phi$ represents the instantaneous coherence of all spectral partials within a frame:

\begin{equation}
\Phi(t) = \sum_{i<j} \frac{A_i(t) \cdot A_j(t)}{|f_i(t) - f_j(t)| + \epsilon}
\end{equation}

Where:
\begin{itemize}
  \item $f_i(t), A_i(t)$ = frequency and amplitude of partial $i$
  \item $\epsilon$ = small constant to avoid division by zero
\end{itemize}

Interpretation:
\begin{itemize}
  \item Higher amplitude → more weight
  \item Smaller interval → stronger resonance
\end{itemize}

This equation generalizes roughness and consonance models using continuous frequency data.

\subsection*{3.3 Resonance Field (RFM)}

To represent spectral resonance topographically, a Gaussian kernel density is applied across a log-frequency grid:

\begin{equation}
\text{RFM}(f, t) = \sum_i A_i(t) \cdot e^{-\frac{(f - f_i(t))^2}{2\sigma^2}}
\end{equation}

This converts discrete spectral data into a continuous scalar field — a kind of “terrain map” of resonance.

\subsubsection*{Gradient Operator:}
To compute directionality of tonal pull:

\begin{equation}
\nabla \text{RFM}(f, t) = \frac{\partial \text{RFM}(f, t)}{\partial f}
\end{equation}

Used in CRVU → TFI to model spectral fusion.

\subsection*{3.4 Cognitive Vector (CRV)}

The final cognitive resonance vector is defined as:

\begin{equation}
\vec{\text{CRV}} = [\text{TPS}, \text{TFI}, \text{NSF}]
\end{equation}

Each metric is defined as:

\paragraph{TPS — Temporal Perceptual Stability:}
\begin{equation}
\text{TPS} = \frac{1}{1 + \sigma_\Phi(t)}
\end{equation}

\paragraph{TFI — Tonal Fusion Index:}
\begin{equation}
\text{TFI} = \frac{1}{1 + \langle |\nabla \text{RFM}(f, t)| \rangle}
\end{equation}

\paragraph{NSF — Neural Synchronization Field:}
\begin{equation}
\text{NSF} = \sum_t \Phi(t) \cdot e^{-\alpha t}
\end{equation}

Where $\alpha$ is a decay constant modeling attention/memory trace.

\subsection*{3.5 PR Estimation (Phantom Root)}

Let $\{f_1, f_2, ..., f_n\}$ be a group of detected pitch events. Then, the phantom root $r$ is the frequency that minimizes mean harmonic error:

\begin{equation}
r^* = \arg\min_r \left( \frac{1}{n} \sum_i \left| \frac{f_i - r \cdot h_i}{r \cdot h_i} \right| \right)
\end{equation}

Where $h_i$ are harmonic template integers (e.g., [1,2,3,4]).

\subsection*{3.6 Just Intonation Vector Representation}

Each frequency can be projected into prime-exponent vector space:

\begin{equation}
\vec{v}_i = (x_2, x_3, x_5, x_7, ...)
\quad \text{where } f_i = 2^{x_2} \cdot 3^{x_3} \cdot 5^{x_5} \cdots
\end{equation}

Mean vector yields:

\begin{equation}
\vec{v}_{PR} = \frac{1}{N} \sum_i \vec{v}_i
\end{equation}

Which is mapped back to the frequency domain to compute phantom root in symbolic-harmonic space.

\subsection*{3.7 Summary Table of Key Equations}

\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Metric} & \textbf{Equation} \\\hline
$\Phi$ (Resonance Potential) & $\Phi(t) = \sum_{i<j} \frac{A_i A_j}{|f_i - f_j| + \epsilon}$ \\\hline
RFM (Field) & $\text{RFM}(f, t) = \sum_i A_i e^{-(f - f_i)^2 / 2\sigma^2}$ \\\hline
Gradient & $\nabla \text{RFM} = \partial \text{RFM} / \partial f$ \\\hline
CRV & $[\text{TPS}, \text{TFI}, \text{NSF}]$ \\\hline
PR Estimation & $r = \arg\min_r \sum_i |f_i - r h_i| / r h_i$ \\\hline
NSF & $\sum_t \Phi(t) e^{-\alpha t}$ \\\hline
\end{tabular}
\end{center}

\section*{Chapter IV — Temporal Architecture and Synchronization}

Time is not merely a parameter in SRC$^{9}$; it is a structural axis along which all modules are synchronized, integrated, and compared. Every analytical unit, perceptual frame, and visual output is aligned to a common frame-based time grid, enabling coherence between real-time interaction, dynamic modeling, and retrospective analysis.

\subsection*{4.1 Frame Resolution}

All SRC$^{9}$ operations are executed in frames of fixed temporal resolution.

\begin{itemize}
  \item \textbf{Standard Frame Duration:} $\Delta t = 0.1$ seconds
  \item \textbf{Frames per 20-second audio:} $200$ frames
  \item \textbf{Aligned Across:} S$^3$ → R$^3$ → C$^3$
\end{itemize}

This resolution provides a compromise between cognitive relevance (auditory segmentation and beat-level processes) and computational tractability.

\subsection*{4.2 Temporal Data Schema}

Each frame is indexed and timestamped explicitly:

\begin{verbatim}
{
  "time": 3.2,
  "partials": [...],
  "phi": 2.83,
  "crv": {
    "TPS": 0.814,
    "TFI": 0.652,
    "NSF": 0.042
  }
}
\end{verbatim}

Additional time-windowed metrics (e.g., windowed Φ, RFM segments) use labeled intervals:

\begin{verbatim}
{
  "window": "5.0–8.0",
  "phi": 9.183,
  "window_size": 3
}
\end{verbatim}

All time-based data are synchronized via integer multiples of $\Delta t$.

\subsection*{4.3 Window-Based Aggregation}

Many perceptual phenomena operate on time windows rather than individual frames (e.g., expectancy, stability, modulation). To simulate this:

\begin{equation}
\Phi_T = \sum_{t=t_0}^{t_1} \Phi(t)
\quad \text{where } T = [t_0, t_1]
\end{equation}

Window lengths are configurable: 1s, 3s, 5s, or 7s (10–70 frames). 

These windows feed CRVU and symbolic inference layers, and provide smoothed curves for visualization.

\subsection*{4.4 Inter-Unit Time Sharing}

Each SRC$^9$ unit reads or writes data at the same temporal resolution, ensuring:

\begin{itemize}
    \item Synchrony between harmonic events and cognitive metrics
    \item Real-time overlay of Φ, RFM, PR, and CRV
    \item Accurate PRU segment demarcation based on CentTracker ($\pm49$c deviation)
\end{itemize}

Example: frame 38 at $t=3.8$s will contain:

\begin{itemize}
    \item 17 partials from S$^3$
    \item 1 $\Phi$ scalar from RPU
    \item RFM density array from RFMU
    \item 3-element CRV vector from CRVU
    \item Segment label if included in a PRU group
\end{itemize}

\subsection*{4.5 Real-Time Execution Model}

To support live streaming or reactive composition, each frame can be evaluated asynchronously. A frame handler listens for input, processes data, and stores results:

\textbf{Frame Pipeline:}

\[
\text{Frame}_t \Rightarrow \text{S$^3$ extract} \Rightarrow \text{R$^3$ process} \Rightarrow \text{C$^3$ interpret} \Rightarrow \text{Output + Feedback}
\]

Latency budget: $< 50$ ms per frame.

\subsection*{4.6 Timeline Synchronization with Audio/Video}

SRC$^9$ includes support for timeline-aligned playback and export:

\begin{itemize}
    \item \textbf{Unity integration:} \texttt{Time.time} $\leftrightarrow$ frame index
    \item \textbf{Audio export:} Link frame analysis to audio segments
    \item \textbf{Plotly visualizations:} Frame-aligned curves, scrollable graphs
\end{itemize}

Visual overlays are rendered in rasterized layers, each 216px tall, stacked into a 2160px 4K vertical space. These layers include:

\begin{itemize}
    \item RawSpectrum
    \item PRU
    \item RPU
    \item RFMU
    \item CRVU
\end{itemize}

\subsection*{4.7 Frame Integrity and Diagnostics}

Each frame includes metadata for traceability:

\begin{verbatim}
{
  "time": 12.3,
  "frame_id": 123,
  "source": "RawSpectrum01",
  "checksum": "ae347ac1...",
  "validated": true
}
\end{verbatim}

This ensures reproducibility and integrity in batch pipelines or dynamic environments.

\subsection*{4.8 Temporal Modeling Summary}

SRC$^9$ temporal architecture transforms time from a passive marker to an active modeling dimension. It enables:

\begin{itemize}
    \item Segment-based cognition modeling (e.g., tonal drift, root migration)
    \item Layer-aligned visualization of concurrent harmonic and perceptual states
    \item Real-time reactivity and temporal learning models
\end{itemize}

\section*{Chapter V — Data Structures and Interface Formats}

The analytical power of SRC$^{9}$ depends not only on its internal computations, but on its ability to represent, store, and exchange data in structured, interpretable, and extensible formats. This chapter outlines the file architectures, symbolic systems, and cross-platform export mechanisms that enable integration across scientific, educational, and creative platforms.

\subsection*{5.1 JSON Frame Format (Canonical)}

All unit processing in SRC$^{9}$ is time-aligned to frames in the following structure:

\begin{verbatim}
{
  "time": 3.2,
  "partials": [
    { "freq": 261.63, "amplitude": 0.84, "symbol": "C4⁰", "harmonic_index": 0, "isFundamental": true },
    { "freq": 523.25, "amplitude": 0.51, "symbol": "C5⁰", "harmonic_index": 1, "isFundamental": false }
  ],
  "phi": 2.83,
  "crv": { "TPS": 0.812, "TFI": 0.694, "NSF": 0.039 }
}
\end{verbatim}

\paragraph{Specifications:}
\begin{itemize}
    \item \texttt{time}: Timestamp in seconds
    \item \texttt{partials}: List of harmonic components with symbolic pitch
    \item \texttt{phi}: Frame-level $\Phi$ scalar
    \item \texttt{crv}: Cognitive resonance vector output (from CRVU)
\end{itemize}

\subsection*{5.2 Unit-Specific Outputs}

Each SRC$^{9}$ unit outputs a structured file:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Unit} & \textbf{File} & \textbf{Contents} \\
\hline
PRU & PR-unit-temporal.json & PR frequency, symbol, harmonic group, fusion score \\
RPU & RP-framewise.json, RP-windowed.json & $\Phi$ per frame or time window \\
RFMU & RFM-unit.json & Resonance field grid + gradient vector \\
CRVU & CRV-unit.json & Cognitive vector: TPS, TFI, NSF \\
\hline
\end{tabular}
\end{center}

All outputs are timestamp-aligned at 0.1s resolution.

\subsection*{5.3 Symbolic Microtonal Encoding}

SRC$^{9}$ uses a compact symbolic system to encode pitch with microtonal precision:

\begin{itemize}
    \item Format: \texttt{[PitchClass][Octave][Superscript]}
    \item Superscripts denote deviation in cents:
    \begin{itemize}
        \item $⁰$ = 0 cent
        \item $⁺¹$ = +25 cent
        \item $⁻¹$ = –25 cent
        \item $⁺²$ = +50 cent
    \end{itemize}
\end{itemize}

\textbf{Examples:}

\begin{itemize}
    \item \texttt{C4⁰} = C4 at 0c
    \item \texttt{A4⁺¹} = A4 +25 cents
    \item \texttt{G3⁻²} = G3 –50 cents
\end{itemize}

This allows symbolic readability while preserving microtonal resolution from S$^{3}$ partial tracking.

\subsection*{5.4 CSV Export for Unity and Visual Systems}

Unity and WebGL-based environments operate on line-by-line streaming. Each partial is exported as:

\begin{verbatim}
time,freq,amplitude,isFundamental,harmonic_index,symbol
1.0,220.0,0.82,True,0,G3⁰
1.0,440.0,0.51,False,1,G4⁰
\end{verbatim}

Used to instantiate prefabs or terrain meshes in:

\begin{itemize}
    \item \texttt{CSVLoader.cs}
    \item \texttt{SpectrumVisualizer.cs}
    \item \texttt{RFM Terrain Generator.cs}
\end{itemize}

\subsection*{5.5 Matrix and Vector Data Structures}

Internally, each frame can also be represented in matrix form for ML pipelines:

\[
\text{PartialMatrix}_t = 
\begin{bmatrix}
f_0 & A_0 & h_0 \\
f_1 & A_1 & h_1 \\
\vdots & \vdots & \vdots \\
f_{16} & A_{16} & h_{16}
\end{bmatrix}
\quad
\text{CRV}_t = 
\begin{bmatrix}
\text{TPS}_t \\
\text{TFI}_t \\
\text{NSF}_t
\end{bmatrix}
\]

This supports CRV-based AI composition, harmonic fingerprint learning, or real-time ML inference.

\subsection*{5.6 File Structure Conventions}

\begin{itemize}
    \item \texttt{../data/raw/} — RawSpectrum-unit.json
    \item \texttt{../data/output/PR/} — Phantom root segments
    \item \texttt{../data/output/RP/} — Framewise/windowed Φ
    \item \texttt{../data/output/RFM/} — Grid + gradient fields
    \item \texttt{../data/output/CRV/} — Cognitive vector layers
\end{itemize}

All files are UTF-8 encoded and stored in flat JSON or CSV formats for interoperability with scientific tools and frontend visual platforms.


---

```latex
\section*{Chapter VI — Visualization Layers and Multimodal Rendering}

SRC$^{9}$ is designed not only to compute resonance and cognition, but to render it visually. Every analytical layer, from raw spectral frames to cognitive vectors, is projected into a coherent visual system aligned across time and frequency. These visualizations are not cosmetic: they serve as cognitive tools, allowing researchers, composers, and users to intuitively see the dynamics of harmonic structure.

\subsection*{6.1 Layer-Based Stack Design}

SRC$^{9}$ visualization output is composed of vertically stacked unit layers, each aligned to a shared horizontal timeline (0–20s). Each layer occupies 216px vertical space, with the RawSpectrum occupying 1080px as base.

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer} & \textbf{Visual Form} & \textbf{Height (px)} \\
\hline
RawSpectrum (S$^3$) & colored partial markers & 1080 \\
PRU & red bars with pitch labels & 216 \\
RPU & $\Phi$ line + window overlays & 216 \\
RFMU & resonance field heatmap & 216 \\
CRVU & RGB stacked bars & 216 \\
\hline
Total & 2160 px (4K) & \\
\hline
\end{tabular}
\end{center}

\subsection*{6.2 Visual Encoding Principles}

\paragraph{Frequency:} Y-axis (log scale)  
\paragraph{Amplitude:} Marker size, object scale, emission intensity  
\paragraph{Time:} X-axis (0–20s, 0.1s resolution)  
\paragraph{Color:}  
\begin{itemize}
  \item Frequency class (e.g., pitch class palette)
  \item Functional role (e.g., PR = red, $\Phi$ = gray, field = inferno colormap)
  \item CRV: red = TPS, green = TFI, blue = NSF
\end{itemize}

\paragraph{Symbolic labels:} Microtonal notations (e.g., G2⁺¹) appear on PR bars and RawSpectrum points.

\subsection*{6.3 Plotting Tools}

\begin{itemize}
    \item \textbf{Plotly (Python):} for interactive HTML visualizations, hoverable markers, frame-aligned curves
    \item \textbf{Matplotlib:} for static PNG exports, segment overlays, symbol-annotated graphs
    \item \textbf{Unity (C\#):} for 3D mesh-based field rendering and partial animation
\end{itemize}

\subsection*{6.4 Master Overlay Generation}

The file \texttt{visualize_overlay_all.py} combines all unit visual outputs into a single 3840×2160 image or HTML frame.

\textbf{Features:}

\begin{itemize}
    \item Frame-synchronized overlays
    \item Independent vertical scaling per layer
    \item Interactive time cursor
    \item Toggleable layers
\end{itemize}

\subsection*{6.5 Cognitive Color Mapping}

CRVU’s cognitive outputs (TPS, TFI, NSF) are displayed as stacked colored bars:

\begin{itemize}
    \item \textcolor{red}{Red:} TPS — stability
    \item \textcolor{green}{Green:} TFI — fusion
    \item \textcolor{blue}{Blue:} NSF — memory encoding
\end{itemize}

The bar height corresponds to value magnitude $[0,1]$.

\subsection*{6.6 RFM Surface Rendering in Unity}

RFMU’s scalar field data are converted into 3D terrain meshes:

\begin{itemize}
    \item X = time
    \item Z = frequency (log scale)
    \item Y = field strength $\rightarrow$ terrain height
    \item Emission map = normalized $\Phi$
    \item Overlay: peak paths, PR lines, curvature ridges
\end{itemize}

Implemented using Unity’s \texttt{MeshFilter}, \texttt{MaterialPropertyBlock}, and shader-based vertex displacement.

\subsection*{6.7 Animation and Playback Features}

\begin{itemize}
    \item Timeline scrubbing (linked to frame index)
    \item Real-time playback at 10 FPS (0.1s/frame)
    \item Sound-reactive visuals (optional)
    \item Dynamic camera tracking (e.g., PR curve follower)
\end{itemize}

\subsection*{6.8 Use Cases}

\begin{itemize}
    \item \textbf{Education:} visually teach harmonic fields, voice leading, polyphony
    \item \textbf{Analysis:} detect modulation, PR shift, dissonance zones
    \item \textbf{Performance:} display resonance terrain live in VR
    \item \textbf{Composition:} use RFM as a topographic canvas for generative tools
\end{itemize}

\section*{Chapter VII — Intermodular Feedback and Adaptive Control}

A defining feature of SRC$^{9}$ is its recursive structure: each module not only feeds into the next but also receives feedback from downstream layers. This transforms the system from a static analyzer into a dynamic resonance engine — capable of adaptive learning, reweighting, and perceptually informed transformation.

\subsection*{7.1 Loop Architecture}

The primary communication loop of SRC$^{9}$ is:

\[
\text{S$^3$} \rightarrow \text{R$^3$} \rightarrow \text{C$^3$} \rightarrow \text{Feedback to R$^3$ or S$^3$}
\]

\paragraph{Forward Path:}
\begin{itemize}
    \item S$^3$ $\rightarrow$ R$^3$: partial frames $\rightarrow$ harmonic reasoning
    \item R$^3$ $\rightarrow$ C$^3$: CRV vector + $\Phi$ + RFM data
\end{itemize}

\paragraph{Feedback Path:}
\begin{itemize}
    \item C$^3$ $\rightarrow$ R$^3$: attention, immersion, memory modulation
    \item R$^3$ $\rightarrow$ S$^3$: spectral filtering, dynamic rescaling, visualization tuning
\end{itemize}

\subsection*{7.2 C$^3$ $\rightarrow$ R$^3$ Feedback Mechanisms}

\paragraph{Affective Salience (AOU, PIU):}
High immersion scores increase weight on core partials in RFM generation:

\[
A_i^{*} = A_i \cdot (1 + \lambda_{\text{PIU}})
\]

\paragraph{Tension Focus (CTU):}
RPU’s $\Phi$ calculation uses tension-weighted denominators:

\[
\Phi'(t) = \sum_{i<j} \frac{A_i A_j}{|f_i - f_j| + \epsilon} \cdot \omega_{\text{CTU}}
\]

\paragraph{Memory Anchoring (SAU):}
SAU can extend windowed Φ integration across prior phrases to model phrase re-entry or long-term attractor stabilization.

\subsection*{7.3 C$^3$ $\rightarrow$ S$^3$ Modulation}

\begin{itemize}
    \item \textbf{Spectral Masking:} Hide partials with low salience or low CRV
    \item \textbf{Symbol Injection:} Annotate or override f₀ labels with C$^3$-informed symbolic tags
    \item \textbf{Display Scaling:} Increase opacity/size of key partials if memory/affect signal is high
\end{itemize}

\subsection*{7.4 Modulation Example (Narrative Music)}

Assume a phrase begins with stable CRV:

\[
\vec{\text{CRV}}_1 = [0.91, 0.88, 0.82]
\]

The system increases visual brightness of corresponding partials and amplifies RFM terrain peaks.

A modulation or PR shift occurs:

\[
\vec{\text{CRV}}_2 = [0.41, 0.33, 0.17]
\]

This results in:
\begin{itemize}
    \item Sharpened $\nabla$RFM contours
    \item Increase in partial flicker effect in Unity
    \item CRV bars collapse → signaling cognitive destabilization
\end{itemize}

\subsection*{7.5 Feedback API Specification}

\paragraph{Feedback Packet (JSON):}

\begin{verbatim}
{
  "time": 4.3,
  "feedback": {
    "CTU": 0.87,
    "PIU": 0.76,
    "NSU": 0.41
  }
}
\end{verbatim}

Received by:

\begin{itemize}
    \item RFM filter generator
    \item Partial weighting engine
    \item Visual modulation manager
\end{itemize}

\subsection*{7.6 Live Feedback and Loop Safety}

\begin{itemize}
    \item Feedback modulation is clamped between $\pm 25\%$ per frame
    \item Frame history buffers used to prevent oscillation artifacts
    \item Async-safe handlers allow interruption or override at runtime
\end{itemize}

\subsection*{7.7 Toward Resonance-Centric Interactivity}

The feedback loop is not just for refinement. It enables new applications:

\begin{itemize}
    \item Interactive composition: CRV $\rightarrow$ generative seed adjustment
    \item Brain–music co-evolution: EEG $\rightarrow$ C$^3$ $\rightarrow$ R$^3$ reshaping
    \item Self-regulating installations: perception $\rightarrow$ structure $\rightarrow$ perception
\end{itemize}

\section*{Chapter VIII — Scientific Contribution and Comparative Positioning}

SRC$^{9}$ is more than a computational toolkit — it is a conceptual shift in how music is understood, analyzed, and linked to perception. This chapter contextualizes SRC$^{9}$ within existing scientific disciplines and explains its novel contribution to music theory, auditory neuroscience, cognitive modeling, and AI.

\subsection*{8.1 Bridging Fragmented Disciplines}

\begin{itemize}
    \item \textbf{Traditional Music Theory:} Offers symbolic, style-specific models (e.g., Roman numerals, keys) that lack generalizability to non-Western, microtonal, or electronically produced music.
    \item \textbf{Auditory Neuroscience:} Describes neural encoding of sound, but lacks structural models of music capable of predicting EEG/fMRI response.
    \item \textbf{AI/ML Music Systems:} Generate convincing audio, but are black-box and devoid of interpretability or symbolic grounding.
\end{itemize}

\textbf{SRC$^{9}$ bridges these silos} by combining spectral analysis, resonance modeling, and cognitive simulation into a unified framework.

\subsection*{8.2 Novel Contributions by Module}

\paragraph{S$^3$ — Spectral Extraction}
\begin{itemize}
    \item Sub-cent partial tracking with harmonic identification
    \item Microtonal symbolic pitch encoding ($\pm$25c steps)
    \item 4K-resolution time–frequency mapping
\end{itemize}

\paragraph{R$^3$ — Resonance Modeling}
\begin{itemize}
    \item Real-valued $\Phi$ coherence computation
    \item RFM field topography and attractor mapping
    \item Phantom root detection without symbolic grammar
    \item Resonance-based perception modeling (TPS, TFI, NSF)
\end{itemize}

\paragraph{C$^3$ — Cognitive Circuitry}
\begin{itemize}
    \item Unit–Node–Element hierarchy with EEG/fMRI anchors
    \item Integration with real-time neural interfaces (e.g., OpenBCI, Emotiv)
    \item Emotional salience, memory encoding, and inter-brain synchrony modeling
\end{itemize}

\subsection*{8.3 Epistemological Reversal}

Traditionally:

\[
\text{Notation} \rightarrow \text{Structure} \rightarrow \text{Sound}
\]

SRC$^{9}$ reverses this:

\[
\text{Sound} \rightarrow \text{Structure} \rightarrow \text{Perception}
\]

This change recognizes that human listeners don’t hear scores — they hear waveforms, from which meaning emerges through spectral convergence, not grammatical rule sets.

\subsection*{8.4 Relationship to Existing Models}

\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Model} & \textbf{Comparison to SRC$^9$} \\
\hline
Lerdahl/Jackendoff GTTM & Symbolic-only, lacks spectral realism \\
Huron’s ITPRA & Predictive cognition, no spectral grounding \\
Bregman’s Auditory Scene Analysis & Compatible perceptually, no structural formalism \\
Schenkerian Analysis & Hierarchical tonality, score-dependent \\
Tonnetz/Neo-Riemannian Theory & Static topology, lacks time/frequency axes \\
MusicLM / Magenta & Non-interpretable deep generative models \\
\hline
\end{tabular}
\end{center}

\subsection*{8.5 Empirical Grounding}

\begin{itemize}
    \item $\Phi$ aligns with neural synchrony and EEG FFR data (Bidelman 2011)
    \item CRV mirrors attention and memory indices in fMRI studies (Zatorre et al. 2013)
    \item Microtonal segmentation reflects known auditory thresholds (Moore 2012)
    \item Temporal frame length matches auditory ERP resolution (MMN, P300)
\end{itemize}

\subsection*{8.6 Scientific Use Cases}

\begin{itemize}
    \item Neurocognitive analysis of music listening
    \item Dynamic modeling of musical form without score
    \item Empirical testing of tension, memory, or absorption in real time
    \item Cross-cultural harmonic modeling (e.g., gamelan, maqam, drone music)
    \item Tonotopic map visualization from real audio
\end{itemize}

\subsection*{8.7 Artistic Use Cases}

\begin{itemize}
    \item AI composition using CRV trajectories
    \item VR installations guided by RFM terrain
    \item Generative systems with real-time PR feedback
    \item Improvisation interfaces using $\Phi$ heatmaps and modulation vectors
\end{itemize}

\subsection*{8.8 Educational Use Cases}

\begin{itemize}
    \item Teaching spectral vs. symbolic harmony
    \item Visualizing modulation, drift, tension, and resolution
    \item Exploring affective resonance in sound
    \item Multisensory music learning through waveform → field → cognition
\end{itemize}

\subsection*{8.9 Future Research Integration}

SRC$^{9}$ can integrate with:

\begin{itemize}
    \item EEG systems (OpenBCI, Emotiv) — real-time feedback to CRVU
    \item Machine learning — CRV as feature vector for emotion or form prediction
    \item Neuroscientific experiments — auditory-cognitive mapping under stimuli
    \item Notational systems — hybrid symbolic–spectral scores
\end{itemize}

\section*{Chapter IX — Implementation Architecture and Development Overview}

While SRC$^{9}$ is rooted in scientific theory and cognitive models, it is also a practical software system: a set of coordinated Python, JSON, CSV, Unity, and WebGL components that form a modular, executable pipeline.

This chapter describes the engineering architecture of SRC$^{9}$ — the file structures, runtime logic, APIs, and execution modes that bring its resonance engine to life.

\subsection*{9.1 System Overview}

SRC$^{9}$ is composed of three primary code layers:

\begin{itemize}
    \item \textbf{Core Analysis Layer:} Python modules for S$^3$, R$^3$, and C$^3$ computations
    \item \textbf{Visualization Layer:} Plotly, Matplotlib, and WebGL-based rendering scripts
    \item \textbf{Interaction Layer:} Unity scene controllers, OSC interfaces, and data streaming tools
\end{itemize}

\subsection*{9.2 Folder and File Structure}

\begin{verbatim}
/src/
  /s3/
    extract_frequencies_crepe.py
    harmonics_matching.py
  /r3/
    PR_unit_temporal.py
    RP_unit_combined.py
    RFM_unit_analysis.py
    CRV_unit_analysis.py
  /c3/
    CTU_compute.py
    AOU_compute.py
    ...
  /visualize/
    visualize_PR_temporal.py
    visualize_RFM_unit.py
    visualize_overlay_all.py
  run_SRC9_pipeline.py
/data/
  /raw/
    RawSpectrum-unit.json
  /output/
    /PR/
    /RP/
    /RFM/
    /CRV/
/output/
  R3-overlay.html
  *.png
/unity/
  CSVLoader.cs
  SpectrumVisualizer.cs
  CRVOverlayHUD.cs
\end{verbatim}

\subsection*{9.3 Execution Pipeline}

Execution can occur:

\begin{itemize}
    \item \textbf{Sequentially} — via \texttt{run_SRC9_pipeline.py}
    \item \textbf{Manually} — unit-by-unit for debugging
    \item \textbf{Real-time} — via live frame ingestion, under development
\end{itemize}

\paragraph{Standard Pipeline Order:}

\begin{enumerate}
    \item S$^3$: CREPE $\rightarrow$ base spectrum
    \item R$^3$: PRU, RPU, RFMU, CRVU
    \item C$^3$: All 9 Units $\rightarrow$ summary JSON
    \item Visualizer: Generate overlays and interactive outputs
    \item Unity export: CSV + animation parameters
\end{enumerate}

\subsection*{9.4 API Design Philosophy}

\textbf{Input:} always JSON

\textbf{Output:} JSON + PNG + HTML (Plotly) + CSV (Unity)

Each function or script is:

\begin{itemize}
    \item stateless (idempotent)
    \item reusable (called by other pipelines)
    \item visually testable (through plot outputs)
\end{itemize}

\subsection*{9.5 Modularity Map}

\begin{center}
\includegraphics[width=0.75\textwidth]{modular_block_diagram.pdf}
\end{center}

Modules are black-box compatible — meaning any layer (e.g., RFMU) can be replaced or extended with an alternate implementation without breaking upstream/downstream logic.

\subsection*{9.6 Runtime Profiling (Batch Mode)}

On a standard system (Intel i7, 16 GB RAM):

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Unit} & \textbf{Analysis Time (200 frames)} & \textbf{Visualization Time} \\
\hline
PRU & 2.1 s & 1.4 s \\
RPU & 4.0 s & 2.2 s \\
RFMU & 5.2 s & 3.1 s \\
CRVU & 1.0 s & 1.0 s \\
C$^3$ full unit set & $\sim$9 s & — \\
Overlay (HTML) & — & 3–5 s \\
\hline
\end{tabular}
\end{center}

\textbf{Total batch time:} ~20–25 seconds per 20s audio input.

\subsection*{9.7 Unity Engine Integration}

\textbf{File Format:} CSV  
\textbf{Core Classes:}
\begin{itemize}
    \item \texttt{Partial.cs} — object representation
    \item \texttt{CSVLoader.cs} — parser and loader
    \item \texttt{SpectrumVisualizer.cs} — prefab instantiation
    \item \texttt{CRVOverlayHUD.cs} — affective bar display
\end{itemize}

\textbf{Render Modes:}
\begin{itemize}
    \item glowing spheres for partials
    \item PR curves via \texttt{LineRenderer}
    \item terrain mesh for RFM via \texttt{MeshFilter}
\end{itemize}

\subsection*{9.8 Platform Compatibility}

\begin{itemize}
    \item \textbf{Python:} 3.9+
    \item \textbf{Unity:} 2021.3 LTS
    \item \textbf{Jupyter:} for experiment notebooks
    \item \textbf{Web:} HTML + Plotly/Three.js (experimental)
    \item \textbf{VR/OSC:} WebSocket-ready
\end{itemize}

\subsection*{9.9 Distribution and Open Source}

SRC$^{9}$ is open-source under the MIT license. Code, data samples, visual exports, and Unity demos are available at:

\texttt{\url{https://github.com/src9-framework/src9}}

Contributors are invited to fork units, extend field models, or contribute to future modules such as OL (Overtone Locking) or GMI (Global Musical Inference).

\section*{Chapter X — Final Reflections and Theoretical Outlook}

SRC$^{9}$ is not merely a framework, a pipeline, or a set of scripts. It is a paradigm: a new way of conceptualizing, measuring, and interacting with musical structure. It brings together physics, perception, cognition, and computation into a unified temporal field theory of music.

\subsection*{10.1 A New Definition of Harmony}

Harmony is traditionally defined symbolically — as triads, scales, or functional progressions. SRC$^{9}$ proposes a redefinition:

\textbf{Harmony is a time-varying field of structured resonance, shaped by energy, weighted by perception, and embedded in cognition.}

Instead of working in discrete steps (e.g., I–IV–V), SRC$^{9}$ defines harmony as an evolving topology:

\begin{itemize}
    \item \textbf{Attractors:} Phantom roots, perceptual centers
    \item \textbf{Gradients:} Tonal pull, dissonance slope
    \item \textbf{Fusion zones:} $\Phi$ coherence regions
    \item \textbf{Modulation:} Topographic drift in RFM space
\end{itemize}

\subsection*{10.2 Cognitive Resonance as Musical Logic}

Through C$^{3}$, harmony becomes measurable not only in acoustics, but in brain-space:

\begin{itemize}
    \item TPS $\rightarrow$ perceived stability
    \item TFI $\rightarrow$ spectral coherence
    \item NSF $\rightarrow$ memory encoding strength
\end{itemize}

This transforms music analysis into neuroperceptual logic — one grounded in actual listener behavior, emotion, and timing.

\subsection*{10.3 Generalization Across Styles and Cultures}

Because SRC$^{9}$ is not bound to Western notation, keys, or 12-TET tuning, it generalizes to:

\begin{itemize}
    \item Drone-based music (e.g., Indian raga, Tibetan chant)
    \item Just intonation and spectralism
    \item Electronic soundscapes and ambient textures
    \item Improvised music, microtonal works, non-metered environments
\end{itemize}

Its mathematical core — $\Phi$, RFM, CRV — is culturally neutral but perceptually rich.

\subsection*{10.4 Toward a New Science of Sound}

SRC$^{9}$ invites a reimagination of music cognition as a form of field-based reasoning:

\[
\text{Cognition is not symbolic parsing. It is real-time entrainment to dynamic energy structures.}
\]

This claim opens doors to:

\begin{itemize}
    \item New theories of musical time and memory
    \item Biofeedback systems that respond to sonic states
    \item Emotion-aware generative music engines
    \item Aesthetic theories rooted in resonance, not style
\end{itemize}

\subsection*{10.5 Final Statement}

SRC$^{9}$ is designed to evolve.

It is not only a tool for analyzing the music of the past — it is a language for the music of the future. A music that is:

\begin{itemize}
    \item Spectrally informed
    \item Resonantly grounded
    \item Cognitively engaged
    \item Mathematically coherent
    \item Visually immersive
\end{itemize}

The resonance field is the new score.

The CRV vector is the new expressive arc.

The mind–ear interface is the new stage.

\textbf{This is the architecture of SRC$^{9}$.}

\section*{Chapter X — Final Reflections and Theoretical Outlook}

SRC$^{9}$ is not merely a framework, a pipeline, or a set of scripts. It is a paradigm: a new way of conceptualizing, measuring, and interacting with musical structure. It brings together physics, perception, cognition, and computation into a unified temporal field theory of music.

\subsection*{10.1 A New Definition of Harmony}

Harmony is traditionally defined symbolically — as triads, scales, or functional progressions. SRC$^{9}$ proposes a redefinition:

\textbf{Harmony is a time-varying field of structured resonance, shaped by energy, weighted by perception, and embedded in cognition.}

Instead of working in discrete steps (e.g., I–IV–V), SRC$^{9}$ defines harmony as an evolving topology:

\begin{itemize}
    \item \textbf{Attractors:} Phantom roots, perceptual centers
    \item \textbf{Gradients:} Tonal pull, dissonance slope
    \item \textbf{Fusion zones:} $\Phi$ coherence regions
    \item \textbf{Modulation:} Topographic drift in RFM space
\end{itemize}

\subsection*{10.2 Cognitive Resonance as Musical Logic}

Through C$^{3}$, harmony becomes measurable not only in acoustics, but in brain-space:

\begin{itemize}
    \item TPS $\rightarrow$ perceived stability
    \item TFI $\rightarrow$ spectral coherence
    \item NSF $\rightarrow$ memory encoding strength
\end{itemize}

This transforms music analysis into neuroperceptual logic — one grounded in actual listener behavior, emotion, and timing.

\subsection*{10.3 Generalization Across Styles and Cultures}

Because SRC$^{9}$ is not bound to Western notation, keys, or 12-TET tuning, it generalizes to:

\begin{itemize}
    \item Drone-based music (e.g., Indian raga, Tibetan chant)
    \item Just intonation and spectralism
    \item Electronic soundscapes and ambient textures
    \item Improvised music, microtonal works, non-metered environments
\end{itemize}

Its mathematical core — $\Phi$, RFM, CRV — is culturally neutral but perceptually rich.

\subsection*{10.4 Toward a New Science of Sound}

SRC$^{9}$ invites a reimagination of music cognition as a form of field-based reasoning:

\[
\text{Cognition is not symbolic parsing. It is real-time entrainment to dynamic energy structures.}
\]

This claim opens doors to:

\begin{itemize}
    \item New theories of musical time and memory
    \item Biofeedback systems that respond to sonic states
    \item Emotion-aware generative music engines
    \item Aesthetic theories rooted in resonance, not style
\end{itemize}

\subsection*{10.5 Final Statement}

SRC$^{9}$ is designed to evolve.

It is not only a tool for analyzing the music of the past — it is a language for the music of the future. A music that is:

\begin{itemize}
    \item Spectrally informed
    \item Resonantly grounded
    \item Cognitively engaged
    \item Mathematically coherent
    \item Visually immersive
\end{itemize}

The resonance field is the new score.

The CRV vector is the new expressive arc.

The mind–ear interface is the new stage.

\textbf{This is the architecture of SRC$^{9}$.}


\end{document}



